---
layout: post
title: 임베딩과 신경망 언어 모델 (NPLM)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# Word Embedding

단어를 벡터화 하는 가장 단순하고 직관적인 방법은 **원-핫 인코딩(One-hot Encoding)** 이다. 원-핫 인코딩은 문서에서 전체에 있는 단어 개수 만큼의 크기를 갖고 모든 요소가 0인 벡터를 만든 뒤 단어가 등장하는 순서에 따라 해당 인덱스를 1로 변환하는 방식이다. 예를 들어 *"I go to school"* 에 있는 단어들을 원-핫 인코딩으로 나타내면 아래와 같다.


$$
\qquad \text{I} : \left[{\begin{array}{cccc} 1 & 0 & 0 & 0  \end{array}}\right] \\
\quad \text{go} : \left[{\begin{array}{cccc} 0 & 1 & 0 & 0  \end{array}}\right] \\
\quad \text{to} : \left[{\begin{array}{cccc} 0 & 0 & 1 & 0  \end{array}}\right] \\
\text{school} : \left[{\begin{array}{cccc} 0 & 0 & 0 & 1  \end{array}}\right]
$$

원-핫 인코딩은 단순하고 쉽다는 장점이 있다. 하지만 모든 단어 벡터끼리의 내적이 항상 0이 나오기 때문에 단어 간의 관계를 파악하기 어렵다는 단점을 가지고 있다. 이런 문제를 해결하기 위한 방법으로 **분산 표현(Distributed Representation)** 방법이 있다. 단어를 벡터로 분산 표현하는 방법, 즉 **임베딩(Embedding)** 은 단어를 문서 내 단어의 개수 $\vert V \vert$ 보다 훨씬 작은 숫자인 $n$ 차원의 벡터로 나타낸다. 또한 벡터 내 성분이 연속적인 숫자이기 때문에 단어 벡터의 내적 값이 0이 아니게 되고 덕분에 단어 간의 의미론(Semantic)적인 관계가 보존된다.

<p align="center"><img src="https://www.offconvex.org/assets/analogy-small.jpg" alt="embedding" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://www.offconvex.org/2015/12/12/word-embeddings-1/">offconvex.org</a></p>



## NPLM

**확률기반 신경망 언어 모델(Neural Probilistic Language Model, NNLM)** 은 가장 고전적인 분산 표현 방식이다. 각 단어를 분산 단어 피처 벡터로 표현한다. 이 피처 벡터끼리의 결합 확률분포 함수(Joint probability function)을 이용하여 단어 시퀀스가 발생할 확률을 계산할 수 있다. 어떤 단어 피처 벡터가 좋은지와 이를 통해서 나타나는 확률 함수의 파라미터를 동시에 학습할 수 있다.

NPLM에서는 *(dog, cat), (the, a), (bedroom, room), (is, was), (running, walking)* 의 단어쌍이 의미적, 구조적으로 유사한 역할을 하고 있다면 우리는 이를 사용해 *The cat is walking in the bedroom* 이라는 문장을 다음과 같이 일반화 해볼 수 있다.

*A dog was running in a room, The cat is running is a room, A dog is walking in a bedroom, The dog was walking in the room ...*

NPLM이 N-gram과 같은 Count 기반의 언어 모델과 다른 점은 무엇인가? 기본의 Count 기반 언어 모델에서 단어 시퀀스는 Chain rule에 의해 아래와 같이 Factorization 될 수 있고, 특히 N-gram 모델에서는 마르코프 가정(Markov Assumption)을 사용하여 타깃 단어 이전에 있는 몇 단어(n개의 단어)에 대해서만 계산했다.



$$
p(w_1, \cdots , w_T) = \prod^T_{i=1} p(w_t|w_1, \cdots , w_{t-1}) \\
p(w_t|w_1, \cdots , w_t) \approx p(w_t|w_{t-n}, \cdots, w_{t-1})
$$



NPLM은 위 방식과는 다르다. NNLM 에서는 단어를 $n(<< \vert V \vert)$ 차원 벡터 공간 내의 밀집 벡터(dense vectors)로 표현한다. 그리고 신경망이 특정 단어 다음에 올 단어 벡터가 어떤 것이 될 지에 대해서 훈련하게 된다.


$$
p(w_t = j|w_1, \cdots , w_{t-1}) = \frac{\exp (p^j \cdot g(x_1, \cdots, x_{t-1} + q^j))}{\sum_{j^\prime \in V} \exp (p^{j^\prime} \cdot g(x_1, \cdots, x_{t-1} + q^{j^\prime}))} \\ \qquad \quad \qquad = \text{softmax} (P_{g(x_1, \cdots, x_{t-1})} + q)
$$


NPLM의 구조는 아래 이미지와 같다. 각 단어의 인덱스로부터 추출한 원-핫 벡터로 미리 준비된 행렬 $C_{\vert V \vert \times n}$ 를 **참조(Look-up)** 하여 행렬곱(Matmul)을 취해준다. 행렬곱의 결과에 softmax 함수를 취해주어 각 단어가 해당 자리에 위치할 확률을 구해낸다. 

<p align="center"><img src="http://i.imgur.com/vN66N2D.png" alt="nnlm" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

NPLM을 학습하는 주 목적은 좋은 모델 함수 $f(w_t, \cdots, w_t-n+1)$ = \hat{P} (w_t \vert w_1^{t-1}) 을 찾는 것이다. 주어진 단어 시퀀스 뒤에 오는 $t$ 번째 단어의 확률이 극대화 되는 $f$ 를 찾는 것이 목적이다. NNLM을 적용하기 위해서 두 가지 제약 조건이 있다. 첫 번째로 t-1 자리에 어떤 단어 $w_{1}^{t-1}$ 가 나오더라도 t에 나오는 모든 단어의 확률 합은 1이 되어야 한다. 그리고 각 단어가 생성될 확률은 모두 0보다 크거나 같아야 한다. 이 두 가지 제약 조건을 식으로 나타내면 다음과 같다.


$$
\sum^{\vert V \vert} _{i=1} f(i, w_{t-1}, \cdots, w_{t-n+1}) = 1 \\
f \geq 0
$$


모델링된 함수 $f$ 를 아래와 같이 수식을 참조(Look-up) 행렬 $C_{\vert V \vert \times n}$ 와의 Matmul 을 거쳐 변환된 벡터로 표현되는 새로운 함수 $g$ 로 나타낼 수 있다. 여기서 C는 $n (<< \vert V \vert)$ 차원의 벡터이다. 말이 되는(가능성이 높은) 시퀀스에게는 높은 확률을 부여하고 말이 안되는(가능성이 낮은) 시퀀스에게는 낮은 확률을 부여하는 것이 확률 함수 g의 역할이다. 


$$
f(i, w_{t-1}, \cdots, w_{t-n+1}) = g(i, C(w_{t-1}), \cdots , C(w_{t-n+1}))
$$


아래 수식과 같이 훈련 말뭉치의 로그-우도(log-likelihood)를 최대화하는 과정으로 훈련이 이루어진다.


$$
L = \frac{1}{T} \sum_t \log f(i, w_{t-1}, \cdots, w_{t-n+1};\theta) + R(\theta)
$$

은닉층이 내놓는 값 $y$ 는 아래와 같이 표현할 수 있다. 


$$
y = b + Wx + U \cdot \tanh(d+Hx) \\
b : \text{출력값의 편향,} \quad d : \text{은닉층의 편향} \\
U : \text{은닉층-출력층의 가중치,} \quad W : \text{단어 피처-출력값의 가중치,} \quad H : \text{은닉층의 가중치}
$$


각 단어의 확률이 가장 높을 때의 모델을 구현하는 것이 목적이므로 역전파(Back-propagation)값으로부터 **확률적 경사 상승법(Stochastic Gradient Ascent, SGA)** 를 사용하여 파라미터 셋 $\theta$ 를 개선해나간다.


$$
\theta \leftarrow \theta + \epsilon \frac{\partial \log \hat{P} (w_t|w_{t-1}, \cdots, w_{t-n+1})}{\partial \theta}
$$

