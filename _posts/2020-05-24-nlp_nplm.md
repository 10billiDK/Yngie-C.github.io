---
layout: post
title: 단어 임베딩 (Word Embedding)과 신경망 언어 모델 (NPLM)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# Distributed Representation

지금까지 알아본 [Bag of Words](https://yngie-c.github.io/nlp/2020/05/12/nlp_bow/), [TF-IDF](https://yngie-c.github.io/nlp/2020/05/16/nlp_tf-idf/), [N-gram](https://yngie-c.github.io/nlp/2020/05/22/nlp_ngram/) 방법론은 단어를 등장 횟수 기반으로 벡터로 표현(Count-based representation)하는 방식이었습니다. 이번에는 등장 횟수가 아닌 **단어의 분산 표현(Distributed representation)**방법에 대해 알아볼 것입니다.

횟수 기반 표현을 사용하지 않고 단어를 벡터화하는 가장 단순하고 직관적인 방법은 **원-핫 인코딩(One-hot Encoding)**입니다. 원-핫 인코딩은 적용하는 방법은 다음과 같습니다. 일단 문서에 존재하는 단어의 개수 만큼의 차원을 가진 영벡터를 만듭니다. 그리고 단어가 등장하는 순서에 따라 해당 인덱스를 1로 변환한 뒤 그 단어의 벡터로 매칭시켜줍니다. 예를 들어,  *"What's your name? My name is Yngie."* 라는 문장에 있는 단어를 원-핫 인코딩을 사용하여 벡터로 표현하면 다음과 같습니다.

> what : $\left[{\begin{array}{cccccc} 1 & 0 & 0 & 0 & 0 & 0 \end{array}}\right]$
> is : $\left[{\begin{array}{cccccc} 0 & 1 & 0 & 0 & 0 & 0 \end{array}}\right]$
> your : $\left[{\begin{array}{cccccc} 0 & 0 & 1 & 0 & 0 & 0 \end{array}}\right]$
> name : $\left[{\begin{array}{cccccc} 0 & 0 & 0 & 1 & 0 & 0 \end{array}}\right]$
> my : $\left[{\begin{array}{cccccc} 0 & 0 & 0 & 0 & 1 & 0 \end{array}}\right]$
> Yngie : $\left[{\begin{array}{cccccc} 0 & 0 & 0 & 0 & 0 & 1 \end{array}}\right]$

원-핫 인코딩은 단순하고 이해하기 쉽다는 장점이 있습니다. 하지만 단어끼리의 관계를 모사할 수가 없게 됩니다. 벡터 간 유사도를 판단하는 방법은 다양하지만, 단어 벡터 사이의 유사도를 판단하는 데에는 주로 **코사인 유사도(Cosine similarity)**가 사용됩니다. 코사인 유사도의 수식은 다음과 같습니다.


$$
\text{similarity}(\vec{x},\vec{y}) = \frac{\vec{x} \cdot \vec{y}}{\Vert\vec{x}\Vert \Vert\vec{y}\Vert} = \frac{x_1y_1 + \cdots + x_ny_n}{\sqrt{x_1^2+\cdots+x_n^2} \sqrt{y_1^2+\cdots+y_n^2}}
$$


원-핫 인코딩으로 생성된 임의의 서로 다른 두 단어 벡터 $\vec{x_o}, \vec{y_o}$ 의 단어간 유사도를 구하기 위해 코사인 유사도 식에 대입하면 어떻게 될까요? 결과만 보자면 모두 0이 나오게 됩니다. 원-핫 인코딩으로 생성된 서로 다른 벡터를 내적한 값 $\vec{x_o} \cdot \vec{y_o}$ 은 언제나 0이 나오기 때문입니다. 이 때문에 원-핫 인코딩은 단어의 의미 관계를 전혀 표현하지 못하게 됩니다.

이런 문제를 해결하기 위해서 분산 표현 방식인 **단어 임베딩(Word Embedding)**이 고안되었습니다. 임베딩은 단어를 문서 내 단어의 개수 $\vert V \vert$보다 훨씬 작은 숫자인 $n$ 차원의 벡터로 나타냅니다. 임베딩을 통해 나타내어지는 단어 벡터는 성분이 0, 1이 아닌 연속적인 숫자로 구성되어 있습니다. 그렇기 때문에 단어 벡터끼리 내적하여도 0이 아니게 되며 단어 간의 의미(Semantic) 관계가 보존된다는 장점을 가지고 있습니다. 아래 그림은 임베딩을 통해 추출해낸 단어 벡터 간의 관계를 시각화한 것입니다.

<img src="https://miro.medium.com/max/700/1*OEmWDt4eztOcm5pr2QbxfA.png" alt="embedding"  />

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8">towardsdatascience.com</a></p>



## Neural Network Language Model

**신경망 언어 모델(Neural Probilistic Language Model, NNLM)**은 기존 [단어 횟수 기반의 언어 모델(Language models)](https://yngie-c.github.io/nlp/2020/05/22/nlp_ngram/)의 단점을 극복하고자 만들어진 방법입니다. 이후에 배울 단어 임베딩 방법보다는 고전적인 방법이지만 신경망을 활용하여 단어 임베딩을 할 수 있다는 것을 제안했다는 점에서 의의가 적지 않습니다.

신경망 언어 모델에서 각 단어는 임베딩을 통해 $n$ 차원의 벡터로 표현됩니다. 이 단어 벡터를 언어 모델에 적용했을 때의 장점은 무엇일까요? 기존의 언어 모델은 횟수를 기반으로 등장 확률을 결정하기 때문에 말뭉치에 한 번도 등장하지 않은 시퀀스에 대해서는 등장 확률이 0이 된다는 단점이 있었습니다. 예를 들어, *"마라룽샤를 요리하다."* 라는 단어 시퀀스가 말뭉치에 등장하지 않는다면 이 시퀀스가 생성될 확률 역시 0이 됩니다.

하지만 임베딩을 통해 표현된 단어 벡터는 의미 관계를 가지고 있기 때문에 다릅니다. 예를 들어, *(마라탕, 마라샹궈, 마라룽샤)*라는 세 단어묶음, *(요리하다, 먹다)* 의 관계가 비슷하다고 학습한 모델이 있다고 합시다. 이 모델은 말뭉치에 *"마라탕을 먹다"* 라는 시퀀스만 있더라도 *"마라샹궈를 먹다, 마라탕을 요리하다, 마라룽샤를 먹다, 마라룽샤를 요리하다"* 등 다양한 단어 시퀀스를 생성할 수 있게 됩니다. 이제 더 이상 말뭉치 시퀀스에 집착하지 않아도 되는 것이지요.



NPLM은 위 방식과는 다르다. NNLM 에서는 단어를 $n(\ll \vert V \vert)$ 차원 벡터 공간 내의 밀집 벡터(dense vectors)로 표현한다. 그리고 신경망이 특정 단어 다음에 올 단어 벡터가 어떤 것이 될 지에 대해서 훈련하게 된다.


$$
p(w_t = j|w_1, \cdots , w_{t-1}) = \frac{\exp (p^j \cdot g(x_1, \cdots, x_{t-1} + q^j))}{\sum_{j^\prime \in V} \exp (p^{j^\prime} \cdot g(x_1, \cdots, x_{t-1} + q^{j^\prime}))} \\ \qquad \quad \qquad = \text{softmax} (P_{g(x_1, \cdots, x_{t-1})} + q)
$$


NPLM의 구조는 아래 이미지와 같다. 각 단어의 인덱스로부터 추출한 원-핫 벡터로 미리 준비된 행렬 $C_{\vert V \vert \times n}$ 를 **참조(Look-up)** 하여 행렬곱(Matmul)을 취해준다. 행렬곱의 결과에 softmax 함수를 취해주어 각 단어가 해당 자리에 위치할 확률을 구해낸다. 

<p align="center"><img src="http://i.imgur.com/vN66N2D.png" alt="nnlm" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

NPLM을 학습하는 주 목적은 좋은 모델 함수 $f(w_t, \cdots, w_t-n+1)$ = \hat{P} (w_t \vert w_1^{t-1}) 을 찾는 것이다. 주어진 단어 시퀀스 뒤에 오는 $t$ 번째 단어의 확률이 극대화 되는 $f$ 를 찾는 것이 목적이다. NNLM을 적용하기 위해서 두 가지 제약 조건이 있다. 첫 번째로 t-1 자리에 어떤 단어 $w_{1}^{t-1}$ 가 나오더라도 t에 나오는 모든 단어의 확률 합은 1이 되어야 한다. 그리고 각 단어가 생성될 확률은 모두 0보다 크거나 같아야 한다. 이 두 가지 제약 조건을 식으로 나타내면 다음과 같다.


$$
\sum^{\vert V \vert} _{i=1} f(i, w_{t-1}, \cdots, w_{t-n+1}) = 1 \\
f \geq 0
$$


모델링된 함수 $f$ 를 아래와 같이 수식을 참조(Look-up) 행렬 $C_{\vert V \vert \times n}$ 와의 Matmul 을 거쳐 변환된 벡터로 표현되는 새로운 함수 $g$ 로 나타낼 수 있다. 여기서 C는 $n (<< \vert V \vert)$ 차원의 벡터이다. 말이 되는(가능성이 높은) 시퀀스에게는 높은 확률을 부여하고 말이 안되는(가능성이 낮은) 시퀀스에게는 낮은 확률을 부여하는 것이 확률 함수 g의 역할이다. 


$$
f(i, w_{t-1}, \cdots, w_{t-n+1}) = g(i, C(w_{t-1}), \cdots , C(w_{t-n+1}))
$$


아래 수식과 같이 훈련 말뭉치의 로그-우도(log-likelihood)를 최대화하는 과정으로 훈련이 이루어진다.


$$
L = \frac{1}{T} \sum_t \log f(i, w_{t-1}, \cdots, w_{t-n+1};\theta) + R(\theta)
$$

은닉층이 내놓는 값 $y$ 는 아래와 같이 표현할 수 있다. 


$$
y = b + Wx + U \cdot \tanh(d+Hx) \\
b : \text{출력값의 편향,} \quad d : \text{은닉층의 편향} \\
U : \text{은닉층-출력층의 가중치,} \quad W : \text{단어 피처-출력값의 가중치,} \quad H : \text{은닉층의 가중치}
$$


각 단어의 확률이 가장 높을 때의 모델을 구현하는 것이 목적이므로 역전파(Back-propagation)값으로부터 **확률적 경사 상승법(Stochastic Gradient Ascent, SGA)** 를 사용하여 파라미터 셋 $\theta$ 를 개선해나간다.


$$
\theta \leftarrow \theta + \epsilon \frac{\partial \log \hat{P} (w_t|w_{t-1}, \cdots, w_{t-n+1})}{\partial \theta}
$$

