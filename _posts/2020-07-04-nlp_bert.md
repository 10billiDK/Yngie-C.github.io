---
layout: post
title: BERT (Bidirectional Encoder Representations from Transformer)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# BERT

![bert_twit](https://pbs.twimg.com/profile_banners/2496408438/1579879132/1500x500)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://twitter.com/bertsesame">Twitter - bertsesame</a></p>

**BERT(Bidirectional Encoder Representations from Transformer)** 는 Google이 2018년 10월에 발표한 모델이다. 모델명에서 트랜스포머에서 인코더의 표현을 사용하는데 양방향으로 하겠다는 것을 알 수 있다.

BERT의 Pretrain은 2가지 목적을 가지고 진행된다. 첫 번째는 **Masked LM(Masked Language Model)** 이다. 기존의 언어 모델은 순차적으로 진행되었다. 순방향의 경우는 이전 단어의 정보를 받아 다음 단어를 예측하고, 역방향 언어 모델은 문장의 끝부터 단어 정보를 받아서 이전 단어를 예측한다. 하지만 Masked LM은 약 15%의 단어를 임의로 마스킹하고 양방향으로 탐색하여 그것이 어떤 단어인지를 예측한다. ELMo는 순방향과 역방향 LSTM을 따로 학습하여 그것을 결합했지만 BERT는 선행하는 단어와 후행하는 단어를 동시에 본다. 두 번째 목적은 **NSP(Next Sentence Prediction)** 이다. 2쌍의 문장이 들어왔을 때 뒤에 해당하는 문장이 실제로도 바로 다음에 등장하는 문장인지를 예측한다. 이렇게 학습된 Pretrain 모델에 각 Task에 맞는 하나의 층을 더 추가하여(Fine-Tuning) 다양한 자연어처리 Task를 수행할 수 있다.

![bert1](https://user-images.githubusercontent.com/45377884/86514117-ba428380-be4a-11ea-97d6-c8fd9d56248b.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

## 구조

<p align="center"><img src="http://jalammar.github.io/images/bert-base-bert-large.png" alt="bert2" style="zoom: 50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

BERT는 트랜스포머의 인코더 블록만을 중첩하여 사용한다. BERT 모델이 어떻게 이루어져 있는지 알아보자. 논문에서 사용된 BERT BASE 모델은 총 12개의 인코더 블록으로 이루어져 있다. 768개의 Hidden Node를 사용하고 12개의 Multi Head-Attention을 사용한다. 파라미터의 개수는 1억 1천만개이며 이는 GPT와의 성능 비교를 좀 더 용이하게 하기 위하여 이렇게 설정하였다. 사이즈를 좀 더 키운 BERT LARGE 모델은 24개의 인코더 블록을 사용한다. Hidden Node의 개수도 좀 더 늘어난 1,024이며 Attention Head도 16개를 사용한다. LARGE 모델은 총 3억 4천만 개의 파라미터를 사용하게 된다.

<p align="center"><img src="http://jalammar.github.io/images/bert-base-bert-large-encoders.png" alt="bert3" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

BERT는 데이터를 유연하게 입력할 수 있도록 설계되였다. 이는 BERT를 여러 Downstream Task에 사용하기 위함이다. 입력 Sequence로 하나의 Sentence를 입력할 수도 있고 한 쌍의 Sentence를 입력할 수도 있다. 

주의할 점은 BERT에서 사용하는 Sentence의 의미는 일반적인 의미로 사용되는 하나의 문장과 다르다는 것이다. BERT에서는 Sentence를 연속적으로 펼쳐진 텍스트라는 의미로 사용하기 때문에 우리가 일반적으로 사용하는 문장의 일부분일수도 있고, 여러 문장이 될 수도 있다. (그래서 본 글에서도 언어학에서 사용되는 '문장'과 BERT의 Sentence를 구분하여 사용하기로 한다.)

![bert4](https://user-images.githubusercontent.com/45377884/86514174-4d7bb900-be4b-11ea-923f-d0d0d474e1fc.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

BERT는 레이블 되지 않은 하나 혹은 두 개의 Sentence를 받아 그 중 15%를 마스킹 처리한다. 이렇게 마스킹된 문장은 BERT에서 사용되는 두 개의 스페셜 토큰과 함께 모델에 들어가게 된다. 첫 번째 스페셜 토큰인 [CLS]는 입력 시퀀스의 시작을 알리는 역할을 한다. 이 토큰이 모델 마지막에 갖게 되는 벡터는 NSP를 수행하는 역할을 한다. 나머지 스페셜 토큰은 [SEP]으로 시퀀스 내에 있는 두 Sentence를 구분하는 역할을 하며 시퀀스가 하나의 Sentence로만 구성된 경우에는 시퀀스의 맨 마지막에만 위치한다.

BERT에 입력되는 벡터는 3가지 임베딩 벡터들의 합으로 구성된다. 3만 개의 토큰 사전으로부터 가져온 토큰 임베딩과 [SEP]을 기준으로 이 문장이 앞에 위치하는지 뒤에 위치하는지를 구분하는 Segment 임베딩, 각 토큰의 위치 정보를 가지고 있는 Position 임베딩을 모두 더하여 최종적인 입력 벡터를 구성하게 된다.

![bert5](https://user-images.githubusercontent.com/45377884/86514189-8156de80-be4b-11ea-8437-974e5b95fcb5.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

## Pretrain

먼저 BERT의 첫 번째 Task인 **MLM(Masked Language Model)** 에 대해서 좀 더 자세히 알아보자. 입력 시퀀스의 15%가 임의로 마스킹 된다. 이렇게 처리된 입력 벡터는 BERT를 지난 후 은닉 벡터를 가지게 되고 이를 FFNN(Feed Forward Neural Network)에 통과시켜 최종 단어를 뽑아내게 된다. 이 때 마스킹되었던 단어가 실제 단어와 같은지 아닌지를 비교하며 학습하게 된다. 아래는 이 과정을 그림으로 나타낸 것이다.

![bert6](http://jalammar.github.io/images/BERT-language-modeling-masked-lm.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

위 그림에서는 4번째 단어인 *"improvisation"* 를 마스킹 하였으며 마스킹된 모델을 예측한 것이 실제 단어인 *"improvisation"* 이 되도록 학습을 하게 된다.

하지만 Fine-Tuning에서는 입력 시퀀스에 마스킹을 하지 않기 때문에 Pretrain 과정과 미스매치가 발생할 수 있다. 그렇기 때문에 일종의 트릭이 필요하다. Pretrain에서 마스킹하기로 정한 단어(전체 시퀀스의 15%) 중 80%만 마스킹 처리를 한다. 그리고 10%는 다른 단어로 치환해주고 10%는 바꾸지 않는다. 이렇게 하면 Pretrain과 Fine-Tuning에서 발생하는 미스매치를 막을 수 있다고 한다.

- *"My dog is hairy"* $\rightarrow$ *"My dog is [MASK]"* - (80%)
- *"My dog is hairy"* $\rightarrow$ *"My dog is apple"* - (10%)
- *"My dog is hairy"* $\rightarrow$ *"My dog is hairy"* - (10%)

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514778-38a12480-be4f-11ea-944b-074b3a1f5b79.png" alt="bert_mlm" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

두 번째 Task는 **NSP(Next Sentence Prediction)** 이다. QA(Question & Answering)나 NLI(Natural Language Inference, 자연어 추론)과 같은 Task의 경우에는 문장 2개 이상의 관계를 이해하여야 풀 수 있다. 한 문장씩 들어가는 이전의 모델은 이런 Task에서 잘 작동하지 않았다. 하지만 BERT는 이러한 Task까지 커버하기 위하여 NSP를 학습할 수 있도록 설계되었다.

![bert8](http://jalammar.github.io/images/bert-next-sentence-prediction.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

학습 방법은 간단하다. 학습 데이터 시퀀스 구성 시 Sentence 2개를 뽑을 때  50%는 실제로 이어지는 Sentence로 구성하고, 50%는 말뭉치 내에 있는 랜덤한 Sentence를 가져오도록 한다. 그리고 맨 첫 번째 임베딩 벡터인 $C$ 를 통해서 해당 Sentence가 실제로 이어지는지 안 이어지는지를 판별하게 된다. 학습 아이디어는 간단하지만 이 과정을 추가함으로서 QA나 NLI 같은 문제에서 높은 성능 향상을 가져올 수 있었다. NSP를 수행하는 과정을 이미지로 나타낸 것이다.

![bert_nsp1](https://user-images.githubusercontent.com/45377884/86514846-d0067780-be4f-11ea-9809-c3e43b8ad3f9.png)

![bert_nsp2](https://user-images.githubusercontent.com/45377884/86514847-d137a480-be4f-11ea-82be-d229bf75fbf8.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

아래 그림은 BERT와 GPT, ELMo의 구조를 간단하게 표현한 것이다. ELMo는 LSTM을 동일한 시퀀스를 순방향과 역방향 LSTM 여러 층에 학습시킨 후 각 벡터를 선형 결합하여 최종 출력 벡터를 만들어내는 방식임을 나타내고 있다. GPT는 트랜스포머의 디코더 블록만을 사용하기 때문에 한쪽 방향으로만 학습이 진행되는 것을 알 수 있다. BERT는 일부 단어를 마스킹하여 양방향으로 살펴보고 그 단어를 맞추어 나가는 방식임을 표현하고 있다.

![bert_str](https://user-images.githubusercontent.com/45377884/86514876-0ba14180-be50-11ea-9153-36a8120244ed.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

학습에 사용된 데이터는 8억 단어로 구성된 BookCorpus와 25억 단어로 구성된 Wikipedia를 사용하였다. 설정된 하이퍼파라미터는 다음과 같다. 최대 토큰 길이는 512개이며 배치 사이즈는 256이다. 옵티마이저(Optimizer)는 Adam을 $1.0 \times 10^{-4}$ 의 학습률로 사용하는 것을 볼 수 있다. 모든 층마다 0.1의 드롭아웃(Drop out)이 적용되어 있으며 활성화 함수는 GeLU를 사용하였다.



## Fine-Tuning

BERT의 Fine-Tuning에 대해서 알아보자. Pretrain된 BERT에 Task에 맞는 하나의 층만 걸치면 해당 Task에 맞는 일을 수행할 수 있다. Task에 따라 Fine-Tuning을 크게 4가지 종류로 구분해볼 수 있다. (a)와 (c)는 Sentence Pair를 입력받는다. (a)는 C토큰을 학습하여 Sentence Pair가 어디에 속하는지 분류하는 Task를 수행하는 경우이다. (c)는 QA Task를 수행한다. (b)와 (d)는 하나의 Sentence만을 입력받는다. (b)는 감성분석과 같은 분류를 하는 경우이며 (d)는 각 토큰마다 답을 내야 하는 형태소 분석이나 개체명 인식(Named Entity Recognition, NER) 등을 하는 경우에 사용한다.

![bert_ft](http://jalammar.github.io/images/bert-tasks.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

아래는 각 Task에 대하여 BERT와 다른 모델의 성능을 비교한 것이다. BERT BASE만으로도 이전에 발표된 ELMo나 GPT보다 훨씬 더 좋은 성능을 보이는 것을 알 수 있으며 BERT LARGE의 경우 CoLA, RTE등의 데이터셋에는 BASE보다 더욱 좋은 성능을 보이는 것을 알 수 있다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514902-515e0a00-be50-11ea-9c00-14cab4375724.png" alt="bert_perf" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

아래는 NSP를 학습하는 경우와 그렇지 않은 경우의 성능을 비교한 것으로 NSP를 수행하는 하나의 토큰만을 추가함으로서 QNLI 등의 데이터셋에서 많은 성능 향상을 보이는 것을 알 수 있다.

<img src="https://user-images.githubusercontent.com/45377884/86514948-908c5b00-be50-11ea-9a8c-921bf6bc72c8.png" alt="bert_nsp4" style="zoom: 67%;" />

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

아래는 BERT 모델 사이즈마다의 성능을 평가한 것이다. #L은 인코더 블록의 개수, #H는 은닉 노드의 개수, #A는 Attention Head의 개수이다. 아래 그림에서 4번째에 있는 것이 BASE 모델이며 맨 아래에 있는 것은  LARGE 모델이다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86514950-93874b80-be50-11ea-9d30-b06df2d67ab6.png" alt="bert_size1" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

아래는 Fine-Tuning과 Feature-based 접근법에 대해서 각각의 결과를 비교한 것이다. Fine-Tuning을 실행해준 경우가 당연히 성능이 가장 높은 것을 볼 수 있다. Feature-based 방법을 사용할 때는 그냥 입력 임베딩을 사용하면 성능이 많이 떨어지게 된다. 아래 결과를 보면 마지막 4개의 은닉 벡터에 대해 가중합을 하거나 Concatenate를 하여 사용하는 것이 가장 좋은 효과를 보이는 것을 볼 수 있다.

![bert_fb](http://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>