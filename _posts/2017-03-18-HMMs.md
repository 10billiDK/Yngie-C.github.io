---
title: Maximum Entropy Models, Hidden Markov Models
category: Machine Learning
tag: MEMs, HMMs
html header: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
---

이번 포스팅에선 **최대엔트로피모델(Maximum Entropy Models)**과 **은닉마코프모델(Hidden Markov Models, HMMs)**을 다루어 보도록 하겠습니다. 둘 품사를 분류하는 **포스태깅** 등 단어의 연쇄로 나타나는 언어구조를 처리하는 데 과거 많은 주목을 받았던 기법입니다. 이 글은 [고려대 산업경영공학부 강필성 교수님 강의](https://github.com/pilsung-kang/text-mining)와 [서울대 언어학과 신효필 교수님 저서](http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&ejkGb=KOR&barcode=9788952113719)를 참고해 작성했음을 먼저 밝힙니다. 그럼 시작해보겠습니다.



## 최대엔트로피모델

**엔트로피(entropy)**란 어떤 대상이 지닌 정보량을 측정하는 기제입니다. 어떤 확률변수가 지니는 평균적인 불확실성(uncertainty)을 의미합니다. 최대엔트로피모델은 포스태깅에 적용돼 많은 관심을 받았는데요, 사실 자연언어처리 분야에서는 **다항로지스틱회귀(multinominal logistic regression)**를 최대엔트로피모델이라 부릅니다. 

**이미 품사 정보가 할당되어 있는** 문맥정보 $C$(단어들의 모음)가 주어졌을 때 가장 확률이 높은 품사 $t$를 할당하는 최대엔트로피모델은 아래와 같이 수식화할 수 있습니다. ($λ_i$=로짓함수의 가중치, $f$=문맥정보 C에 해당하는 자질 벡터, $Z$=확률 합이 1이 되게 하는 정규화 인자)

$$P(t|C)=\frac { 1 }{ Z(C) } { exp( }\sum _{ i=1 }^{ n }{ { \lambda  }_{ i }{ f }_{ i } } )$$

그럼 위 모델을 바탕으로 학습을 하는 예시를 들어볼까요? '올림픽이 끝나고 난 뒤에'라는 구에서 중간에 나타나는 형태소인 '나'를 한번 포스태깅해보겠습니다. 세종코퍼스의 정답은 동사(VX)인데요. 이 '나'가 정답인 동사(VX)인지, 아니면 오답인 대명사(NP)로 태깅되는지 예측해보자는 거죠. 그래서 아래처럼 '나'는 아무것도 태깅되지 않은 '??'로 두었습니다. 이것이 바로 문맥정보 $C$입니다.

> 올림픽/NNG 이/JKS 끝나/VV 고/EC 나/?? + ㄴ/ETM 뒤/NNG 에/JKB

최대엔트로피모델은 위와 같은 $C$에 해당하는 자질($f$)들을 입력값으로 받습니다. 그런데 자질($f$) 벡터의 각 요소는 사람이 일일이 정해서 넣어주어야 합니다~~딥러닝처럼 end-to-end모델이 득세하는 요즘엔 상상하기 어렵죠~~. 예컨대 자질 벡터의 첫번째 요소는 '직전 형태소가 어미이면 1, 그렇지 않으면 0', 두번째 요소는 '해당 형태소가 어간이면 1, 아니면 0'... 이런 식으로 말입니다. 실제로 이 모델이 제안된 시기에는 자질 벡터의 요소를 조금 바꾸어 성능이 개선되면 논문이 될 정도였다고 합니다. 그만큼 특징/자질 추출에 사람 손이 많이 간다는 이야기이죠.

어쨌든 문맥정보 $C$에 해당하는 자질 벡터의 각 요소를 모두 정하고 그 값 또한 구한 후 이를 위 수식의 입력값으로 해서 P(t\|C)를 최대로 만드는 다항로지스틱회귀의 파라메터($λ_i$)를 찾는 것이 이 모델의 학습 과정입니다. 예측은 어떻게 할까요? 학습 종료 후 예측을 하려면 우리가 품사 정보를 알고 싶은 '나'에 관련된 자질 벡터를 만들어주고 이를 다항로지스틱회귀 모델에 넣어서 가장 큰 확률값을 나타내주는 품사 t에 할당하면 됩니다. 예컨대 '나'의 자질벡터의 첫번째 요소는 1이겠네요. 직전에 등장하는 형태소가 어미(EC)이기 때문입니다. 이렇게 자질벡터를 입력값(X) 삼아 예측을 하는 구조입니다.

그런데 여기서 하나 문제가 있습니다. 학습과정에선 특정 문맥정보 C에 속한 모든 단어들의 품사가 이미 결정돼 있어서(정답 셋이 있으므로) 문제가 없었는데, 예측과정에선 이게 문제가 좀 됩니다. 위 예시 기준으로는 '올림픽', '이', '끝나' 등의 품사가 모두 정확하게 정해져 있지 않으면 '나'의 품사정보를 알기 어려워지기 때문입니다. 바꿔 말하면 예측해야 하는 데이터가 조금만 커져도 **애매성(ambiguity)**이 기하급수적으로 폭증하게 된다는 이야기입니다. 이 때문에 1990년대 **Beam search**와 같이 경우의 수를 줄이는 방법론들이 다양하게 제시됐습니다. 



## 은닉마코프모델

**마코프모델**은 실제로 관찰될 수 있는 사건(event)들의 **연쇄(chain)**에 대한 확률을 계산하는 데 유용합니다. 그러나 사건들이 실제로는 관찰될 수 없는 경우가 많습니다. 단어 연쇄를 생각해볼까요? '나는 어제 학교에 갔다'라는 문장은 네 단어들의 연쇄로 되어 있지만 각 단어들이 어떤 품사로 되어 있는지는 즉, 단어들의 품사 연쇄는 직접적으로 관찰되지 않습니다. 이 때문에 은닉마코프모델은 단어들의 품사 정보가 **은닉(hidden)**되어 있다고 전제합니다. 바꿔 말하면 위 예시에서 '올림픽이 끝나고 난 뒤에'라는 단어들의 연쇄보다 아래와 같은 품사들의 연쇄정보가 먼저 있고, 실제 발화시 이 품사 연쇄정보에 맞는 단어가 **방출(emission)**된다고 가정하는 것이지요. 

> NNG ==> JKS ==> VV ==> EC ==> VX ==> ETM ==> NNG ==> JKB

자, 그럼 HMMs에 따라 실제로 품사를 예측해 볼까요? 단어 X가 주어졌을 때 품사 Y가 나타날 확률을 최대화하는 Y를 예측결과로 내놓는 아래와 같은 식을 만족해야 합니다.

$$
\begin{align*}
argmax_{ Y }{ P(Y|X) }&=argmax_{ Y }\frac { P(X|Y)P(Y) }{ P(X) } \\ 
&=argmax_{ Y }P(X|Y)P(Y)
\end{align*}
$$

여기서 $P(Y)$는 **전이확률(transition probability)**로서 단어들의 연쇄에서 이전 단어의 품사가 등장했을 때 다음 품사 Y로 전이할 확률을 말합니다. 위 시작상태에서 명사(NNG), 명사(NNG)에서 조사(JKS), 조사에서 어미(EC)... 이렇게 등장하는 모든 경우의 수를 전체 말뭉치에서 세어서 표로 만들어 놓는 것이죠. 대략 아래와 같은 경우가 될 겁니다.

|  구분  | NNG  | ...  |  VV  |
| :--: | :--: | :--: | :--: |
| NNG  | 0.1  | ...  | 0.2  |
| ...  | ...  | ...  | ...  |
|  VV  | 0.2  | ...  | 0.3  |

위 표를 해석하면 이렇습니다. 명사(NNG)가 나오고 그 다음 단어가 또 명사가 나올 확률은 0.1입니다. 전이확률도 확률값이므로 각 행이나 각 열의 합계는 1이 됩니다. 이 확률값들은 다시 한번 말씀드리지만 해당 케이스의 빈도를 전체 단어수로 나누어서 구한 것입니다.

P(X\|Y)는 **방출확률(emission)**로서 품사 정보 Y가 주어졌을 때 단어 X가 나타날 확률입니다. 히든마코프모델의 가정은 품사 정보가 이미 있고, 여기에 맞는 단어가 방출된다는 내용이기 때문에 방출확률이라는 이름이 붙은 것 같습니다. 어쨌든 방출확률은 품사 Y 가운데 단어 X의 빈도를 전체 말뭉치에서 일일이 세어서 구합니다. 예컨대 위 예시 기준으로 설명드리면 동사(VV)로 쓰인 '나'(여기서 주의할 점은 대명사로 쓰인 '나'는 빈도 체크에서 제외)가 얼마나 쓰였는지 세어서 확률값을 구한다는 겁니다.

결과적으로 우리가 어떤 단어 X가 주어졌을 때 품사 Y일 확률을 알고 싶다고 할 때 이미 말뭉치로부터 구해놓은 전이확률과 방출확률들을 참고해 Y의 모든 경우의 수를 대입해 확률값들을 구합니다. 이 확률값들 중 가장 높은 확률을 내어주는 Y로 X의 품사 분류를 하게 됩니다.



## 마코프가정과 HMMs

일부러 따로 설명드리려고 빼놓은 중요한 개념이 있습니다. 바로 **마코프 가정(Markov assumption)**인데요, 히든마코프모델은 이 가정에 기초하고 있습니다. 러시아 수학자 마코프가 1913년경에 러시아어 문헌에 나오는 글자들의 순서에 관한 모델을 구축하기 위해 제안된 개념입니다. 마코프 가정은 한 상태의 확률은 단지 그 이전 상태에만 의존한다는 것이 핵심입니다. 즉 한 상태에서 다른 상태로의 **전이(transition)**는 그동안 상태 전이에 대한 긴 이력(history)을 필요로 하지 않고 바로 직전 상태에서의 전이로 추정할 수 있다는 이야기입니다. 마코프가정은 아래와 같이 도식화됩니다.

$$P({ q }_{ i }|{ q }_{ 1 },...,{ q }_{ i-1 })=P({ q }_{ i }|{ q }_{ i-1 })$$

그럼 이를 어떻게 이해할 수 있을까요? 한번 예를 들어보겠습니다. (품사로 예를 들어야 하는데 품사의 연쇄는 눈에 잘 들어오지 않아서 아래 문장으로 예시를 대체했습니다) '나는 어제 수학 공부를'라는 문장이 주어졌을 때 그 다음 단어가 '했다'가 나올 확률을 한번 계산해 보자는 겁니다. 그럼 아래와 같은 식이 될 겁니다. (C=괄호안에 있는 단어의 빈도)

$$P(했다|나는\quad어제\quad수학\quad공부를)=\frac { P(나는\quad어제\quad수학\quad공부를 \quad했다) }{ P(나는\quad 어제 \quad수학\quad공부를) } \quad $$

그런데 아시다시피 '나는 어제 수학 공부를'이라는 표현이 말뭉치에 정확하게 등장할 확률은 매우 낮을 겁니다. 단어들은 물론 그 표현도 매우 자유로운 게 자연언어의 속성이기 때문입니다. 하물며 '나는 어제 수학 공부를 했다'라는 표현은 어떻겠습니까. 따라서 위 식의 분자를 구하든 분모를 구하든 현실적인 대안으로 아래처럼 바로 다음에 나오는 단어들과의 연쇄로 처리를 합니다.

$$P(나는\quad어제\quad수학\quad공부를\quad했다)\approx P(나는)P(어제|나는)P(수학|나는\quad어제)P(공부를|나는\quad어제\quad수학)P(했다|나는\quad어제\quad수학\quad공부를)$$

그러나 위와 같은 방식으로도 연쇄가 길어질수록 그 연쇄가 나타날 가능성이 매우 희박해 집니다. 이럴 경우 연쇄를 마코프가정에 의해 아래와 같이 단순화하여 단어가 등장할 확률을 근사화할 수 있습니다. 아래와 같이 구하면 각각의 확률은 바로 위의 식보다는 한결 커지겠죠.

$$P(나는\quad어제\quad수학\quad공부를\quad했다)\approx P(나는)P(어제|나는)P(수학|어제)P(공부를|수학)P(했다|공부를)$$

실제로 히든마코프모델의 전이확률을 구할 때는 단어가 아니라 **품사정보**를 대상으로 확률을 계산하기 때문에 위 예시를 이해하실 때 약간 주의를 하셔야 합니다. 어쨌든 히든마코프모델의 전이확률은 위와 같은 원리를 바탕으로 직전 단어의 품사정보와 바로 다음 단어의 품사정보만을 비교해 계산을 하게 됩니다. 물론 이렇게 구한 전이확률마저도 대단히 작은 값을 지니기 때문에 **평탄화(smoothing)** 등 후처리 작업을 수행해야 한다고 합니다.



## 마치며

이상으로 최대엔트로피모델과 히든마코프모델에 대해 살펴보았습니다. 두 모델 모두 1990년대 제안되어서 활발히 연구되었던 기법들인데요, 요즘엔 딥러닝이 워낙 인기를 끌어서 그 관심이 좀 줄어든 감이 있기도 합니다. 하지만 모델 가정이라든지 접근방식 등에서 차용할 부분도 분명 있다고 생각이 되어서 추후 제 연구에 반영을 좀 해볼까 합니다. 솔직히 말씀드리면 두 모델에 대해서는 저도 완벽히 이해가 된 건 아닌데요, 제가 오해한 부분이 있거나 의견이 있으시면 언제든지 메일이나 댓글로 알려주시기 바랍니다. 여기까지 읽어주셔서 감사합니다.

