layout: post
title: 4. Training Models
category: Hands on Machine Learning
tag: Machine-Learning

 

## 1) 선형회귀 

- Error

  - MAE
  - RSS : 회귀에서 이 RSS는 Cost이며 w 변수 (회귀 계수)로 구성되는 RSS를 비용 함수라고도 함.

  $$
  RSS(w_0, w_1) = \frac{1}{N}\sum_{i=1}^N(y_i - (w_0 + w_1*x_i))^2
  $$

<br/>

## 2) 경사 하강법

그렇다면 어떻게 Cost Function이 최소가 되는 W 파라미터를 구할 수 있을까요? 경사 하강법은 고차원 방정식에서도 비용 함수의 값을 최소화 할 수 있다. 
$$
R(w) = \frac{1}{N}\sum_{i=1}^N(y_i - (w_0 + w_1x_i))^2
$$
$R(w)$ 를 $w_0, w_1$ 에 대해 미분하면, 다음과 같은 식이 도출된다.
$$
\frac{\delta R(w)}{\delta w_1} = \frac{2}{N}\sum_{i=1}^N-x_t*(y_i - (w_0 + w_1x_i)) = -\frac{2}{N}\sum_{i=1}^{N}x_i(실제값_i - 예측값_i)
$$

$$
\frac{\delta R(w)}{\delta w_0} = \frac{2}{N}\sum_{i=1}^N-(y_i - (w_0 + w_1x_i)) = -\frac{2}{N}\sum_{i=1}^{N}(실제값_i - 예측값_i)
$$

경사 하강법을 이용한 단순 선형 회귀는 $w_0, w_1$ 의 초기값을 모두 0으로 두고,
$$
w_1 = (이전의)w_1 - \eta\frac{2}{N}\sum_{i=1}^{N}x_i*(실제값_i - 예측값_i)
$$

$$
w_0 = (이전의)w_0 - \eta\frac{2}{N}\sum_{i=1}^{N}(실제값_i - 예측값_i)
$$

을 반복하여 계속해서 $w_1, w_0$ 을 업데이트 해나간다. 일반적인 경사 하강법은 위와 같이 하나하나 반복해나가므로, 데이터의 크기가 커질 경우 시간이 오래 걸린다는 단점이 있다. 때문에 실전에서는 대부분 확률적 경사 하강법(Stochastic Gradient Descent, SGD)을 이용합니다. 확률적 경사 하강법에서는 전체 데이터가 아닌 일부만 골라 $w_0, w_1$ 의 값을 업데이트 하므로 

<br/>

## 3) 다항 회귀



<br/>

## 4) 학습 곡선



<br/>

## 5) 규제가 있는 선형 모델



<br/>

## 6) 로지스틱 회귀

