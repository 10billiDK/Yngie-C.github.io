layout: post
title: 4. Training Models
category: Hands on Machine Learning
tag: Machine-Learning

 

## 1) 선형회귀 

**선형 회귀모델** : 일반적인 선형 회귀 모델은 다음과 같이 나타난다.
$$
\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
$$

- $\hat{y}$ : 예측값, $\quad n$ : 특성의 수,$\quad x_i$ : $i$ 번째 특성값
- $w_j$ : j번째 모델 파라미터 ($w_0$ : 편향,    $w_1, w_2, ... , w_n$ : 특성의 가중치)

위의 식을 벡터 형태로 나타내면
$$
\hat{y} = h_w(\bold{x}) =  w^T \sdot{\bold{x}}
$$

- $h_w$ : 가설 함수
- $w^T$ : 모델 파라미터 벡터
- $\bold{x}$ : 샘플의 특성 벡터

[2장]([https://yngie-c.github.io/hands%20on%20machine%20learning/2020/01/29/homl2/](https://yngie-c.github.io/hands on machine learning/2020/01/29/homl2/)) 에서 회귀모델에 가장 널리 사용되는 지표를 평균 제곱근 오차(RMSE)라고 하였음. 실제로는 RMSE와 같은 결과를 내면서 더 간단한 평균 제곱 오차(Mean Square Error, MSE)를 사용한다.
$$
MSE(\bold{X}, h_w) = \frac{1}{m}\sum_{i=1}^m( w^T\sdot\bold{x}^{(i)} - y^{(i)})^2
$$
**정규 방정식** : 비용 함수를 최소화 하는 해석적인 방법 (자세한 설명은 [이곳]([https://ko.wikipedia.org/wiki/%EC%B5%9C%EC%86%8C%EC%A0%9C%EA%B3%B1%EB%B2%95#%EC%84%A0%ED%98%95_%EC%B5%9C%EC%86%8C%EC%A0%9C%EA%B3%B1%EB%B2%95](https://ko.wikipedia.org/wiki/최소제곱법#선형_최소제곱법)) 을 참조하자)
$$
\hat{w} = (\bold{X}^T \sdot \bold{X})^{-1} \sdot \bold{X}^T \sdot \bold{y}
$$

- $\hat{w}$ 는 비용 함수를 최소화하는 $w$ 의 값
- $\bold{y}$ 는 $y_1$ 부터 $y_m$ 까지 포함하는 타깃 벡터

정규 방정식을 이용해 $\hat{w}$ 를 구해보자

```python
import
X_b = np.c_[np.ones((100, 1)), X]
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
```

정규 방정식의 계산 복잡도 : $(\bold{X}^T \sdot \bold{X})^{-1}$ 을 구하기 위한 계산 복잡도(Computational Complexity)는 샘플의 수 $m$ 에 대해서는 $O(m)$ , 특성 수 $n$ 에 대해서는 일반적으로 $O(n^{2.4})$ 에서 $O(n^{3})$ 사이를 나타낸다. 즉, 특성 수가 늘어날수록 정규 방정식을 이용한 방법은 계산 시간이 훨씬 더 오래 걸리게 된다.

<br/>

## 2) 경사 하강법

**경사 하강법** (Gradient Descent, GD) : 매우 일반적인 최적화 알고리즘. 비용 함수를 최소화하는 방향으로 반복해서 파라미터를 조정해 나간다. 구체적으로 보면 $w$ 를 임의의 값으로 시작하여 최솟값에 수렴할 때까지 점진적으로 향상시켜나감. 중요한 파라미터는 스텝의 크기. 이는 **학습률** (Learning rate) 하이퍼 파라미터로 결정한다. 학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로 시간이 오래 걸리게 되고, 학습률이 너무 크면 적절한 수렴점을 찾지 못할 수도 있다. 

무작위로 결정되는 시작점이 유발하는 경사 하강법의 2가지 문제점

- 지역 최솟값(local minimum)으로의 수렴 : 전역 최솟값(global minimum)이 아닌 지역 최솟값에 수렴할 수도 있다. 
- 평탄한 구간에서의 시간 지연 : 평탄한 구간을 지나게 되는 경우 시간이 오래 걸리고 모델이 일찍 멈추게 된다.

다행히 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택하여 선을 긋더라도 곡선을 가르지 않는 볼록 함수(Convex Function). 오직 하나의 전역 최솟값만을 가진다. 또한 연속된 함수이고 기울기가 갑자기 변하지 않음. [^1] 

특성의 스케일이 다른 경우에도 수렴하는데 시간이 오래 걸릴 수 있으므로 특성 스케일링을 통해 같은 스케일을 갖도록 만들어야 한다.

**배치 경사 하강법** : 경사 하강법을 구현하려면 모델의 각 파라미터 $w_j$ 에 대해서 비용 함수의 그래디언트를 계산. 이를 위해서는 **편도함수** (Partial Derivative)를 구해야 한다. 파라미터 $w_j$ 에 대한 비용 함수의 편도함수는 다음과 같다.
$$
\frac{\part}{\part w_j}MSE(w) = \frac{2}{m}\sum_{i=1}^{m}(w^T \sdot \bold{x}^{(i)} - y^{(i)})x^{(i)}_j
$$

편도함수는 아래의 식을 사용해 한꺼번에 계산할 수도 있다. [^2]
$$
\nabla_w MSE(w) = \left[\begin{array}{ccc} \frac{\part}{\part w_0}MSE(w) \\ \frac{\part}{\part w_1}MSE(w) \\ ... \\ \frac{\part}{\part w_n}MSE(w) \end{array}\right] = \frac{2}{m}\bold{X}^T \sdot (\bold{X} \sdot w - \bold{y})
$$
그래디언트 벡터가 구해지면 학습률($\eta$) 을 곱하여 기존의 $w$ 에서 빼주어 새로운 $w$ 를 구하는 방법을 계속 반복한다.
$$
w^{d+1} = w^d - \eta \nabla_w MSE(w)
$$
여기서 그래디언트 벡터에 곱해줄 적절한 학습률을 찾기 위해 그리드 탐색을 사용한다. 그리드 탐색에서 수렴하는데 너무 오래 걸리는 모델을 막기 위해 반복 횟수를 제한해야 한다. 반복 횟수가 너무 작으면  최적점에 도달하기 전에 알고리즘이 멈추고, 너무 크면 모델 파라미터가 변하지 않는 동안 시간을 낭비하게 된다.  그래서 일반적으로 반복 횟수는 크게 하되 그래디언트 벡터가 특정 값($\epsilon$)[^3]보다 작아지면 알고리즘을 중지하는 방법을 사용한다.

**확률적 경사 하강법** (Stochastic Gradient Descent) : 배치 경사 하강법은 매 스텝마다 전체 훈련 세트를 이용하여 그래디언트를 계산하므로 훈련 세트가 커지면 시간이 오래 걸린다는 단점이 있다. 반대로 확률적 경사 하강법은 매 스텝에서 딱 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 그래디언트를 계산한다. 반복마다 사용되는 데이터의 양이 적기 때문에 알고리즘이 훨씬 빠르다.

대신 배치 경사 하강법에 비해 훨씬 불안정하며, 알고리즘이 최솟값과 유사한 값에 도달하긴 하지만 최솟값에 도달하지 못한다. 이런 문제를 해결하기 위해 시작할 때는 학습률을 크게 하고 점차 작게 줄여서 전역 최솟값에 도달하게 하는 학습 스케줄(Learning Schedule) 함수를 사용한다. 일반적으로 한 반복에서 샘플의 개수($m$)만큼 되풀이 되고, 이때의 각 반복을 **에포크** (epoch)라고 한다. 사이킷런에서는 확률적 경사하강법을 위한 SGDRegressor() 클래스를 제공한다.

```python
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=50, penalty=None, eta0=0.1)
sgd_reg.fit(X, y.ravel())
```

**미니배치 경사 하강법** (Mini-batch Gradient Descent) : 각 스텝에서 전체 훈련 세트나 하나의 샘플을 기반으로 그래디언트를 계산하는 것이 아니라 미니배치라고 부르는 임의의 작은 샘플 세트에 대해 그래디언트를 계산하는 방법.

| 알고리즘             | 샘플 수(m)<br>이 클 때 | 특성 수(n)<br>이 클 때 | 외부 메모리<br>학습 지원 | 스케일<br>조정 필요 | 하이퍼<br>파라미터 수 | 사이킷런<br>클래스 |
| -------------------- | ---------------------- | ---------------------- | ------------------------ | ------------------- | --------------------- | ------------------ |
| 정규방정식           | 빠름                   | 느림                   | X                        | X                   | 0                     | LinearRegression   |
| 배치 경사 하강법     | 느림                   | 빠름                   | X                        | O                   | 2                     | N/A                |
| 확률적 경사 하강법   | 빠름                   | 빠름                   | O                        | O                   | >=2                   | SGDRegressor       |
| 미니배치 경사 하강법 | 빠름                   | 빠름                   | O                        | O                   | >=2                   | N/A                |

<br/>

## 3) 다항 회귀



<br/>

## 4) 학습 곡선



<br/>

## 5) 규제가 있는 선형 모델



<br/>

## 6) 로지스틱 회귀



[^1]: 이 함수의 도함수가 립시츠 연속(Lipschitz continuous). 립시츠 연속이란 어떤 함수의 도함수가 일정한 범위 안에서 변할 때 이 함수를 립시츠 연속 함수라고 한다. MSE는 x가 무한대일 때 기울기가 무한대가 되므로 국부적인(locally) 립시츠 함수. 
[^2]: 이 공식은 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 훈련한다. 때문에 이 알고리즘을 배치 경사 하강법(Batch Gradient Descent)이라고 한다. 배치 경사 하강법은 모든 데이터 셋을 사용하기 때문에 훈련 세트가 커질수록 매우 느려진다. 하지만 특성의 수에는 민감하지 않으므로 특성의 수가 많을 때는 정규 방정식보다 경사 하강법을 사용한다.
[^3]: 비용 함수의 모양에 따라 달라지겠지만 특정 값($\epsilon$) 의 범위 안에서 최적의 솔루션에 도달하기 위해서는 $O(1/\epsilon)$ 의 반복이 걸릴 수 있다. 즉,  $\epsilon$ 을 줄일수록 반복 횟수는 늘어나게 된다.  