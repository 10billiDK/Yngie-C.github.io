---
layout: post
title: SVM (Support Vector Machine)
category: Machine Learning
tag: Machine-Learning
---

 본 포스트는  [문일철 교수님의 인공지능 및 기계학습 개론 I](https://www.edwith.org/machinelearning1_17/joinLectures/9738) 강의를 바탕으로 작성하였습니다.



# SVM

## Decision Boundary

결정 경계는 서포트 벡터 머신에서도 여전히 중요하다. 분류기의 성능을 결정하기 때문이다. 이전에 있던 나이브 베이즈 분류기와 로지스틱 회귀에서는 확률을 기반으로 결정 경계를 결정했다. 그러다 보니 분류만 제대로 된다면 어떤 결정 경계를 선택하더라도 문제가 되지 않았다. 예를 들면 아래 그림에 있는 3가지 결정 경계(파랑, 빨강, 초록) 중에서 결정 경계를 파랑으로 선택하든 빨강으로 선택하든 나이브 베이즈와 로지스틱 회귀에서는 별 문제가 되지 않는다. 그러나 언뜻 보기에도 파랑보다는 빨간색으로 결정 경계를 나누는 것이 더 나아 보인다. 

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Svm_separating_hyperplanes.png/1024px-Svm_separating_hyperplanes.png" alt="SVM_DeciBound" style="zoom: 33%;" />

<p align="center" style="font-size:80%">이미지 출처 : 위키백과</p>

**서포트 벡터 머신(Support Vector Machine, SVM)**은 위 그림에서의 빨간색으로 나타나는 결정 경계를 찾기 위한 방법이다. SVM이 결정 경계를 찾는 방식 다음과 같다. 먼저, 각 클래스에서 가장 앞(결정 경계 가까이)에 위치한 3개의 벡터를 찾는다. 3개를 선정하는 방법은 각 클래스에서 가장 앞에 있는 벡터를 하나씩 찾는다. 그 다음으로 앞에 위치한 벡터 하나를 클래스에 상관없이 찾는다. 이렇게 선정된 3개의 벡터 중 한 개 클래스에 속한 두 벡터를 찾아 그들을 잇는 하나의 직선을 긋는다. 다음으로 나머지 클래스에 위치한 하나의 벡터를 지나면서 이전 직선에 평행한 직선을 하나 더 긋는다. 마지막으로 평행선 위의 아무 점의 중점을 지나며 평행선과 평행한 하나의 직선을 그을 수 있는데 이 직선이 SVM의 결정 경계이다. 

<img src="https://upload.wikimedia.org/wikipedia/commons/2/2a/Svm_max_sep_hyperplane_with_margin.png" alt="SVM_DB2" style="zoom:40%;" />

<p align="center" style="font-size:80%">이미지 출처 : 위키백과</p>

## Margin

이렇게 결정된 결정 경계에서 양쪽 직선까지의 거리를 **Margin(마진)** 이라고 한다. 결정 경계는 위 그림에서 볼 수 있듯 $\mathbf{w} \cdot \mathbf{x} + b = 0$ 으로 나타낼 수 있다. 새로운 데이터가 들어올 경우 결정 경계를 기준으로 $\mathbf{w} \cdot \mathbf{x} + b > 0$ 인지 $\mathbf{w} \cdot \mathbf{x} + b < 0$ 인지에 따라서 클래스를 결정한다. 위 그림에서는 $\mathbf{w} \cdot \mathbf{x} + b > 0$ 이면 검은색으로 분류되고 $\mathbf{w} \cdot \mathbf{x} + b < 0$ 이면 흰색으로 분류된다.

$\mathbf{w} \cdot \mathbf{x} + b > 0$ 인 경우를 Positive case $y_i (> 0)$ , $\mathbf{w} \cdot \mathbf{x} + b < 0$ 인 경우를 Negative case $y_i (< 0)$ 라고 하자. 이 때 $(\mathbf{w} \cdot \mathbf{x} + b) y_i$를 신뢰도(Confidence level)라고 하며 이 신뢰도를 최대화 하는 것이 SVM에서 우리의 목표가 된다. 신뢰도에서 $y_i$ 값은 어떤 결정 경계를 택하더라도 변화가 없으므로 $\mathbf{w} \cdot \mathbf{x} + b$ 으로 나타나는 마진을 최대화하는 것이 중요하다. 결정 경계 위에 있는 점을 $x_p$ 라 하면 임의의 점 $x$ 는 거리 $r$ 을 사용하여 다음과 같이 나타낼 수 있다.

 

$$
x = x_p + r \frac{w}{||w||}, f(x_p) = 0
$$



이를 사용하여 다시 $f(x)$ 를 나타내면 아래와 같고 이를 r에 대한 식으로 정리할 수 있다. 



$$
f(x) = w \cdot x + b = w \cdot (x_p + r \frac{w}{||w||}) + b = wx_p + b + r\frac{w \cdot w}{||w||} \\
\because wx_p + b = 0, \quad f(x) = r\frac{w \cdot w}{||w||} = r\frac{||w||^2}{||w||} = r ||w|| \\
\therefore r = \frac{f(x)}{||w||}
$$



좋은 결정 경계를 위해서는 Margin( $r$ ) 을 최대로 늘려야 하는데 이를 식으로 나타내면 다음과 같다. 첫 번째 식에서 $a$ 는 임의의 상수이므로 이를 일반화하여 1로 나타내고 $||w||$ 에 대한 식으로 정리하면 두 번째 식처럼 나타낼 수 있다.



$$
\max_{w,b} 2r = \frac{2a}{||w||} \qquad s.t. (wx_j + b)y_j \geq a \\
\min_{w,b} ||w|| \qquad s.t. (wx_j + b)y_j \geq 1
$$



위에서 $||w||$ 를 최소화 하는 것은 Quadratic optimization이다. Linear programming, Quadratic programming에 대한 자료는 [Wikidocs : 모두를 위한 컨벡스 최적화](https://wikidocs.net/17852) 를 참고하면 좋다.



# Error Handling

## Losses

<img src="https://www.researchgate.net/profile/Marimuthu_Palaniswami/publication/220606206/figure/fig2/AS:339477512900611@1457949154899/The-soft-margin-SVM-classifier-with-slack-variables-x-and-support-vectors-shown.png" alt="soft_SVM" style="zoom:80%;" />

<p align="center" style="font-size:80%">이미지 출처 : Researchgate</p>

위 그림처럼 직선으로 결정 경계를 나누기 어려운 경우도 있다. 실선으로 나타낸 결정 경계는 빨강 클래스 사이에 위치한 파랑 클래스 데이터를 잘못 분류하게 된다. 이 경우에 에러를 처리하는 방법은 크게 두 가지다. 첫 번째는 Error case에 벌칙을 주는(Panelization) 방법이다. 이 경우 에러와 관련된 항을 식에 추가해 준다. 두 번째는 non-linear한 결정 경계를 긋는 방법이다. 먼저 첫 번째(에러를 식에 포함하여 처리하는) 방법에 대해 알아보도록 하자.

에러를 처리하는 방법에도 두 가지 방법이 있다. 첫 번째는 발생하는 에러 데이터의 개수를 세는 방법이다. 이를 식으로 나타내면 아래와 같다. 이 때 계산되는 Loss $\#_{error}$ 를 0-1 Loss 라고 한다.



$$
\min_{w,b} ||w|| + C \times \#_{error} \quad s.t.(wx_j + b) \geq 1
$$



두 번째 방법은 Hinge Loss를 사용하는 것이다. 이를 나타내는 새로운 파라미터 $\xi_i$ 가 등장한다. Hinge Loss $(\xi_i)$ 는 원래 클래스의 최전선까지는 0이며 이를 지나면 서서히 증가한다. 결정 경계를 지날 때 $\xi_i = 1$ 이며 거리가 멀어질 수록 두 점((원래 클래스의 최전선, 0), (결정 경계, 1))을 잇는 직선을 따라 서서히 증가한다. 두 번째 방법을 식으로 나타내면 다음과 같다.



$$
\min_{w,b} ||w|| + C \sum_j \xi_j \quad s.t.(wx_j + b) \geq 1 - \xi_j
$$



실제로는 두 방법 중 아래 방법을 더 많이 사용한다. 0-1 Loss는 Quadratic으로 구현하기 어려우며 에러 케이스의 에러 정도를 반영하지 못하기 때문이다. 그리고 각 식에서 $C$ 라는 하이퍼 파라미터가 등장한다. 



## Soft Margin SVM

이렇게 에러를 고려하는 SVM 모델을 Soft margin SVM 이라고 한다. 반대로 하나의 오차도 허용하지 않는 모델을 Hard margin SVM이라고 한다. 아래 그림을 보면 둘의 차이를 구분할 수 있다.

![](https://miro.medium.com/max/1104/1*CD08yESKvYgyM7pJhCnQeQ.png)

<p align="center" style="font-size:80%">이미지 출처 : <a herf="https://medium.com/@ankitnitjsr13/math-behind-svm-support-vector-machine-864e58977fdb">Medium</a></p>

꼭 한 클래스의 데이터가 다른 클래스 데이터 사이에 들어가 있지 않은 경우에도 Soft margin SVM을 사용해볼 수 있다. 이 경우에는 에러에 대해 Robust 하다는 점에서 Soft margin SVM의 특징을 찾을 수 있다. 아래의 왼쪽과 오른쪽 그림은 데이터를 각각 Soft margin SVM 과 Hard margin SVM 으로 분류한 것이다.

![](https://miro.medium.com/max/898/1*A3MIALgW3xdCGuuLNqpCEA.png)

<p align="center" style="font-size:80%">이미지 출처 : <a herf="https://medium.com/@ankitnitjsr13/math-behind-svm-support-vector-machine-864e58977fdb">Medium</a></p>

다시 Soft margin SVM의 Loss function으로 돌아가보자. Hinge loss를 사용하는 Soft margin SVM의 Loss function은 아래와 같다.



$$
\min_{w,b, \xi_j} ||w|| + C \sum_j \xi_j \quad s.t.(wx_j + b) \geq 1 - \xi_j
$$



이 식에서 하이퍼 파라미터 $C$ 값이 커지면 커질수록 $\xi_j$ , 즉 Error term 이 확대된다. 즉 C가 크면 클수록 Error에 대한 Panelty가 더욱 크게 나타나며 모델이 에러를 발생시키지 않는 방향으로 생성된다. 아래 그림은 $C=1000 , C=10, C=0.1$ 일 때의 결정 경계를 보여주고 있다. 얼핏 보면 C가 늘어날수록 올바른 결정 경계가 생기는 것처럼 보이지만 항상 그런 것은 아니니 여러 $C$ 값에 의해서 생성되는 모델들을 잘 비교해야 한다. 

![](https://i.stack.imgur.com/8va0T.png)

<p align="center" style="font-size:80%">이미지 출처 : <a herf="https://datascience.stackexchange.com/questions/5717/where-is-the-cost-parameter-c-in-the-rbf-kernel-in-svm">Stackexchange</a></p>



# Kernal Trick

이번에는 non-linear한 결정 경계를 만드는 방법에 대해서 알아보자. 에러를 허용하는 분류기 모델은 하나의 대안일 뿐 데이터를 잘 표현하는 모델이 될 수는 없다. 이런 경우 데이터의 차원을 확장하면 non-linear한 데이터를 나타낼 수 있다. 차원을 높이는 기본적인 아이디어는 선형 회귀 모델에서 보았던 Polynomial regression(다항 회귀) 모델에서  비슷하다. 특성끼리 곱한 것을 새로운 특성으로 만들어 줌으로서 데이터셋의 차원을 높일 수 있다.

하지만 SVM에서는 이렇게 마구잡이로 차원을 늘리는 방법을 사용하면 너무 많은 컴퓨팅 자원을 필요로 하게 된다. 이럴 때 사용하는 것이 커널 트릭(Kernel Trick) 이다. 커널 트릭을 사용하면 계산량을 늘리지 않고도 Non-linear한 결정 경계를 그려낼 수 있다.

## Kernel Trick

이전에도 이야기 했듯 SVM 분류기를 만드는 문제는 Constrained Quadratic Programming 이다. 주어진 Convex 문제에 대해서 Lower Bound를 찾는 Duality를 사용한다. Duality에 대한 설명은 [Wikidocs : 모두를 위한 컨벡스 최적화](https://wikidocs.net/19932) 와 [Ratsgo님 블로그](https://ratsgo.github.io/convex optimization/2018/01/25/duality/) 에서 살펴볼 수 있다. 우리의 문제는 Constrained optimization(조건부 최적화) 이므로 다음과 같은 조건을 가지는 $g(x), h(x)$ 에 따라서 **라그랑주법(Lagrange method)** 을 적용할 수 있다.



$$
\min_x f(x) \qquad s.t. \quad g(x) \leq 0 , h(x) = 0 \\
\text{Lagrange Prime Function : } L(x, \alpha, \beta) = f(x) + \alpha g(x) + \beta h(x) \qquad \\
\text{Lagrange Multiplier : } \alpha \geq 0, \beta \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \\
\text{Lagrange Dual Function : } d(\alpha, \beta) = \inf_{x \in X} L(x, \alpha, \beta) = \min_x L(x, \alpha, \beta) \\
\max_{\alpha \geq 0, \beta} L(x, \alpha, \beta) = \begin{cases} f(x) : \text{if x is feasible} \\ \infty : \text{otherwise} \end{cases} \\
\therefore \min_x f(x) \rightarrow \min_x\max_{\alpha \geq 0, \beta} L(x, \alpha, \beta)
$$


$$
\text{Primal Problem : } \min_x f(x) \quad s.t. g(x) \leq 0, h(x) = 0 \rightarrow \min_x \max_{\alpha \geq 0, \beta} L(x, \alpha, \beta) \\
\text{Lagrange Dual Problem : } \max_{\alpha \geq 0, \beta} d(\alpha, \beta) \quad s.t. \alpha > 0 \rightarrow \max_{\alpha \geq 0, \beta} \min_x L(x, \alpha, \beta)
$$


Duality theorem(쌍대성 이론) 중에서 Strong duality는 KKT(Karush-Kunh-Tucker) 조건을 만족할 때 Primal과 Dual problem이 같다고 나타난다는 이론이다. KKT 조건은 그 아래 제시되어 있다.


$$
d^* = \max_{\alpha \geq 0, \beta} \min_x L(x, \alpha, \beta) = \min_x \max_{\alpha \geq 0, \beta} L(x, \alpha, \beta) = p^* \\
\text{KKT Condition} : \nabla L(x^*,\alpha^*,\beta^*) = 0, \alpha^*=0, g(x^*) \leq 0, h(x^*) = 0, \alpha^*g(x^*) = 0
$$


## Dual Problem of SVM

원래의 SVM 식으로 다시 돌아와보자.


$$
\min_{w,b} ||w|| \qquad s.t. (wx_j + b)y_j \geq 1
$$


이에 $L(w, b, \alpha) = \frac{1}{2}w \cdot w - \sum_j \alpha_j[(wx_j + b)y_j -1]$ 임을 이용하여 Primal Problem 식으로 만들면 다음과 같이 나타낼 수 있다.


$$
\min_{w,b} \max_{\alpha \geq 0, \beta} \frac{1}{2}w \cdot w - \sum_j \alpha_j[(wx_j + b)y_j -1]
$$


이 Primal 문제를 Dual problem으로 바꿔줄 수 있다. 아래는 Dual problem으로 바꾸어준 SVM의 목적식이고 그 아래는 이 때의 KKT 조건을 나타낸 것이다.


$$
\max_{\alpha \geq 0} \min_{w,b} \frac{1}{2}w \cdot w - \sum_j \alpha_j[(wx_j + b)y_j -1] \\
\frac{\partial w,b, \alpha}{\partial w} = 0 , \frac{\partial w,b, \alpha}{\partial b} = 0, \alpha_i \geq 0, \alpha_j((wx_j + b)y_j -1) = 0
$$


이제 $L(w, b, \alpha)$ 을 전개하여 나타내보자.


$$
L(w, b, \alpha) = \frac{1}{2}w \cdot w - \sum_j \alpha_j[(wx_j + b)y_j -1] \qquad \qquad \qquad \qquad \quad \\
= \frac{1}{2}w \cdot w - \sum_j \alpha_jy_jwx_j - b \sum_j a_jy_j + \sum_j \alpha_j \\
= \sum_j \alpha_j - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_j y_i y_j x_i x_j \qquad \qquad \quad
$$


이렇게 다시금 Quadratic Programming 문제가 되었다. (아 커널 핸즈온에 이어서 두번째로 보는데 다시 공부해도 어렵다. 컨벡스 최적화랑 쌍대성 자세히 보고 다음에 다시 써야겠다)
