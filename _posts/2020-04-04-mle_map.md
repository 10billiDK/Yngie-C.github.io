---
layout: post
title: MLE & MAE
category: Machine Learning
tag: Machine-Learning
---

본 포스트는 [카이스트 문일철 교수님의 강의](https://www.edwith.org/machinelearning1_17/joinLectures/9738) 를 바탕으로 작성하였습니다.



# MLE

## Binomial Distribution(이항 분포)

압정을 던져 머리부분(*Head*)과 꼬리부분(*Tail*)이 나오는 게임을 한다고 해보자. 

던지는 행위는 연속적이지 않으므로(던지는 횟수는 1,2,3,4... 이지 1.5, 3.34 일 수 없음) 이 게임은 **Discrete probability distribution(이산 확률 분포)** 이다. 게임에서 발생할 수 있는 사건은 *Head, Tail* 의 2가지 경우 뿐이다. 이렇게 사건이 2가지(0과 1, 앞과 뒤, 성공과 실패 등)로 나타나는 실험을 가리켜 Bernoulli trial(베르누이 시행)이라고도 부른다.

던지는 행위는 **i.i.d(independent and identical distributed, 독립 항등 분포)** 이다. i.i.d는 Independent(독립적)이며 Identical distributed(동일한 확률 분포)를 가지는 것을 말한다. 압정을 던지는 행위는 먼저 던진 사건이 이후 사건에 영향을 주지 않고(독립적) 사건이 고유한 정규분포를 따르므로(동일한 확률 분포) i.i.d이다.

압정을 던져 머리부분이 나오는 확률을 $P(H)$ 라 하고, 꼬리부분이 나오는 확률을 $P(T)$ 라고 하자. $P(H) = \theta$ , $ P(T) = 1 - \theta$  라면, $HHTHT$ 의 경우가 발생할 경우는 아래와 같이 나타낼 수 있다.

<br>
$$
\\
P(HHTHT) = \theta^3 (1 - \theta)^2 \\
\text{일반화하면 } P(D\vert \theta) = \theta^{a_H}(1 - \theta)^{a_T} \\
D : \text{발생한 데이터} \quad (a_H, a_T) : D \text{ 에서 H와 T가 발생한 횟수} \\
P(D\vert \theta) \text{ 는 } \theta \text{ 가 주어졌을 때 D(data)가 관측될 확률}
$$

<br>
$$
n=5 \text{ (총 시도 횟수)} \quad k=a_H=3 \text{ (하나의 사건이 발생한 횟수)} \quad p=\theta \text{ (확률)} \\ 
f(k;n,p) = P(K=k) = \left(\begin{array}{c}n\\k\end{array}\right) p^k (1-p)^{n-k}  \quad \text{여기서, }\left(\begin{array}{c}n\\k\end{array}\right) = \frac{n!}{k!(n-k)!}
$$



## Maximum Likelihood Estimation(최대 우도 추g정)

압정 실험이 $p(H) = \theta$ 인 Binomial distribution을 따른다는 가설을 세워보자. 이 가설이 맞다는 것을 강화하려면 어떻게 해야 할까? 정해지지 않은 $\theta$ 의 가장 적절한( $P(D \vert \theta)$ 를 최대로 하는) 값을 찾아내면 될 것이다. 이 때의 $\theta$ 값을 **MLE(Maximum Likelihood Estimation, 최대 우도 추정)** 라고 한다. 수식으로는 아래와 같이 나타낼 수 있다. 

<br>
$$
\hat{\theta} = \text{argmax}_{\theta} P(D \vert \theta)
$$

최대 우도를 찾는 과정은 다음과 같다. $P(D \vert \theta)$ 식을 끼워넣고 로그 함수를 취해주어 함수를 단순화 한다. 로그 함수( $\ln$ )는 $[0,1]$ 에서 단조 증가 함수[^1]이므로, $P(D \vert \theta)$ 를 최대로 하는 $\theta$ 와 $\ln P(D \vert \theta)$ 는 $\theta$ 는 같다. 

<br>
$$
\hat{\theta} = \text{argmax}_{\theta} P(D \vert \theta) = \text{argmax}_{\theta} \theta^{a_H}(1 - \theta)^{a_T} \\
\hat{\theta} = \text{argmax}_{\theta} \ln P(D \vert \theta) = \text{argmax}_{\theta} \{ a_H \ln \theta + a_T \ln (1 - \theta) \}
$$

로그를 취하여 단순화 해준 함수를 미분하여 최댓값을 구할 수 있다. 미분값을 0으로 하는 $\theta$ 가 $\ln P(D \vert \theta)$ 함수 최댓값의 $\theta$ 이다.

<br>
$$
\frac{d}{d\theta}(a_H \ln \theta + a_T \ln (1 - \theta)) = \frac{a_H}{\theta} - \frac{a_T}{1 - \theta} = 0 \\
\theta = \frac{a_H}{a_H + a_T} \\
\therefore \quad \hat{\theta}_{MLE} = \frac{a_H}{a_H + a_T}
$$


## Simple Error Bound(오차 범위)

위 식에서 $a_H$ 와 $a_T$ 의 비율만 지켜진다면 최대 우도는 전체 시도 횟수 $(n)$ 에는 영향을 받지 않는다. 예를 들어, 압정 던지기 게임에서 5번 던져서 *Head* 가 3번, *Tail* 이 2번 나오는 경우가 있다고 하자. 그리고 50번 던져서 *Head* 가 30번, *Tail* 이 20번 나오는 경우가 있다고 하자. 두 경우 모두 최대 우도는 $0.6 = 3/(3+2) = 30/(30+20)$ 이다. 그렇다면 많이 던지는 것은 확률에 있어서 아무런 효과가 없는 것일까?

결론부터 말하자면 아니다. 우리가 구했던 최대 우도는 사건이 발생할 실제 확률이 아니다. 실제 확률을 $\theta^*$ 라 하면 둘의 차이 $\vert \hat{\theta} - \theta^\* \vert$ 를 오차라고 한다. 오차의 범위는 아래와 같이 나타낼 수 있다.

<br>
$$
P(\vert \hat{\theta} - \theta^* \vert \geq \epsilon ) \leq 2e^{-2N \epsilon^2} \\
N = a_H + a_T
$$
여기서 $N$ 이 증가하면 $2e^{-2N \epsilon^2}$ , 즉 오차 범위(Error bound)가 줄어들게 된다. $\epsilon$ 이 주어진 상태라면 N을 높여 일정 확률(낮은 확률) 이하로 오차 범위를 낮출 수 있다. 이런 학습 방식을 **PAC(Probably Approximate Correct) learning** 이라고 한다. 높은 확률로(Probably) 낮은 오차 범위를(Approximately Correct) 가지도록 하는 것이 PAC learning의 목적이다.



# MAP

## Incorporating Prior Knowledge

MLE 말고도 확률을 바라보는 다른 관점이 있다. MLE는 우리가 알고 있는 사전 지식을 확률에 반영하지 않는다. 새로운 관점은 확률 계산에 사전 지식 $P(\theta )$ 을 넣는다. 그렇게 계산되어 나오는 확률은 MLE에서 계산했던 $P(D \vert \theta)$ 가 아닌 $P(\theta \vert D)$ 이다. MLE에서는 주어진 $\theta$ 에 대해서 $D$ 가 발생할 확률을 구했다면, **MAP(Maximum a Posterior Estimation)** 에서는 주어진 $D$ 에 대해서 $\theta$ 인 확률, 즉 사후 확률(Posterior)을 구한다.

<br>
$$
P(\theta \vert D) = \frac{P(D \vert \theta) P(\theta)}{P(D)} \\
\text{Posterior} = \frac{\text{Likelihood}\times \text{Prior Knowledge}}{\text{Normalizing Constant}}
$$


## Bayes Viewpoint

위 식에서 $P(D)$ 는 상수이므로 $P(\theta \vert D) \propto P(D \vert \theta) P(\theta)$ 로 나타낼 수 있다. 여기서 $P(D \vert \theta) = \theta^{a_H}(1 - \theta)^{a_T}$ 로 나타낼 수 있다. 사전 지식인 $P(\theta)$ 는 Beta distribution(베타 분포)를 사용해서 구할 수 있다.

<br>
$$
P(\theta) = \frac{\theta^{\alpha-1}(1 - \theta)^{\beta-1}}{B(\alpha, \beta)} \\
B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)} ,\quad \Gamma(\alpha) = (\alpha-1)!
$$


## Maximum a Posteriori Estimation(최대 사후 확률 추정)

위에서 나온 식을 같은 밑끼리 정리하면 아래와 같이 쓸 수 있다. 

<br>
$$
P(\theta \vert D) \propto P(D \vert \theta) P(\theta) =  \theta^{a_H}(1 - \theta)^{a_T} \cdot \theta^{\alpha-1}(1 - \theta)^{\beta-1} = \theta^{a_H + \alpha -1}(1 - \theta)^{a_T +\beta-1}
$$
MLE에서 $P(D \vert \theta) = theta^{a_H}(1 - \theta)^{a_T}$ 를 최대화하는 $\hat{\theta} = \frac{a_H}{a_H + a_T}$ 임을 이용하여  $P(\theta \vert D)$ 를 최대화하는 $\hat{\theta}$ 의 값도 나타낼 수 있다.

<br>
$$
P(\theta \vert D) \propto \theta^{a_H + \alpha -1}(1 - \theta)^{a_T +\beta-1} \\
\hat{\theta}_{MAP} = \frac{a_H + \alpha -1}{a_H + \alpha + a_T +\beta - 2}
$$


MLE와 MAP 방식으로 구해진 $\hat{\theta}$ 은 $\alpha=\beta=1$ 인 경우를 제외하고는 서로 같지 않다. 하지만 $N$ 이 커질수록(시도가 늘어날수록) $a_H , a_T$ 가 커지게 되어 점점 비슷해지게 된다. 



[^1]: 어떤 수열이나 함수가 있을 때, 해당 수열이나 함수가 정의된 구간에서 감소하지 않는 경우를 단조증가, 증가하지 않는 경우를 단조감소 함수라고 한다. 위 그림에서 오른쪽 그래프의 경우 중간에 감소하는 구간이 존재하므로 해당 그래프는 단조증가 함수가 아니나, 왼쪽의 그래프의 경우 감소하는 구간이 존재하지 않으므로 단조증가 함수라고 할 수 있다. 결국 감소하는 구간이 없는 경우란 값이 일정하거나 증가하는 구간밖에 없는 경우가 된다. 이 단조증가에서 모든 구간에서 항상 수치가 증가하는 경우를 ‘강한 단조증가’라고 하기도 한다. 출처 : 과학포털 사이언스올