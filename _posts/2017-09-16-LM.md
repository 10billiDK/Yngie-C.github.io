---
title: 언어모델(Language Model)
category: From frequency to semantics
tag: Language Model
---

이번 글에서는 **통계적 언어모델(Statistical Language Model, 언어모델)**에 대해 살펴보도록 하겠습니다. 이 글은 고려대 정순영 교수님 강의를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.





## 정의

언어모델이란 단어 시퀀스에 대한 **확률분포(probability distribution)**를 가리킵니다. 언어모델은 $m$개 단어가 주어졌을 때 $m$개 단어 시퀀스가 나타날 확률, 즉 $P(w_1, w_2, ..., w_m)$을 할당(assign)합니다. 예컨대 다음과 같습니다.

> (1) $P(Today\ is\ Wednesday)=0.001$
>
> (2) $P(Today\ Wednesday\ is)=0.0000000001$

언어모델은 **context-dependent** 성격을 지닙니다. 일상적인 대화 말뭉치로 언어모델을 구축했다면 일상 대화가, 수학 컨퍼런스의 말뭉치로 언어모델을 구축했다면 수식 표현이 나타날 확률이 클 겁니다. 다시 말해 언어모델은 학습데이터에 민감합니다.





## 언어모델의 이점

자연언어에는 기본적으로 불확실성(uncertainties)이 존재합니다. 그런데 언어모델은 이러한 불확실성을 단어 시퀀스의 출현 확률로 정량화(quantify)할 수 있는 장점을 가집니다. 예컨대 언어모델의 힘을 빌리면 앞선 예제에서 **(1)이 나타날 확률은 (2)보다 $10^7$배 크다**고 '숫자'로 말할 수 있게 됩니다.

언어모델이 주어지면, 우리는 확률분포를 가지고 단어의 시퀀스를 뽑을 수(sample) 있습니다. 다시 말해 해당 언어모델로 텍스트를 생성(generation)해낼 수 있다는 뜻입니다. 이런 취지에서 언어모델은 종종 **생성모델(generative model)**이라고도 불리는데요. 

예컨대 'John'이라는 단어와 'feels'라는 단어가 주어졌다고 칩시다. 그러면 그 다음 단어는 'happy'일 가능성이 높을까요? 아니면 'habit'일 확률이 클까요? 사실 'happy'와 'habit'은 말소리가 비슷하지만, 언어모델의 힘을 빌리면 그 확률이 높은 'happy'를 뽑게 됩니다. 응답(answer) 생성에 도움이 된다는 이야기입니다.





## 유니그램 언어모델

가장 단순한 언어모델은 **유니그램 언어모델(unigram language model, 유니그램)**입니다. 각 단어가 서로 독립(independent)이라고 가정합니다. $n$개 단어가 동시에 나타날 확률은 다음과 같습니다.



$$
P\left( { w }_{ 1 },{ w }_{ 2 },...,{ w }_{ n } \right) =\prod _{ i=1 }^{ n }{ P\left( { w }_{ i } \right)  }
$$


유니그램에서는 단어의 시퀀스를 고려한다기보다는 단어 셋(set)을 상정한다는 것이 더 정확한 표현입니다. 단어 시퀀스의 등장확률이 각 단어 발생확률의 곱으로 정의돼 있기 때문입니다. 다시 말해 각 단어의 등장 순서가 바뀌어도 개별 단어 확률의 곱은 변하지 않는다는 이야기입니다. 

유니그램 언어모델은 다음과 같은 테이블로 구성됩니다. 학습말뭉치에 등장한 각 단어 빈도를 세어서 전체 단어수로 나누어준 것입니다. 물론 확률의 총합은 1이 됩니다.

|   단어 $w$    | 확률 $P(w$\|$θ_2)$ |
| :---------: | :--------------: |
|    text     |       0.2        |
|   mining    |       0.1        |
| association |       0.01       |
| clustering  |       0.02       |
|     ...     |       ...        |
|    food     |     0.00001      |
|    total    |        1         |

텍스트마이닝 논문 말뭉치로 학습한, 위와 같은 유니그램 모델 $θ_1$이 주어진 상황에서 'text'와 'mining', 'clustering'이라는 세 개 단어로 구성된 첫번째 문서 $D$의 출현확률을 구해보겠습니다.



$$
\begin{align*}
P\left( D|{ \theta  }_{ 1 } \right) &=P\left( text\quad mining\quad clustering|{ \theta  }_{ 1 } \right) \\ &=P\left( text|{ \theta  }_{ 1 } \right) \times P\left( mining|{ \theta  }_{ 1 } \right) \times P\left( clustring|{ \theta  }_{ 1 } \right) \\ &=0.2\times 0.1\times 0.02=0.0004
\end{align*}
$$
유니그램 모델에서는 말뭉치 등장 빈도가 높은 단어가 많이 포함된 문서일 수록 해당 문서의 출현확률이 높아집니다. 바꿔 말해 등장빈도 높은 단어를 해당 문서의 주제(topic)으로 볼 여지가 있다는 얘기입니다. 위 예시에선 'text'를 $D$의 주제로 볼 수도 있습니다. 아울러 당연한 이야기겠지만, 만일 다른 단어로 구성된 문서가 존재한다면 이 유니그램 모델은 해당 문서에 다른 확률을 할당하게 될 겁니다. 

이번에는 건강/식이요법 관련 논문 말뭉치로 학습한, 유니그램 모델 $θ_2$가 아래처럼 주어졌다고 가정해 보겠습니다. 그렇다면 $P(D$\|$θ_1)>P(D$\|$θ_2)$일 겁니다. 같은 단어로 구성된 문서라도 모델이 다르면 그 확률값이 크게 달라지게 됩니다(context-dependent).

|  단어 $w$   | 확률 $P(w$\|$θ_2)$ |
| :-------: | :--------------: |
|   food    |       0.25       |
| nutrition |       0.1        |
|  healthy  |       0.05       |
|   diet    |       0.02       |
|    ...    |       ...        |
|   text    |     0.00001      |
|   total   |        1         |





## 최대우도추정

