---
layout: post
title: ELMo (Embeddings from Language Models)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# ELMo

<p align="center"><img src="https://pbs.twimg.com/profile_banners/962197608/1579878926/1500x500" alt="elmo_twitter" style="zoom: 50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://twitter.com/elmo">Twitter - Elmo</a></p>

2018년 6월 발표된 ELMo(Embeddings from Language Models), 말 그대로 언어 모델로부터 만들어진 임베딩이다. 이 ELMo를 시작으로 다양한 Pretrained Model이 등장하게 된다. ELMo가 나오게 된 이유는 좋은 표현(Representation)을 만들기 위함이다.

좋은 표현은 두 가지를 모델링 할 수 있어야 한다. 첫 번째는 단어의 복잡한 특성을 만족시킬 수 있어야 한다. 구문(Syntax) 분석 관점에서 사용하든 의미(Semantic) 분석 관점에서 사용하든 해당 단어가 어떻게 사용되는 지를 잘 표현해야 좋은 표현이라 할 수 있다. 두 번째는 언어학적으로 문맥에 맞는 표현을 할 수 있어야 한다. 예를 들어, 다의어(Polysemy) 같은 경우 사람의 '눈(Eye)'과 내리는 '눈-(Snow)'이 있을 때 주변 문맥으로부터 이를 구분해내야 한다.

GloVe도 문서에 등장하는 단어를 고려한 모델이었는데 GloVe에 비해 ELMo가 갖는 장점은 무엇이 있을까? 아래의 예시를 보자.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86380931-aa4e6680-bcc7-11ea-8810-98ef965b7452.png" alt="elmo1" style="zoom:50%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

위 그림에서 볼 수 있듯 GloVe에서 주어진 *"play"* 라는 단어와 가장 가까운 단어를 살펴보면 주로 스포츠와 관련된 단어들만 있는 것을 알 수 있다. 하지만 ELMo의 경우에는 *"play"* 가 어떤 의미로 사용되는지에 따라서 가장 가까운 단어의 성격이 바뀌는 것을 볼 수 있다. 주어진 표에서 맨 마지막 *"play"* 는 연극의 의미로 사용되었는데 ELMo는 이를 구분하여 가장 가까운 단어에 연극과 관련된 내용이 위치하는 것을 볼 수 있다. 이렇게 ELMo는 주변 문맥을 고려하기 때문에 GloVe가 해결하지 못하는 다의어 등의 문제를 해결할 수 있다.



## Embeddings

ELMo의 표현들은 biLSTM(bidirectional LSTM)에 존재하는 모든 내부 층에 대한 은닉 벡터(Hidden Vector)에 가중치를 부여하고 선형 결합(Linear combination)하여 사용한다. 이렇게 각 벡터마다 가중치를 부여 선형 결합을 하여 나타내면 해당 단어가 상당히 많은 표현을 할 수 있게 된다. 각각의 은닉 벡터 중 위쪽에 위치한 LSTM은 단어의 문맥적인(Context-dependent) 의미를 포착하게 되며, 아래쪽에 위치한 LSTM은 단어의 구조적인(Syntax) 의미를 포착하게 된다. 따라서 단어의 구조적인 표현이 중요해지는 구문 분석(Syntax Analysis)이나 품사 태깅(POS Tagging) 등의 Downstream Task에는 아래쪽 은닉 벡터에 가중치를 더 많이 주게 되며, 의미적인 표현이 중요해지는 NLU(Natural Language Understanding)이나 QA(Question & Answering)와 같은 Downstream Task를 할 때는 위쪽에 있는 은닉 벡터에 더 많은 가중치를 부여하게 된다.

ELMo는 전체 문장을 한 번 본 후 단어마다의 임베딩을 해준다. 다음은 ELMo가 전체 문장을 한 번 훑는 것을 이미지로 표현하고 있다. 

<p align="center"><img src="http://jalammar.github.io/images/elmo-word-embedding.png" alt="elmo2" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

ELMo의 방식은 기본적으로 언어 모델(Language Model)을 따르고 있으므로 지금까지의 시퀀스로부터 다음 단어를 예측하는 방식으로 작동한다. 아래는 엘모가 작동하는 방식을 이미지로 나타낸 것이다. 특정 단어 이전까지의 단어는 임베딩된 후 LSTM의 층을 지나면서 ELMo 임베딩을 만들게 된다. 이들로부터 적절한 연산을 통하여 가장 확률값이 높은 단어를 출력하는 것이 ELMo가 작동하는 방식이다. 아래의 식을 계산한 결과를 통해서 가장 적절한 단어인 *"improvisation"* 을 학습하게 된다.
$$
P(w_t|w_{\text{Let's}}, w_{\text{stick}}, w_{\text{to}})
$$


<p align="center"><img src="http://jalammar.github.io/images/Bert-language-modeling.png" alt="elmo3" style="zoom: 80%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

실제로는 한 방향으로의 LSTM이 아니라 양방향으로의 LSTM, 즉 biLSTM을 통해서 학습하게 된다. 그래서 실제로는 다음 단어를 예측하는 것 뿐만 아니라 이전 단어를 예측 방향으로도 학습이 이루어진다. 다음은 *"Let's stick to"* 라는 문장에서 *"stick"* 임베딩 되는 과정을 나타낸 것이다. 먼저, 양방향으로 해당 문장을 쭉 읽어나간다.

![elmo4](http://jalammar.github.io/images/elmo-forward-backward-language-model-embedding.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

두 번째로는 임베딩할 단어, 즉 *"Stick"* 에 해당하는 입력 벡터 및 순방향(Forward LSTM)에서의 학습된 은닉 벡터들과 역방향(Backward LSTM)에서 학습된 은닉 벡터를 서로 Concatenate한다. 그리고 그 벡터들마다 가중치 $s_0, s_1, s_2$ 를 곱해준 뒤 모든 벡터를 더해준다. 이렇게 생성된 가중합 벡터가 ELMo 임베딩 벡터가 된다.

![elmo5](http://jalammar.github.io/images/elmo-embedding.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

위 그림에서 가중치에 해당하는 $s_0, s_1, s_2$ 는 학습과정에서 갱신되는 파라미터로 우리가 수행하고자 하는 Downstream Task에 의해 달라지게 된다. 앞서 말했듯 단어의 의미가 중요해지는 Task의 경우에는 위쪽에 곱해지는 가중치인 $s_2$ 가 커지게 되고, 단어의 구조가 중요해지는 Task에서는 아래쪽에 곱해지는 가중치인 $s_1, s_0$ 이 커진다.

우리가 실질적으로 맞닥뜨리는 문제는 Downstream Task들이다. 그렇기 때문에 Task에 맞는 가중치를 학습하는 것이 중요해지며 같은 문장을 대상으로 하더라도 Task별로 다른 엘모 벡터가 생성된다. 아래 식에서 $\gamma$ 에 해당하는 것을 엘모 벡터의 값을 얼마나 증폭 혹은 감소해야 하는지에 대한 Scale Factor이며 이 또한 ELMo를 사용할 Task에 따라 달라지게 된다.

![elmo6](https://user-images.githubusercontent.com/45377884/86392025-b50ff800-bcd5-11ea-91bc-3562391ef058.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

## biLM

ELMo를 구성하고 있는 **양방향 언어 모델(Bidirectional Language Model, biLM)** 에 대해 알아보자. 일단 순방향으로 진행되는 언어 모델을 수식으로 나타내면 아래와 같이 나타낼 수 있다.


$$
p(t_1,t_2, \cdots, t_N) = \prod_{k=1}^N (t_k|t_1, t_2, \cdots, t_{k-1})
$$


가장 먼저 문장의 맨 앞에 $t_1$ 이 올 확률을 구한 뒤, $t_1$ 뒤에 $t_2$ 가 올 확률을 구하고 다음으로 $t_1, t_2$ 뒤에 $t_3$ 가 올 확률을 구한다. 이 과정을 문장이 끝나는 $t_N$ 까지 반복하게 되며 이 확률을 모두 곱한 것이 위의 식이다. 여기서 각 입력 토큰 벡터에 해당하는 $x_k^{LM}$ 은 문맥에 상관없이 표현된 것으로 GloVe를 통해 학습된 임베딩을 사용하여도 되고, Character 단위에서 CNN으로 학습한 임베딩 등 다양한 방식으로 표현된 것을 사용하면 된다. 하지만 각각의 $x_k^{LM}$ LSTM을 통해 학습되며 나타나는 Hidden state $\overrightarrow{h_{k,j}^{LM}}$ 은 문맥을 고려하여 표현된다.

이번에는 역방향으로 진행되는 언어 모델을 알아보자. 일단 역방향 모델을 수식으로 나타내면 아래와 같다.


$$
p(t_1,t_2, \cdots, t_N) = \prod_{k=1}^N (t_k|t_{k+1}, t_{k+2}, \cdots, t_{N})
$$


역방향 또한 가장 먼저 문장의 맨 끝에 $t_N$ 이 올 확률을 구한 뒤 그 앞에 올 단어의 확률을 구하는 이러한 과정을 문장이 시작하는 $t_1$ 까지 구한 뒤 모든 확률을 곱하면 위 식과 같게 된다. 그리고 역방향 LSTM을 통해서 생성된 Hidden state $\overleftarrow{h_{k,j}^{LM}}$ 는 역시 문맥을 고려하여 표현된다.

그럼 이 두 방향을 결합하여 양방향 모델을 만든다고 하자. 우리가 고려해야 할 것은 양 방향에서의 $t_k$ 의 최대 우도(Maximum Likelihood)를 구하는 것이다.


$$
\prod_{k=1}^N (t_k|t_1, t_2, \cdots, t_{k-1}) \times \prod_{k=1}^N (t_k|t_{k+1}, t_{k+2}, \cdots, t_{N})
$$


식에 로그를 취해주어 다음과 같은 식이 나오게 된다.


$$
\sum^N_{k=1} \bigg( \log p(t_k|t_1, t_2, \cdots, t_{k-1}; \Theta_x, \overrightarrow{\Theta}_{LSTM} , \Theta_s)\\ \qquad \qquad
+ \log p(t_k|t_{k+1}, t_{k+2}, \cdots, t_N; \Theta_x, \overleftarrow{\Theta}_{LSTM} , \Theta_s \bigg)
$$


위 식에서 $\Theta_x, \Theta_s$ 는 각각 입력되는 토큰 벡터와 소프트맥스 층에서 사용되는 파라미터를 가리킨다. L개의 층을 가진 biLM을 사용하게 되면 토큰 $t_k$ 마다 2L+1 개의 표현을 얻을 수 있다. 최초의 단어 임베딩 벡터 1개가 존재하고, 순방향 LM으로부터 L개, 역방향 LM으로부터 L개의 벡터를 얻는다. 이를 수식으로 표현하면 다음과 같이 나타낼 수 있다.


$$
R_k = \{\mathbf{x}_k^{LM}, \overrightarrow{h_{k,j}^{LM}}, \overleftarrow{h_{k,j}^{LM}} | j = 1, \cdots, L\} = \{h_{k,j}^{LM} | j=0, \cdots, L\}
$$


이렇게 각 토큰마다 2L+1 개의 은닉 상태 벡터(Hidden state vector)를 구한 후에는 Downstream Task에 따라서 이를 선형결합하여 ELMo의 임베딩을 구한다. 아래 식에서 쓰이는 2개의 가중치 모두 ELMo가 처리할 Task에 따라서 달라지게 되며 $s$ 는 각 층의 표현을 얼마나 중요하게 사용할 것인지를 결정하는 가중치이고 $\gamma$ 는 Task에 필요한 만큼 전체 벡터 요소의 크기(Scale)를 조정하는 가중치이다.


$$
\text{ELMo}_k^\text{task} = E(R_k;\Theta^\text{task}) = \gamma^\text{task} \sum^L_{j=0} s_j^\text{task} h_{k,j}^{LM}
$$


## Evaluation

아래는 다양한 Task에 대한 엘모의 성능을 측정한 것이며 몇몇 Task에 대하여 당시까지의 SOTA 모델보다 좋은 성능을 보여주는 것을 알 수 있다.

![elmo7](https://user-images.githubusercontent.com/45377884/86509629-53aa6f00-be24-11ea-82e0-3baffe2ee634.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

아래는 '가중치를 어떻게 부여할 것인가?'에 대한 결과를 이미지로 나타낸 것이다. 위에서 적용한 것과 같이 각 층의 아웃풋마다 다른 가중치를 적용할 때의 성능이 가장 좋으며 모든 벡터에 동일한 가중치를 사용할 경우에 두 번째로 좋은 성능을 보였다. 벡터를 하나만 사용할 경우에는 모든 벡터를 사용할 때보다 성능이 떨어지게 된다. 벡터를 하나만 사용한다면 $L$ 번째 층의 벡터를 사용하는 것이 ELMo를 적용하지 않은 벡터 $\mathbf{x}$ 를 사용하는 것보다 좋다.   

![elmo8](https://user-images.githubusercontent.com/45377884/86509632-56a55f80-be24-11ea-8a4e-21cb62586864.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

다음은 'ELMo 임베딩 벡터를 어떤 단계에 Concatenate하는 것이 좋은가?' 에 대한 결과를 이미지로 나타낸 것이다. 입출력 단계에 전부 ELMo 벡터를 적용하는 것이 가장 좋다. 입/출력 중 하나의 단계에만 엘모 벡터를 적용할 때는 입력 벡터에 ELMo를 적용하는 것이 출력 벡터에 적용하는 것보다 좋다. 어떤 방법을 선택하든 ELMo를 사용하지 않았을 때보다는 더 좋은 결과를 보이는 것을 알 수 있다.

![elmo9](https://user-images.githubusercontent.com/45377884/86509633-573df600-be24-11ea-9acd-6e9df7c32a00.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>