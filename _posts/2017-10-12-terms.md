---
title: 용어 정리
category: Machine Learning
tag: Clustering
---

이번 글에서는 머신러닝과 딥러닝 관련 다양한 용어들을 정리하였습니다. 이 글은 기본적으로 구글에서 정리해 놓은 [용어 사전 리스트](https://developers.google.com/machine-learning/glossary/)를 기본으로 위키피디아 등 자료를 참고해 정리하였습니다. 그럼 시작하겠습니다.



## activation function

이전 계층의 모든 입력값의 가중합을 받아서 다음 계층에 보낼 출력값을 산출하는 함수. 대개 ReLU나 시그모이드 같은 비선형함수가 적용된다.



## loss

모델의 예측과 정답 사이에 얼마나 차이가 있는지 나타내는 **측도(measure)**. 이 값을 정하기 위해서는 손실함수(loss function)이 정의되어 있어야 한다. 예컨대 선형회귀 모델에서 손실함수는 대개 Mean Squared Error, 로지스틱회귀에서는 로그우도가 쓰인다. 

*여기에서 측도란 1차원에서의 길이, 2차원에서의 넓이, 3차원에서의 부피 등의 개념을 일반의 집합으로까지 확장한 개념이다. 즉 집합의 ‘크기’에 상당하며, 적분이론은 이 개념을 기초로 하는 경우가 많다.*



## prior belief

모델을 학습하기 전 데이터에 대한 당신의 믿음. 예컨대 딥러닝 모델 파라메터에 대한 L2 정규화는 해당 파라메터들이 작고 0을 중심으로 정규분포를 이룬다는 사전믿음을 전제한다.



## regularization

모델 복잡도(complexity)에 대한 패널티(penalty). 정규화는 과적합을 예방하고 일반화 성능을 높이는 데 도움을 준다. 정규화에는 L1 정규화, L2 정규화, 드롭아웃, early stopping 등이 있다.



## squared loss

선형회귀에 쓰이는 손실함수. L2 Loss로도 불린다. 이 함수는 모델이 예측한 값과 실제값 간 차이(오차)의 제곱이다. 미분이 가능하다는 장점이 있다. 하지만 오차를 '제곱'하기 때문에 잘못된 예측 혹은 이상치(outlier)에 의해 그 값이 큰 영향을 받게 된다는 단점이 있다. (추가 내용 : [Quora](https://www.quora.com/When-is-square-loss-not-good-for-loss-function-for-regression))



## absolute loss

모델이 예측한 값과 실제값 간 차이(오차)의 절대값. L1 Loss로도 불린다. L1 Loss는 L2 Loss에 비해 이상치에 덜 민감하다는 장점이 있지만 0인 지점에서 미분이 불가능하다는 단점이 있다.



## L1 regularization

정규화의 일종. 모델 가중치의 L1 norm(가중치 각 요소 절대값의 합)에 대해 패널티를 부과한다. 대부분의 요소값이 0인 sparse feature에 의존한 모델에서 L1 정규화는 불필요한 피처에 대응하는 가중치들을 정확히 0으로 만들어 해당 피처를 모델이 무시하도록 만든다. 다시 말해 변수선택(feature selection) 효과가 있다는 말이다. 이는 L2 정규화와 대조된다. 

*2차원상 L1 norm의 자취는 마름모꼴이어서 미분이 불가능한데 이같은 성질과 깊은 관련을 맺고 있는듯하다. [이곳](https://ratsgo.github.io/machine%20learning/2017/05/22/RLR/) 참고.*



## L2 regularization

정규화의 일종. 모델 가중치의 L2 norm의 제곱(가중치 각 요소 제곱의 합)에 대해 패널티를 부과한다. L2 정규화는 아주 큰 값이나 작은 값을 가지는 outlier 모델 가중치에 대해 0에 가깝지만 0은 아닌 값으로 만든다. 이는 L1 정규화와 대조된다. L2 정규화는 선형모델의 일반화 능력을 언제나 항상 개선시킨다.



## structural risk minimization

알고리즘은 다음과 같은 두 가지 목표를 만족시켜야 한다.

- 예측력이 좋은 모델 (예컨대 손실이 가장 적은 모델)
- 가급적 간단한 모델 (예컨대 강한 수준의 정규화가 이뤄진 모델)

예컨대 학습데이터에 대해 손실 최소화와 정규화를 동시에 달성한 모델은 structural risk minimization이 이뤄진 알고리즘이라 할 수 있다. structural risk minimization는 empirical risk minimization 개념과 대조된다.



## empirical risk minimization

학습데이터에 대해 손실이 적은 모델을 선택하는 것. structural risk minimization 개념과 대조된다.



## least squares regression

L2 loss를 최소화함으로써 학습된 선형회귀 모델.



## generalized linear model

least squares regression의 일반화된 버전. 가우시안 노이즈에 기반한 모델도 있고 포아송 노이즈나 범주형 노이즈(categorical noise) 같은 다른 종류의 노이즈에 기반한 모델도 있다. generalized linear model의 예시는 다음과 같다.

- logistic regression
- multi-class regresstion
- least squares regression

generalized linear model의 파라메터는 convex optimization 기법으로 구한다. generalized linear model은 다음 두 가지 속성이 있다.

- 최적 generalized linear model의 예측 평균은 학습 데이터 정답의 평균과 같다.
- 최적 로지스틱 회귀 모델이 예측한 확률의 평균은 학습 데이터 정답의 평균과 같다.

generalized linear model의 예측력은 피처에 의해 제한된다. 딥러닝 모델과 달리 generalized linear model은 (학습데이터에 없는)새로운 피처를 학습할 수 없다.