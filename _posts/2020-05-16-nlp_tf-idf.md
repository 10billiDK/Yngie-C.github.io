---
layout: post
title: TF-IDF
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) , [자연어 처리 인 액션](http://www.yes24.com/Product/Goods/89232661) 책을 참고하였습니다.



# Word Weighting

**단어 가중치 부여(Word Weighting)** 는 특정 문서에서 더 중요한 역할을 하는 단어에 가중치를 부여하는 방법이다. 단어에 가중치를 부여하는 기준은 TF와 DF라는 두 수치를 기준으로 한다.

먼저 **TF(Term Frequency)** 는 우리가 분석하고자 하는 문서에 등장하는 단어의 숫자를 나타낸 수치다. Bag of Words에서 가정했던 문서에 등장하는 빈도가 높을수록 그 단어가 더 중요할 것이라는 전제를 이어나간다. **DF(Document Frequency)** 는 특정 단어가 코퍼스 내에 있는 전체 문서 중 몇 개의 문서에 등장했는지를 나타낸 수치다. DF는 그 수치가 낮을수록, 즉 그 단어가 등장하는 문서의 수가 적을수록 더 중요한 단어일 확률이 높다는 새로운 가정에서 출발한다. 이를테면, ***is, can, the*** 등의 단어는 어디에든 많이 등장하기 때문에 TF가 매우 높다. TF만 사용한다면 이런 단어는 실제와 다르게 중요한(의미있는) 단어로 취급된다. 이 때 이들의 중요도를 낮춰주는 것이 DF이다. 이들은 모든 문서에 등장하기 때문에 DF가 높고 이를 통해 중요도가 낮은 단어로 보정된다. 실제 계산에서는 DF에 역수를 취하고 로그를 취한 수치인 **IDF(Inverse Document Frequency)** 를 사용한다.

위에서 알아본 TF와 IDF를 사용하여 특정 Term이 문서에 얼마나 중요한지를 나타낼 수 있다. 두 값을 곱한 수치를 **TF-IDF** 라고 하며 단어에 가중치를 부여하는 수치로 사용된다.





$$
\text{TF-IDF(w)} = \text{TF(w)} \times \log \frac{N}{\text{DF(w)}}
$$



TF-IDF도 ~에 따라 일반적으로는 TF에서 l(logarithm)를, IDF에서는 t(idf)를 사용한다. 하지만 해결해야 하는 문제에 따라 다양하게 바꾸어 가며 가장 좋은 조합을 찾아 나가야 한다.

<img src="https://user-images.githubusercontent.com/45377884/81643718-1b5c6500-9461-11ea-8e30-c41cbc1e6dc7.png" alt="tf-idf" style="zoom:80%;" />

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>


