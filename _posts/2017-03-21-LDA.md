---
title: Linear Discriminant Analysis
category: Machine Learning
tag: Linear Discriminant Analysis
html header: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
---

이번 포스팅에선 **선형판별분석(Linear Discriminant Analysis : LDA)**에 대해서 살펴보고자 합니다. LDA는 데이터 분포를 학습해 **결정경계(Decision boundary)**를 만들어 데이터를 **분류(classification)**하는 모델입니다. 이번 포스팅은 기본적으로 고려대 [강필성 교수님](http://dsba.korea.ac.kr/wp/?page_id=97), [김성범 교수님](http://dmqm.korea.ac.kr/content/page.asp?tID=101&sID=108) 강의를 참고로 했음을 먼저 밝힙니다. 자, 그럼 시작하겠습니다.



## LDA의 배경과 목적

LDA는 아래 그림처럼 데이터를 특정 한 축에 **사영(projection)**한 후에 두 범주를 잘 구분할 수 있는 **직선**을 찾는 걸 목표로 합니다. 모델 이름에 linear라는 이름이 붙은 이유이기도 합니다.

![LDA](http://i.imgur.com/6ggd2F0.png)

왼쪽과 오른쪽 축 가운데 분류가 더 잘 됐다고 판단할 수 있는 축은 무엇일까요? 딱 봐도 오른쪽이 좀 더 나아보입니다. 빨간색 점과 파란색 점을 분류하는 걸 목표로 했는데 왼쪽 그림에 나타난 축은 중간에 빨간색과 파란색 점이 뒤섞여 있기 때문이지요. 반대로 오른쪽 그림은 색깔이 서로 다른 점들이 섞이지 않고 비교적 뚜렷하게 구분되고 있는 점을 확인할 수 있습니다. 녹색 축을 따라서 같은 위치에 있는 점들의 빈도를 세어서 히스토그램처럼 그리면 바로 위와 같은 그림이 됩니다.

그렇다면 두 범주를 잘 구분할 수 있는 직선은 어떤 성질을 지녀야 할까요? **사영 후 두 범주의 중심(평균)이 서로 멀도록, 그 분산이 작도록 해야할 겁니다.** 왼쪽 그림을 오른쪽과 비교해서 보면 왼쪽 그림은 사영 후 두 범주 중심이 가깝고, 분산은 커서 데이터가 서로 잘 분류가 안되고 있는 걸 볼 수가 있습니다. 반대로 오른쪽 그림은 사영 후 두 범주 중심이 멀고, 분산은 작아서 분류가 비교적 잘 되고 있죠. LDA는 바로 이런 직선을 찾도록 해줍니다.



## LDA의 절차

우선 사영에 대해 알아보겠습니다. 벡터 b를 벡터 a에 사영한 결과(x)는 아래 그림과 같습니다.

![projection](http://i.imgur.com/h21igrF.png)

벡터 덧셈의 기하학적 성질을 이용해 위 그림에서 정보를 얻어낼 수 있는데요. 벡터 b를 빗변으로 하는 직각삼각형의 너비는 벡터 x, 높이는 b-x가 될 겁니다(너비와 높이를 더하면 빗변에 해당하는 b가 됨). 서로 직교하는 벡터의 내적은 0이 되므로 스칼라 p와 x를 아래와 같이 구할 수 있게 됩니다.

$${ (\overrightarrow { b } -x) }^{ T }\overrightarrow { a } =0\\ { (\overrightarrow { b } -p\overrightarrow { a } ) }^{ T }\overrightarrow { a } =0\\ { \overrightarrow { b }  }^{ T }\overrightarrow { a } -p{ \overrightarrow { a }  }^{ T }\overrightarrow { a } =0\\ p=\frac { { \overrightarrow { b }  }^{ T }\overrightarrow { a }  }{ { \overrightarrow { a }  }^{ T }\overrightarrow { a }  } \\ \overrightarrow { x } =p\overrightarrow { a } =\frac { { \overrightarrow { b }  }^{ T }\overrightarrow { a }  }{ { \overrightarrow { a }  }^{ T }\overrightarrow { a }  } \overrightarrow { a } $$

자, 그럼 이제 본격적으로 LDA에 대해 알아볼까요? d차원의 입력벡터 x를 w라는 벡터(축)에 사영시킨 후 생성되는 1차원상의 좌표값(스칼라)를 아래와 같이 y라고 정의합니다. 각각 N1개와 N2개의 관측치를 갖는 C1과 C2 두 범주에 대해 원래 입력공간(2차원)에서 각 범주의 중심(평균) 벡터도 아래와 같이 m1, m2라고 정의합니다. 

$$y={ \overrightarrow { w }  }^{ T }\overrightarrow { x } \\ { m }_{ 1 }=\frac { 1 }{ { N }_{ 1 } } \sum _{ n\in { C }_{ 1 } }^{  }{ { x }_{ n } } \\ { m }_{ 2 }=\frac { 1 }{ { N }_{ 2 } } \sum _{ n\in { C }_{ 2 } }^{  }{ { x }_{ n } }  $$

일단 사영후 두 범주의 중심이 멀리 떨어져 위치하는 벡터 w를 찾아야 합니다. 각 중심과 w와의 관계식은 아래와 같습니다.

$${ m }_{ 2 }-{ m }_{ 1 }={ w }^{ T }({ m }_{ 2 }-{ m }_{ 1 })\\ { m }_{ k }={ w }^{ T }{ m }_{ k }$$

사영 후 각 범주에 속한 관측치들은 해당 범주 중심에 가까이 있을 수록 좋습니다. 다시 말해 분산이 작아야 한다는 겁니다. 사영 후 분산은 아래 식과 같습니다.

$${ s }_{ k }^{ 2 }=\sum _{ n\in { C }_{ k } }^{  }{ { ({ y }_{ n }-{ m }_{ k }) }^{ 2 } } $$

두 범주의 중심은 최대화하고, 분산은 최소화하는 게 우리의 목표입니다. 동시에 한꺼번에 할 수는 없을까요? 간단한 방법이 있습니다. 두 범주 중심을 분자, 두 범주의 분산을 분모에 넣고 이 식을 최대화하는 겁니다. 이를 행렬 형태의 목적함수로 나타내면 다음과 같습니다.

$$J(w)=\frac { { ({ m }_{ 1 }-{ m }_{ 2 }) }^{ 2 } }{ { s }_{ 1 }^{ 2 }+{ s }_{ 2 }^{ 2 } } =\frac { { w }^{ T }{ S }_{ B }w }{ { w }^{ T }{ S }_{ W }w } \\ { S }_{ B }=({ m }_{ 1 }-{ m }_{ 2 }){ ({ m }_{ 1 }-{ m }_{ 2 }) }^{ T }\\ { S }_{ W }=\sum _{ n\in { C }_{ 1 } }^{  }{ ({ x }_{ n }-{ m }_{ 1 }){ ({ x }_{ n }-{ m }_{ 1 }) }^{ T } } +\sum _{ n\in { C }_{ 2 } }^{  }{ ({ x }_{ n }-{ m }_{ 2 }){ ({ x }_{ n }-{ m }_{ 2 }) }^{ T } } $$

목적함수 J(w)은 w에 대해 미분한 값이 0이 되는 지점에서 최대값을 가집니다. 아래 식과 같습니다.

$$({ w }^{ T }{ S }_{ B }w){ S }_{ W }w=({ w }^{ T }{ S }_{ W }w){ S }_{ B }w$$

그런데 위 식에서 괄호 안은 계산해보면 스칼라가 됩니다. 각각의 차원수를 고려하면 (1Xd) (dXd) (dX1)이 되기 때문입니다. 그럼 위 식 양변을 좌변 괄호안의 스칼라값으로 나누어 약간 정리를 하면 아래와 같은 형태가 됩니다.

$${ S }_{ W }w=\lambda { S }_{ B }w\\ { { S }_{ B } }^{ -1 }{ S }_{ W }w=\lambda w$$

이 식 어디서 많이 보시지 않았나요? 네 그렇습니다. 바로 고유값, 고유벡터 구할 때 바로 그 AX=λX 꼴입니다. 즉 새로운 축 w는 Sb의 역행렬과 Sw를 내적한 행렬의 고유벡터라는 이야기죠. 자, 그럼 여기서 Sb와 Sw는 어떻게 구할까요? 각각 우리가 가진 데이터의 평균과 분산입니다. 데이터로부터 구할 수 있다는 이야기죠. 이 행렬을 **고유값분해**하여 새로운 축 w를 구할 수 있습니다. 이를 가지고 예측은 어떻게 할까요? 새로운 데이터(x')가 주어지면 이를 w와 내적해 각각의 스코어를 낼 수 있습니다. 그 스코어가 0보다 크면 C1범주, 작으면 C2 범주로 분류를 하게 됩니다.



## LDA의 또다른 접근 : 베이지안 법칙

LDA를 확률모형으로부터 도출할 수도 있습니다. **베이지안 법칙(Bayes' Rule)**을 이용한 방식입니다. 일반적으로 사건 A1, A2, A3가 서로 배반(mutually exclusive)이고 A1, A2, A3의 합집합이 표본공간(sample space)과 같으면 사건 A1, A2, A3는 표본공간 S의 **분할**이라고 정의합니다. 우리가 관심있는 사건 B가 나타날 확률을 그림과 식으로 나타내면 다음과 같습니다.

![bayes](http://i.imgur.com/jC7FfHv.png)

$$P(B)=P({ A }_{ 1 }\cap B)+P({ A }_{ 2 }\cap B)+P({ A }_{ 3 }\cap B)$$

P(B)를 조건부확률의 정의를 이용해 다시 쓰면 아래와 같습니다. 이를 **전확률 공식(Law of Total Probability)** 또는 **베이즈안 법칙**이라고 합니다.

$$P(B)=P({ A }_{ 1 })P(B|{ A }_{ 1 })+P({ A }_{ 2 })P(B|{ A }_{ 2 })+P({ A }_{ 3 })P(B|{ A }_{ 3 })=\sum _{ i=1 }^{ 3 }{ P({ A }_{ i })P(B|{ A }_{ i }) } $$

그럼 반대로 사건 B가 A1에 기인했을 조건부확률은 어떻게 구할까요? 바로 아래와 같이 구할 수 있습니다.

$$
\begin{align*}
P({ A }_{ 1 }|B)&=\frac { P({ A }_{ 1 })P(B|{ A }_{ 1 }) }{ P(B) } \\
&=\frac { P({ A }_{ 1 })P(B|{ A }_{ 1 }) }{ P({ A }_{ 1 })P(B|{ A }_{ 1 })+P({ A }_{ 2 })P(B|{ A }_{ 2 })+P({ A }_{ 3 })P(B|{ A }_{ 3 }) } 
\end{align*}
$$

같은 방식으로 P(A2\|B), P(A3\|B)도 구할 수 있습니다. 보통 P(A1), P(A2), P(A3)는 미리 알고 있다는 의미의 **사전확률(prior probability)**로 불립니다. P(A1\|B), P(A2\|B), P(A3\|B)는 사건 B를 관측한 후에 그 원인이 되는 사건 A의 확률을 따졌다는 의미의 **사후확률(posterior probability)**로 정의됩니다. 사후확률은 사건 B의 정보가 더해진, 사전확률의 업데이트 버전 정도라고 생각하면 좋을 것 같습니다(Posterior probability is an updated version of prior probability). 

한번 예를 들어보겠습니다. 어떤 이름모를 질병에 걸린 환자가 전체 인구의 약 1% 정도 되는 것으로 알려져 있다고 칩시다. 그렇다면 전체 인구라는 표본공간에서 질병에 걸릴 확률 **P(D)**는 0.01, 그렇지 않을 확률 **P(~D)**는 0.99입니다. 이것이 바로 사전확률이 되겠네요. 질병 발생 여부를 측정해주는 테스트의 정확도는 이렇다고 합니다. 진짜 환자를 양성이라고 정확하게 진단할 확률 **P(+\|D)**은 97%, 정상환자를 양성이라고 오진할 확률 **P(+\|~D)**은 6%입니다. 

그럼 진단테스트 결과 양성이라고 나왔는데 실제 환자일 확률 **P(D\|+)**는 얼마일까요? 이건 우리가 이미 알고 있는 정보를 활용해 아래와 같이 구할 수 있습니다. 이것이 바로 사후확률입니다. 즉 '진단'이라는 사건의 정보를 더해 '질병에 걸릴 확률'이라는 사전확률을 업데이트한거죠. 

$$
\begin{align*}
P(+)&=P(D\cap +)+P(\~ D\cap +)\\ 
&=P(D)P(+|D)+P(\~ D)P(+|\~ D)\\ 
&=0.01\times 0.97+0.99\times 0.06\\ 
&=0.691\\ P(D|+)&=\frac { P(D)P(+|D) }{ P(+) } \\
&=\frac { 0.01\times 0.97 }{ 0.691 } \\
&=0.014
\end{align*}
$$

그런데 왜 이렇게 복잡하게 사후확률을 구하는거냐고요? 실제로 사후확률은 구하기 어려운 경우가 많다고 합니다. 위의 예시에서도 볼 수 있듯 전체 인구 가운데 환자 비율, 질병 테스트의 정확도는 비교적 쉽게 구할 수 있지만 사후확률인 '진단테스트 결과 양성이라고 나왔는데 실제 환자일 확률'은 전자들보다 구하기 어려울 것 같습니다.

자, 그럼 베이즈 법칙을 활용해 LDA를 구해보도록 할까요?