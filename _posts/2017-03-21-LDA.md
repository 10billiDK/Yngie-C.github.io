---
title: Linear Discriminant Analysis
category: Machine Learning
tag: Linear Discriminant Analysis
html header: <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"></script>
---

이번 포스팅에선 **선형판별분석(Linear Discriminant Analysis : LDA)**에 대해서 살펴보고자 합니다. LDA는 데이터 분포를 학습해 **결정경계(Decision boundary)**를 만들어 데이터를 **분류(classification)**하는 모델입니다. 이번 포스팅은 기본적으로 고려대 [강필성 교수님](http://dsba.korea.ac.kr/wp/?page_id=97), [김성범 교수님](http://dmqm.korea.ac.kr/content/page.asp?tID=101&sID=108) 강의를 참고로 했음을 먼저 밝힙니다. 자, 그럼 시작하겠습니다.



## LDA의 배경과 목적

LDA는 아래 그림처럼 데이터를 특정 한 축에 **사영(projection)**한 후에 두 범주를 잘 구분할 수 있는 **직선**을 찾는 걸 목표로 합니다. 모델 이름에 linear라는 이름이 붙은 이유이기도 합니다.

![LDA](http://i.imgur.com/6ggd2F0.png)

왼쪽과 오른쪽 축 가운데 분류가 더 잘 됐다고 판단할 수 있는 축은 무엇일까요? 딱 봐도 오른쪽이 좀 더 나아보입니다. 빨간색 점과 파란색 점을 분류하는 걸 목표로 했는데 왼쪽 그림에 나타난 축은 중간에 빨간색과 파란색 점이 뒤섞여 있기 때문이지요. 반대로 오른쪽 그림은 색깔이 서로 다른 점들이 섞이지 않고 비교적 뚜렷하게 구분되고 있는 점을 확인할 수 있습니다. 녹색 축을 따라서 같은 위치에 있는 점들의 빈도를 세어서 히스토그램처럼 그리면 바로 위와 같은 그림이 됩니다.

그렇다면 두 범주를 잘 구분할 수 있는 직선은 어떤 성질을 지녀야 할까요? **사영 후 두 범주의 중심(평균)이 서로 멀도록, 그 분산이 작도록 해야할 겁니다.** 왼쪽 그림을 오른쪽과 비교해서 보면 왼쪽 그림은 사영 후 두 범주 중심이 가깝고, 분산은 커서 데이터가 서로 잘 분류가 안되고 있는 걸 볼 수가 있습니다. 반대로 오른쪽 그림은 사영 후 두 범주 중심이 멀고, 분산은 작아서 분류가 비교적 잘 되고 있죠. LDA는 바로 이런 직선을 찾도록 해줍니다.



## LDA의 절차

우선 사영에 대해 알아보겠습니다. 벡터 b를 벡터 a에 사영한 결과(x)는 아래 그림과 같습니다.

![projection](http://i.imgur.com/h21igrF.png)

벡터 덧셈의 기하학적 성질을 이용해 위 그림에서 정보를 얻어낼 수 있는데요. 벡터 b를 빗변으로 하는 직각삼각형의 너비는 벡터 x, 높이는 b-x가 될 겁니다(너비와 높이를 더하면 빗변에 해당하는 b가 됨). 서로 직교하는 벡터의 내적은 0이 되므로 스칼라 p와 x를 아래와 같이 구할 수 있게 됩니다.

$${ (\overrightarrow { b } -x) }^{ T }\overrightarrow { a } =0\\ { (\overrightarrow { b } -p\overrightarrow { a } ) }^{ T }\overrightarrow { a } =0\\ { \overrightarrow { b }  }^{ T }\overrightarrow { a } -p{ \overrightarrow { a }  }^{ T }\overrightarrow { a } =0\\ p=\frac { { \overrightarrow { b }  }^{ T }\overrightarrow { a }  }{ { \overrightarrow { a }  }^{ T }\overrightarrow { a }  } \\ \overrightarrow { x } =p\overrightarrow { a } =\frac { { \overrightarrow { b }  }^{ T }\overrightarrow { a }  }{ { \overrightarrow { a }  }^{ T }\overrightarrow { a }  } \overrightarrow { a } $$

자, 그럼 이제 본격적으로 LDA에 대해 알아볼까요? p차원의 입력벡터 x(변수 p개)를 w라는 벡터(축)에 사영시킨 후 생성되는 1차원상의 좌표값(스칼라)를 아래와 같이 y라고 정의합니다. 각각 N1개와 N2개의 관측치를 갖는 C1과 C2 두 범주에 대해 원래 입력공간(2차원)에서 각 범주의 중심(평균) 벡터도 아래와 같이 m1, m2라고 정의합니다. 

$$y={ \overrightarrow { w }  }^{ T }\overrightarrow { x } \\ { m }_{ 1 }=\frac { 1 }{ { N }_{ 1 } } \sum _{ n\in { C }_{ 1 } }^{  }{ { x }_{ n } } \\ { m }_{ 2 }=\frac { 1 }{ { N }_{ 2 } } \sum _{ n\in { C }_{ 2 } }^{  }{ { x }_{ n } }  $$

일단 사영후 두 범주의 중심이 멀리 떨어져 위치하는 벡터 w를 찾아야 합니다. 각 중심과 w와의 관계식은 아래와 같습니다.

$${ m }_{ 2 }-{ m }_{ 1 }={ w }^{ T }({ m }_{ 2 }-{ m }_{ 1 })\\ { m }_{ k }={ w }^{ T }{ m }_{ k }$$

사영 후 각 범주에 속한 관측치들은 해당 범주 중심에 가까이 있을 수록 좋습니다. 다시 말해 분산이 작아야 한다는 겁니다. 사영 후 분산은 아래 식과 같습니다.

$${ s }_{ k }^{ 2 }=\sum _{ n\in { C }_{ k } }^{  }{ { ({ y }_{ n }-{ m }_{ k }) }^{ 2 } } $$

두 범주의 중심은 최대화하고, 분산은 최소화하는 게 우리의 목표입니다. 동시에 한꺼번에 할 수는 없을까요? 간단한 방법이 있습니다. 두 범주 중심을 분자, 두 범주의 분산을 분모에 넣고 이 식을 최대화하는 겁니다. 이를 행렬 형태의 목적함수로 나타내면 다음과 같습니다.

$$J(w)=\frac { { ({ m }_{ 1 }-{ m }_{ 2 }) }^{ 2 } }{ { s }_{ 1 }^{ 2 }+{ s }_{ 2 }^{ 2 } } =\frac { { w }^{ T }{ S }_{ B }w }{ { w }^{ T }{ S }_{ W }w } \\ { S }_{ B }=({ m }_{ 1 }-{ m }_{ 2 }){ ({ m }_{ 1 }-{ m }_{ 2 }) }^{ T }\\ { S }_{ W }=\sum _{ n\in { C }_{ 1 } }^{  }{ ({ x }_{ n }-{ m }_{ 1 }){ ({ x }_{ n }-{ m }_{ 1 }) }^{ T } } +\sum _{ n\in { C }_{ 2 } }^{  }{ ({ x }_{ n }-{ m }_{ 2 }){ ({ x }_{ n }-{ m }_{ 2 }) }^{ T } } $$

목적함수 J(w)은 w에 대해 미분한 값이 0이 되는 지점에서 최대값을 가집니다. 아래 식과 같습니다.

$$({ w }^{ T }{ S }_{ B }w){ S }_{ W }w=({ w }^{ T }{ S }_{ W }w){ S }_{ B }w$$

그런데 위 식에서 괄호 안은 계산해보면 스칼라가 됩니다. 각각의 차원수를 고려하면 (1Xd) (dXd) (dX1)이 되기 때문입니다. 그럼 위 식 양변을 좌변 괄호안의 스칼라값으로 나누어 약간 정리를 하면 아래와 같은 형태가 됩니다.

$${ S }_{ W }w=\lambda { S }_{ B }w\\ { { S }_{ B } }^{ -1 }{ S }_{ W }w=\lambda w$$

이 식 어디서 많이 보시지 않았나요? 네 그렇습니다. 바로 고유값, 고유벡터 구할 때 바로 그 AX=λX 꼴입니다. 즉 새로운 축 w는 Sb의 역행렬과 Sw를 내적한 행렬의 고유벡터라는 이야기죠. 자, 그럼 여기서 Sb와 Sw는 어떻게 구할까요? 각각 우리가 가진 데이터의 평균과 분산입니다. 데이터로부터 구할 수 있다는 이야기죠. 이 행렬을 **고유값분해**하여 새로운 축 w를 구할 수 있습니다. 이를 가지고 예측은 어떻게 할까요? 새로운 데이터(x')가 주어지면 이를 w와 내적해 각각의 스코어를 낼 수 있습니다. 그 스코어가 0보다 크면 C1범주, 작으면 C2 범주로 분류를 하게 됩니다.



## LDA의 또다른 접근 : 베이지안 법칙

LDA를 확률모형으로부터 도출할 수도 있습니다. **베이지안 법칙(Bayes' Rule)**을 이용한 방식입니다. 일반적으로 사건 A1, A2, A3가 서로 배반(mutually exclusive)이고 A1, A2, A3의 합집합이 표본공간(sample space)과 같으면 사건 A1, A2, A3는 표본공간 S의 **분할**이라고 정의합니다. 우리가 관심있는 사건 B가 나타날 확률을 그림과 식으로 나타내면 다음과 같습니다.

![bayes](http://i.imgur.com/jC7FfHv.png)

$$P(B)=P({ A }_{ 1 }\cap B)+P({ A }_{ 2 }\cap B)+P({ A }_{ 3 }\cap B)$$

P(B)를 조건부확률의 정의를 이용해 다시 쓰면 아래와 같습니다. 이를 **전확률 공식(Law of Total Probability)** 또는 **베이즈안 법칙**이라고 합니다.

$$P(B)=P({ A }_{ 1 })P(B|{ A }_{ 1 })+P({ A }_{ 2 })P(B|{ A }_{ 2 })+P({ A }_{ 3 })P(B|{ A }_{ 3 })=\sum _{ i=1 }^{ 3 }{ P({ A }_{ i })P(B|{ A }_{ i }) } $$

그럼 반대로 사건 B가 A1에 기인했을 조건부확률은 어떻게 구할까요? 바로 아래와 같이 구할 수 있습니다.

$$
\begin{align*}
P({ A }_{ 1 }|B)&=\frac { P({ A }_{ 1 })P(B|{ A }_{ 1 }) }{ P(B) } \\
&=\frac { P({ A }_{ 1 })P(B|{ A }_{ 1 }) }{ P({ A }_{ 1 })P(B|{ A }_{ 1 })+P({ A }_{ 2 })P(B|{ A }_{ 2 })+P({ A }_{ 3 })P(B|{ A }_{ 3 }) } 
\end{align*}
$$

같은 방식으로 P(A2\|B), P(A3\|B)도 구할 수 있습니다. 보통 P(A1), P(A2), P(A3)는 미리 알고 있다는 의미의 **사전확률(prior probability)**로 불립니다. P(A1\|B), P(A2\|B), P(A3\|B)는 사건 B를 관측한 후에 그 원인이 되는 사건 A의 확률을 따졌다는 의미의 **사후확률(posterior probability)**로 정의됩니다. 사후확률은 사건 B의 정보가 더해진, 사전확률의 업데이트 버전 정도라고 생각하면 좋을 것 같습니다(Posterior probability is an updated version of prior probability). 

한번 예를 들어보겠습니다. 어떤 이름모를 질병에 걸린 환자가 전체 인구의 약 1% 정도 되는 것으로 알려져 있다고 칩시다. 그렇다면 전체 인구라는 표본공간에서 질병에 걸릴 확률 **P(D)**는 0.01, 그렇지 않을 확률 **P(~D)**는 0.99입니다. 이것이 바로 사전확률이 되겠네요. 질병 발생 여부를 측정해주는 테스트의 정확도는 이렇다고 합니다. 진짜 환자를 양성이라고 정확하게 진단할 확률 **P(+\|D)**은 97%, 정상환자를 양성이라고 오진할 확률 **P(+\|~D)**은 6%입니다. 

그럼 진단테스트 결과 양성이라고 나왔는데 실제 환자일 확률 **P(D\|+)**는 얼마일까요? 이건 우리가 이미 알고 있는 정보를 활용해 아래와 같이 구할 수 있습니다. 이것이 바로 사후확률입니다. 즉 '진단'이라는 사건의 정보를 더해 '질병에 걸릴 확률'이라는 사전확률을 업데이트한거죠. 

$$
\begin{align*}
P(+)&=P(D\cap +)+P(\~ D\cap +)\\ 
&=P(D)P(+|D)+P(\~ D)P(+|\~ D)\\ 
&=0.01\times 0.97+0.99\times 0.06\\ 
&=0.691\\ P(D|+)&=\frac { P(D)P(+|D) }{ P(+) } \\
&=\frac { 0.01\times 0.97 }{ 0.691 } \\
&=0.014
\end{align*}
$$

그런데 왜 이렇게 복잡하게 사후확률을 구하는거냐고요? 실제로 사후확률은 구하기 어려운 경우가 많다고 합니다. 위의 예시에서도 볼 수 있듯 전체 인구 가운데 환자 비율, 질병 테스트의 정확도는 비교적 쉽게 구할 수 있지만 사후확률인 '진단테스트 결과 양성이라고 나왔는데 실제 환자일 확률'은 전자들보다 구하기 어려울 것 같습니다.

자, 그럼 베이즈 법칙을 활용해 LDA를 구현해보도록 할까요? 범주가 두 개(W1, W2) 있다고 가정해보죠. 이를 그림으로 나타내보면 아래처럼 될 겁니다.
![LDA_bayes](http://i.imgur.com/VUhETyK.png)

우선 데이터(X)로부터 사전확률 P(W1), P(W2)을 구합니다. 어떻게 구하느냐고요? 쉽습니다. 범주가 W1인 데이터 개수, W2인 데이터 개수를 각각 전체 데이터 개수로 나눠주면 됩니다. 우리가 하고 싶은 건 이겁니다. 새로운 데이터 x가 주어졌을 때 이게 어떤 클래스에 속하는지 알아맞혀 보려고 하는 것이죠. 이를 지금까지 논의한 내용을 바탕으로 식을 쓰면 아래와 같이 됩니다.


$$
\begin{align*}
P({ W }_{ i }|x)&=\frac { P(x|{ W }_{ i })P({ W }_{ i }) }{ P(x) } \\ 
&=\frac { P(x|{ W }_{ i })P({ W }_{ i }) }{ P(x|{ W }_{ 1 })P({ W }_{ 1 })+P(x|{ W }_{ 2 })P({ W }_{ 2 }) } 
\end{align*}
$$

새로운 데이터 x가 등장하면 모델은 x라는 사건이 발생했을 때 범주를 따지는 사후확률인 P(W1\|x)와 P(W2\|x)를 각각 구합니다. 둘 중 전자가 크면 W1으로, 후자가 크면 W2로 분류를 하게 되는 방식입니다. 이때 중요한 가정은 바로 데이터 분포가 **다변량 정규분포(multivariate normal distribution)**을 따른다는 사실입니다. 많은 자연, 사회현상이 정규분포를 따르고 있고 그 예측력 또한 많은 실험을 통해 검증된 연속확률분포입니다. 다변량 정규분포의 파라메터는 평균(벡터)와 공분산(행렬)입니다. 두 개 파라메터만 알고 있으면 다변량 정규분포 확률함수로 P(x\|Wi)를 구할 수 있다는 얘기죠. 

P(x\|Wi)의 의미는 무엇일까요? 범주정보가 주어졌을 때(=정답 범주를 알고 있을 때=학습할 때) x가 나타날 확률, 즉 학습데이터 내에 있는 Wi의 분포가 됩니다. x는 다변량 정규분포이기 때문에 p개 변수 각각의 평균과 분산이 각각 다변량 정규분포의 평균벡터와 공분산행렬 형태로 나타나게 될 겁니다. 사후확률인 P(Wi\|x)는 새로운 데이터 x가 주어졌을 때(=정답 범주를 모를 때=예측할 때) Wi일 확률, 즉 검증데이터가 특정 범주에 할당하기 위한 스코어를 의미합니다.

N개 데이터가 k개 범주로 구성돼 있는 p차원(변수 개수=p개)의 데이터 X의 평균과 공분산은 각각 아래 식과 같습니다. 

$$\mu =\begin{bmatrix} { \mu  }_{ 1 } \\ ... \\ { \mu  }_{ p } \end{bmatrix}\\ \Sigma =\begin{bmatrix} { \sigma  }_{ 11 } & ... & { \sigma  }_{ 1p } \\ ... & ... & ... \\ { \sigma  }_{ p1 } & ... & { \sigma  }_{ pp } \end{bmatrix}$$

식으로 더 쓰면 더 복잡해질까봐 아래처럼 간단히 나타냈는데요. 평균벡터의 요소는 각각 변수 관측치의 평균입니다. 예컨대 μ1은 데이터 첫번째 변수의 평균입니다. 마찬가지로 공분산행렬의 요소는 각각 변수 관측치의 공분산입니다. σ12는 첫번째 변수와 두번째 변수의 공분산, 즉 cov(X1, X2)가 되겠지요.

**판별함수(discriminant function)**은 새로운 데이터 x에 대해 범주(클래스)를 할당해주는 역할을 하는데요, 범주 개수(k개)만큼의 판별함수가 필요하게 됩니다. 새로운 데이터에 대해 가장 큰 확률값을 내어주는 범주로 분류하게 됩니다. 데이터 x의 사전확률과 사후확률, 그리고 판별함수는 각각 아래 식과 같습니다. 판별함수 식에서 유의할 부분은 P(x)를 나눠주는 부분이 생략된다는 점입니다. k개 판별함수의 분모는 P(x)로 동일하고, 범주 예측엔 판별함수 분자의 크기만 중요하기 때문에 분모를 생략해 계산상 이득을 취하기 위해서입니다.


$$
\begin{align*}
P({ W }_{ i })&={ N }_{ i }/N\\ 
P({ W }_{ i }|x)&=\frac { P(x|{ W }_{ i })P({ W }_{ i }) }{ P(x) } \\ 
{ \sigma  }_{ i }(x)&=P({ W }_{ i }|x)\\ 
&\propto P(x|{ W }_{ i })P({ W }_{ i })\\ 
&=\ln { P(x|{ W }_{ i }) } +\ln { P( W }_{ i }) 
\end{align*}
$$

그럼 이제 **결정경계(decision boundary)**를 살펴보겠습니다. 범주가 두 개일 때를 예를 들어보죠. 판별함수 σ1과 σ2의 값이 같은 지점이 바로 분류 경계면이 될 겁니다. σ1이 조금이라도 크면 W1 범주, 반대 상황이면 W2 범주로 분류되기 때문입니다. 다변량 정규분포 확률함수에서 도출된 이 식 양변은 exp 앞의 계수 부분이 좌변, 우변 모두 동일하므로 이를 제거하면 아래와 같습니다.

$${ (x-{ \mu  }_{ 1 }) }^{ T }{ { \Sigma  }_{ 1 }^{ -1 } }(x-{ \mu  }_{ 1 })={ (x-{ \mu  }_{ 2 }) }^{ T }{ { \Sigma  }_{ 2 }^{ -1 } }(x-{ \mu  }_{ 2 })$$

두 범주의 공분산, 즉 Σ1과 Σ2가 같다고 가정하고 식을 정리하면 아래와 같습니다. 

$${ { (\mu  }_{ 1 }-{ \mu  }_{ 2 }) }^{ T }{ \Sigma  }^{ -1 }x-\frac { 1 }{ 2 } { { (\mu  }_{ 1 }-{ \mu  }_{ 2 }) }^{ T }{ \Sigma  }^{ -1 }{ { (\mu  }_{ 1 }-{ \mu  }_{ 2 }) }-\ln { \frac { P({ W }_{ 2 }) }{ P({ W }_{ 2 }) }  } =0$$

위 식은 복잡해보이지만 자세히 뜯어보면 Ax+b=0의 **선형식(linear equation)**이 됩니다. x를 제외하면 미지수가 전혀 없는 상수항이기 때문입니다. 이 상수항들은 모두 학습데이터의 패턴이 녹아들어 있는 결과입니다. 



## 두 접근 비교

자, 그럼 복기를 해볼까요? 두번째 챕터에서 설명드린 두 범주의 데이터를 가장 잘 분류하는 새로운 축인 w는 두 범주의 평균 벡터의 차이를 분자, 두 범주 분산의 합이 분모로 두고 이 식을 최대화하는 과정에서 도출됩니다. 이 접근에서 별도로 설명을 드리진 않았지만 데이터의 분포가 p차원의 다변량 정규분포를 따른다는 전제가 내재돼 있습니다. 미지수 w로 미분을 해서 0이 되는 지점이 이 식을 최대화하는 지점입니다. 세번째 챕터에서 설명드린 베이지안 법칙 관점에서는 사후확률 P(W1\|x)와 P(W2\|x)가 같은 지점을 결정경계로 놓고 식을 도출하게 되는데요, 이것이 결과적으로는 두번째 방식인 w로 미분을 해서 0이 되는 지점과 본질적으로 같게 돼서 두 접근이 사실상 같은 결과를 내는 것을 확인할 수 있습니다.



## 시각화 및 마무리

![시각화](http://i.imgur.com/Lz0ZaDK.png)

LDA로 두 개 범주를 분류한 부분은 왼쪽 그림과 같습니다. 세번째 챕터에서 각 범주의 공분산행렬이 동일하다는 가정을 하고 식을 정리하면 Ax+b=0 꼴의 선형식이 된다고 설명을 했는데요. 실제로 결정경계가 직선임을 시각적으로도 확인할 수 있습니다. 그런데 공분산이 동일하다는 가정은 너무 엄격하기 때문에 공분산이 다르다고 놓고 식을 정리하면 Ax^2+Bx+c=0 꼴의 **Quadratic form**이 됩니다. 그래서 오른쪽 그림을 보면 결정경계가 비선형인 모양을 볼 수 있습니다. LDA이든 QDA이든 학습데이터의 정보를 압축해 하나의 직선(내지 경계)를 만들어낸다는 점에서 본질적으로 동일합니다. 학습데이터의 패턴만 가지고도 충분히 좋은 예측모델을 만들어낼 수 있다는 얘기입니다. 질문이나 제언 있으시면 언제든지 댓글, 이메일로 알려주시기 바랍니다. 여기까지 읽어주셔서 대단히 감사합니다.