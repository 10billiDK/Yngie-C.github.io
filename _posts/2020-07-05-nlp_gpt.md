---
layout: post
title: GPT, GPT-2 (Generative Pre-Training of a language model)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# GPT

**GPT(Generative Pre-Training of a Language Model)** 는 2018년 6월 OpenAI에서 발표한 모델이다. GPT의 기본이 되는 아이디어는 ELMo와 비슷하다. 

*"자연어 데이터들 중에는 라벨링된 것보다 그렇지 않은 데이터가 훨씬 많다. 하지만 지금까지의 모델은 지도학습 Task에만 맞추어 라벨링된 데이터만을 학습해왔다. 그래서 라벨링되지 않은 많은 양의 데이터를 잘 활용(Pretrain)하면 약간의 지도학습(Fine-Tuning)만을 통해서 훨씬 더 좋은 퍼포먼스를 낼 수 있을 것이다"* 라는 것이 GPT 저자들의 생각이다. 

하지만 Fine-Tuning역시 중요하다. Pretrain 단계에서 사용되는 라벨링되지 않은 텍스트 데이터로부터 단어 레벨 이상의 정보를 활용하기가 어렵기 때문이다. 이런 데이터를 사용할 때는 어떤 방식으로 최적화를 하는 것이 효과적인지 불분명 하며, Task마다 어떻게 전이학습을 진행해야 하는 것이 효과적인가에 대한 Consensus도 없기 때문이다.

 

## Pretrain

GPT는 Pretrain 단계에서 최대우도추정에 대한 학습을 진행한다. 사용하는 수식은 다음과 같다. 아래 식에서 $k$ 는 Context Window의 크기이다.


$$
L_1 (U) = \sum_i \log P(u_i|u_{i-k}, ... , u_{i-1};\Theta)
$$


트랜스포머 디코더 블록에서 이루어지는 학습의 과정은 다음과 같이 나타낼 수 있다. $h$ 는 각 디코더를 지난 Hidden state vector를 나타내며 $W_e$ 는 각 토큰의 임베딩 행렬을, $W_p$ 는 각 위치의 임베딩 행렬을 나타낸다. $U$ 는 각 토큰의 Context 벡터이다.


$$
h_0 = UW_e + W_p \qquad \qquad U = \{u_{-k}, \cdots, u_{-1} \} \\  
h_l = \text{TransformerDecoderBlock} (h_{l-1}) \qquad \quad \\
P(u) = \text{softmax}(h_n W_e^T)
$$


트랜스포머에서는 디코더 블록 내에 총 3개의 Sub-layer(Masked Self-Attention, Encoder-Decoder Attention, FFNN)가 있다. 하지만 GPT는 인코더를 사용하지 않기 때문에 블록 내에 2개의 Sub-layer(Masked Self-Attention, FFNN)만 존재한다. 

![gpt_decoder](http://jalammar.github.io/images/xlnet/transformer-decoder-intro.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

## Fine-Tuning

Fine-Tuning에서는 라벨링된 데이터 $C$ 를 사용한다. 각각의 입력된 데이터는 Pretrained 모델을 지나며 $y$ 를 예측하게 된다. 이 때의 예측 확률을 최대로 하는 것이 Fine-Tuning의 목적함수 $L_2$ 이다.


$$
P(y|x_1, \cdots, x_m) = \text{softmax}(h_l^m W_y) \\
L_2(C) = \sum_{(x,y)} \log P(y|x_1, \cdots, x_m)
$$


Fine-Tuning 과정에서 $L_2$ 뿐만 아니라 Pretrained 에서 목적함수로 사용했던 $L_1$ 의 파라미터도 함께 활용하는 것이 좋다. 지도 학습에 대한 일반화(Generalization)가 향상되며 최적화 속도가 빨라지는 효과가 있다. 아래의 식은 두 목적함수를 결합한 새로운 목적함수 $L_3$ 를 수식으로 나타낸 것이다.


$$
L_3 (C) = L_2 (C) + \lambda \times L_1 (C)
$$


GPT의 Fine-Tuning은 각 Task마다 **입력 데이터의 형태가 달라지게 된다.** 분류(Classification)의 경우 별도의 구분자 없이 텍스트 데이터를 넣는다. 함의(Entailment) 문제는 구분자(Delimiter)를 기준으로 전제(Premise)와 가설(Hypothesis)를 나누어 넣게 된다. 문서의 유사도(Similarity)를 파악할 때는 두 개의 텍스트를 구분자로 나누되 순서를 바꾸어 2개의 세트를 집어넣는다. 각 세트가 모델로부터 출력하는 벡터를 Concatenate하여 최종 출력값을 뽑아내게 된다. 마지막으로 객관식(Multiple Choice) 문제의 경우 보기 N개 만큼의 세트를 만든다. 각 세트는 구분자를 기준으로 동일한 Context와 각각의 Answer로 구분되어 있다. 이를 트랜스포머에 넣은 뒤 Softmax를 취해주어 각 보기의 확률을 구해내게 된다. 아래는 이것을 이미지로 도식화한 것이다. 

![gpt_input_data](http://jalammar.github.io/images/openai-input%20transformations.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

GPT 논문에서는 다음과 같은 지도학습 Dataset을 사용하여 다른 모델과의 비교를 하였으며 대부분의 결과에서 SOTA를 달성한 것을 볼 수 있다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521567-a9673180-be8d-11ea-800e-5875ba786711.png" alt="gpt_perf1" style="zoom:67%;" /></p>

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521569-ab30f500-be8d-11ea-87c3-ce919c676f15.png" alt="gpt_perf2" style="zoom:67%;" /></p>

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521572-ac622200-be8d-11ea-8a9e-e5a21ccfb0bf.png" alt="gpt_perf3" style="zoom:67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

##  Ablation Studies

아래 왼쪽의 그래프는 디코더 블록의 개수와 성능의 관계를 보여주고 있다. 디코더 블록이 많을수록 성능이 좋아지는 것을 알 수 있다. 오른쪽은 각각의 Task에 대하여 Pretrain update 횟수와 성능의 관계를 LSTM과 트랜스포머 각각에 대하여 보여주는 그래프이다. 횟수가 늘어날수록 모든 Task에서 성능이 증가하는 것을 볼 수 있으며 특히, 감성분석과 같은 Task는 특정 횟수 이후로 급격하게 성능이 증가하는 것을 볼 수 있다. 그리고 모든 Task에서 LSTM보다 Transformer를 사용할 때 더 좋은 성능을 보이는 것도 알 수 있다.

![gpt_perf4](https://user-images.githubusercontent.com/45377884/86521599-0531ba80-be8e-11ea-8a9a-8390aa0f8406.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

아래는 각 데이터셋마다 목적함수를 사용하였을 때 성능이 어떻게 나오는지를 보여주고 있다. 신기한 것은 크기가 작은 데이터셋에 대해서는 보조 목적함수를 사용하는 것이 오히려 성능을 떨어뜨리는 것을 볼 수 있다. 커다란 지도학습 데이터셋에 대해서만 목적함수를 적용하는 것이 성능이 좋다는 것을 보여주고 있다.

![gpt_perf5](https://user-images.githubusercontent.com/45377884/86521602-0662e780-be8e-11ea-8457-3d8a167363a2.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

# GPT-2

2019년 2월 14일 OpenAI는 GPT-2 모델을 발표하였다. GPT-2는 GPT와 구조상으로는 거의 차이가 없으며 더 많은 데이터를 통해 Pretrain 되었다는 차이점을 가지고 있다. GPT-2는 무려 40GB 정도의 텍스트 데이터를 학습하였다.

OpenAI는 모델의 크기에 따라 4개의 GPT-2 모델을 발표하였다. 논문에 쓰여진 SMALL의 경우 117M으로 BERT BASE가 사용한 110M과 비슷한 개수의 파라미터를 가지고 있고, 그보다 큰 MEDIUM의 경우 345M으로 BERT LARGE의 파라미터 개수인 340M과 비슷하다는 것을 알 수 있다. OpenAI는 이보다 더 큰 GPT-2 LARGE와 EXTRA LARGE 모델도 공개하였는데 각각 762M, 1542M 개의 파라미터를 가지고 있다.

![gpt2_models](http://jalammar.github.io/images/gpt2/gpt2-sizes.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

## 구조

GPT-2, BERT 등 트랜스포머를 활용하여 고안된 대부분의 모델이 구성되는 방식은 유사하다. 아래의 그림에서도 알 수 있는 것처럼 트랜스포머의 일부 구조, 또는 그것을 개선하여 여러 층을 쌓는다. BERT는 인코더를, GPT-2는 디코더를, TRANSFORMER XL이라는 모델은 순환 디코더(Recurrent Decoder)를 겹쳐 쌓아올렸다. GPT-2 SMALL은 12개의 디코더를 쌓아올린 모델이며 임베딩 벡터의 차원 또한 BERT BASE와 동일한 768차원을 사용하고 있다. GPT-2 MEDIUM은 디코더 24개를 쌓아올렸으며 BERT LARGE와 같은 1024차원의 임베딩 벡터를 사용하고 있다. 나머지 GPT-2 LARGE와 EXTRA LARGE는 각각 36, 48개의 디코더를 쌓아올렸고 1280, 1600차원의 임베딩 벡터를 사용하고 있음을 아래 그림을 통해 확인할 수 있다.

![gpt2_size](http://jalammar.github.io/images/gpt2/gpt2-sizes-hyperparameters-3.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

GPT-2가 최대로 처리할 수 있는 토큰의 개수는 1024개이다. GPT-2는 특정 토큰이 생성될 때 항상 가장 확률이 높은 단어(Top word)만을 선택하지 않도록 top_k 라는 파라미터를 설정할 수 있다. 이 파라미터를 조정하면 단어를 생성할 때 k개의 단어를 보고 그 중에서 선택하게 된다.  그리고 Auto-regressive한 모델이므로 생성된 토큰은 다음 토큰의 생성에 자동으로 영향을 끼치게 되며 이전에 생성된 토큰에 대해서는 다시 계산을 진행하지 않는다.

![gpt2_generate](http://jalammar.github.io/images/gpt2/gpt2-simple-output-2.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

전체 단어의 크기는 50,257개이며 임베딩 사이즈는 모델에 맞게 사용한다. 포지셔널 인코딩도 최대 처리할 수 있는 토큰에 맞게 1024개로 설정되어 있다. 이렇게 단어 임베딩과 포지셔널 인코딩을 해준 벡터를 더해준 후 모델로 집어넣는다. 디코더 블록마다 프로세스는 동일하지만 각각 Self-Attention과 FFNN에서 다른 가중치를 가지고 있다. 모델을 지나면 각 위치에 따른 토큰의 확률이 나오게 되며 그 중 확률이 높은 k개의 토큰 중에서 선택하여 최종 토큰을 출력하게된다. 아래는 일련의 과정을 이미지로 나타낸 것이다.

<p align="center"><img src="http://jalammar.github.io/images/gpt2/gpt2-token-embeddings-wte-2.png" alt="gpt2_word_token" style="zoom:50%;" /></p>

<p align="center"><img src="http://jalammar.github.io/images/gpt2/gpt2-positional-encoding.png" alt="gpt2_position" style="zoom: 50%;" /></p>

![gpt2_input](http://jalammar.github.io/images/gpt2/gpt2-input-embedding-positional-encoding-3.png)

![gpt2_output](http://jalammar.github.io/images/gpt2/gpt2-transformer-block-vectors-2.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/">Jay Alammar Github</a></p>

GPT-2는 단어 토큰을 만들기 위해서 BPE(Byte Pair Encoding)를 사용하며, 입력할 수 있는 최대 토큰의 개수는 1024개이지만 동시에 처리할 수 있는 토큰의 수는 절반인 512개이다. 그리고 Layer Normalization을 해주는 것이 모델의 성능에 중요한 영향을 끼친다고 한다.



## 성능

아래의 그림에서 SMALL부터 EXTRA LARGE까지 모델 사이즈가 커질수록 Perplexity(PPL)이 줄어드는 것을 볼 수 있다.

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521763-d4eb1b80-be8f-11ea-9c6a-ed22728734aa.png" alt="gpt2_perf1" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

아래는 각각의 지도학습 데이터셋에 대한 GPT-2의 성능이다. 많은 부분에서 기존의 모델을 이겨내고 SOTA를 달성한 것을 볼 수 있다. 

![gpt2_perf2](https://user-images.githubusercontent.com/45377884/86521764-d6b4df00-be8f-11ea-9a3c-4be3c2fb8947.png)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

## BERT, GPT-2 이후

ELMo, GPT, BERT, GPT-2 이후에도 아래 그림과 같이 수많은 모델이 나오고 있다. 

<p align="center"><img src="https://user-images.githubusercontent.com/45377884/86521766-d7e60c00-be8f-11ea-93e8-c3370a6ad0a0.png" alt="gpt2_post_bert" style="zoom: 67%;" /></p>

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

아래 그림을 보면 T-NLG가 발표된 시점까지 파라미터의 개수, 즉 모델의 사이즈가 기하급수적으로 늘고 있는 것을 볼 수 있다. 심지어 2020년 6월에 발표된 GPT-3에는 약 1750억(175B)개의 파라미터가 사용되며 그림의 맨 마지막을 기록하고 있는 T-NLG보다도 10배 이상 늘어나며 모델이 거대화되는 속도는 점점 빨라지고 있다.

![gpt2_post_bert2](https://user-images.githubusercontent.com/45377884/86521767-d9afcf80-be8f-11ea-9999-478135af5202.png)



<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>