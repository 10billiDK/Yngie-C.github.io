---
title: Naive Bayesian Classifier
category: Machine Learning
tag: Bayes
---

이번 글에서는 문서 분류를 하기 위한 **나이브 베이지안 분류기(Naive Bayesian Classifier)**에 대해 살펴보도록 하겠습니다. 이번 글 역시 고려대 강필성 교수님 강의를 정리했음을 먼저 밝힙니다. 베이즈 규칙과 관련해서는 [이곳](https://ratsgo.github.io/statistics/2017/07/01/bayes/)을 참고하시면 좋을 것 같습니다. 그럼 시작하겠습니다.



## 나이브 베이즈 모델

문서 이진분류 문제를 예로 들어보겠습니다. 우리가 풀려는 문제는 문서 $d$가 주어졌을 때 범주 $c_1$ 혹은 $c_2$로 분류하는 것입니다. 지금까지 설명한 베이즈 법칙을 다시 쓰면 아래와 같습니다.


$$
\begin{align*}
P({ c }_{ 1 }|d)&=\frac { P({ c }_{ 1 },d) }{ P(d) } =\frac { \frac { P({ c }_{ 1 },d) }{ P({ c }_{ 1 }) } \cdot P({ c }_{ 1 }) }{ P(d) } =\frac { P(d|{ c }_{ 1 }) { P({ c }_{ 1 }) }}{ P(d) } \\ P({ c }_{ 2 }|d)&=\frac { P(d|{ c }_{ 2 }){ P({ c }_{ 2 }) } }{ P(d) }
\end{align*}
$$


위 식에서 $P(c_i)$는 사전확률입니다. 범주 $c_i$인 문서 개수를 전체 문서 개수로 나눈 비율을 뜻합니다. $P(d$\|$c_i)$는 우도입니다. $P(c_i$\|$d)$는 사후확률입니다. 문서 $d$가 주어졌을 때 해당 문서가 범주 $c_i$일 확률, 즉 우리가 알고 싶은 값입니다.

베이즈 모델은 $P(c_1$\|$d)/P(d)$와 $P(c_2$\|$d)/P(d)$를 비교해 큰 쪽으로 범주를 할당합니다. 그런데 여기에서 $P(d)$는 겹치므로 계산을 아예 생략할 수 있습니다. 그러면 위 베이즈 공식을 아래와 같이 다시 쓸 수 있습니다. 



$$
P({ c }_{ i }|d)\propto P(d|{ c }_{ i }){ P({ c }_{ i }) }
$$

만약 문서 범주 비율, 즉 사전확률 $P(c_1)$과 $P(c_2)$가 0.5로 서로 같다면 사전확률 계산도 생략 가능합니다.


$$
P({ c }_{ i }|d)\propto P(d|{ c }_{ i })
$$

이번엔 문서 $d$가 단어 $w_1$, $w_2$로 구성돼 있다고 칩시다. 식을 또 다시 써보겠습니다.



$$
\begin{align*}
P({ c }_{ i }|d)&=P({ c }_{ i }|{ w }_{ 1 },{ w }_{ 2 })\\ &\propto P({ w }_{ 1 },{ w }_{ 2 }|{ c }_{ i }){ P({ c }_{ i }) } \\ &\propto P({ w }_{ 1 },{ w }_{ 2 }|{ c }_{ i })
\end{align*}
$$

나이브 베이즈 분류기는 각 단어가 **독립(independent)**임을 가정합니다. 모델 이름에 나이브라는 말이 붙은 이유이기도 합니다. 이에 따라 식을 다시 쓸 수 있습니다.


$$
P({ w }_{ 1 },{ w }_{ 2 })=P({ w }_{ 1 })\cdot P({ w }_{ 2 })\\ P({ w }_{ 1 },{ w }_{ 2 }|{ c }_{ i })=P({ w }_{ 1 }|{ c }_{ i })\cdot P({ w }_{ 2 }|{ c }_{ i })
$$


## 예시

예를 들어보겠습니다. 'love', 'fantastic' 두 개 단어로 구성된 영화 리뷰1을 긍정, 부정 두 개 범주 가운데 하나로 할당해야 한다고 가정합시다. 리뷰1이 긍정일 확률은 아래와 같이 우도의 연쇄적인 곱으로 구합니다. (긍정/부정 리뷰 비율은 동일하다고 가정)



$$
\begin{align*}
P(positive|{ review }_{ 1 })&\propto P(love|positive)\times P(fantastic|positive)\\ \\ &=\frac { count(love,positive) }{ \sum _{ w\in V }^{  }{ count(w,positive) }  } \times \frac { count(fantastic,positive) }{ \sum _{ w\in V }^{  }{ count(w,positive) }  }
\end{align*}
$$



이와 같은 방식으로 리뷰1이 부정일 확률도 구할 수 있습니다. 둘 중 큰 쪽으로 해당 리뷰의 범주를 할당합니다. 그러면 아래 리뷰 두 개를 분류해 봅시다.

> **review1** : This movie was awesome! I really enjoyed it.
>
> **review2** : This movie was boring and waste of time.



전체 말뭉치로부터 구한 우도는 아래와 같습니다. 

|  Words  | P(Word\|positive) | P(Word\|negative) |
| :-----: | :---------------: | :---------------: |
|  This   |        0.1        |        0.1        |
|  Movie  |        0.1        |        0.1        |
|   Was   |        0.1        |        0.1        |
| Awesome |        0.4        |       0.01        |
|    I    |        0.2        |        0.2        |
| Really  |        0.3        |       0.05        |
| enjoyed |        0.5        |       0.05        |
|   It    |        0.1        |        0.1        |
| Boring  |       0.02        |        0.3        |
|   And   |        0.1        |        0.1        |
|  Waste  |       0.02        |       0.35        |
|   Of    |       0.02        |       0.02        |
|  Time   |       0.15        |       0.15        |

Review1은 위 우도 표에 의해 긍정, Review2는 부정 범주로 분류됩니다.



$$
{ review }_{ 1 }\quad :\quad \prod _{ i }^{  }{ P({ word }_{ i }|Pos)=120\times { 10 }^{ -8 } } >\prod _{ i }^{  }{ P({ word }_{ i }|Neg)=0.5\times { 10 }^{ -8 } } \\ { review }_{ 2 }\quad :\quad \prod _{ i }^{  }{ P({ word }_{ i }|Pos)=0.012\times { 10 }^{ -8 } } <\prod _{ i }^{  }{ P({ word }_{ i }|Neg)=3.15\times { 10 }^{ -8 } }
$$



## 나이브 베이즈 분류기의 장단점

나이브 베이즈 분류기는 앞선 예시의 우도 테이블 하나만 있으면 분류가 가능합니다. 사전확률이 다르다면 전체 문서 범주 비율만 더 반영해주면 됩니다. 그만큼 계산복잡성이 낮다는 얘기입니다. 단어 등장확률을 독립으로 가정하는 [Bag-of-Words](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/10/frequency/) 기법과 잘 어울리는 모델이라고 합니다. 딥러닝 이전 자연언어처리 기법으로 각광받았던 모델입니다.

다만 나이브 베이즈 분류기는 문서에 등장하는 단어 수만큼의 우도 확률 곱으로 분류를 수행하기 때문에 단어 수가 늘어날 수록 그 값이 0으로 수렴하는 경향이 있습니다. 1보다 작은 값은 곱할 수록 작아지는 게 당연한 이치입니다. 특히 특정 범주, 가령 긍정 문서에 단 한번도 등장하지 않은 단어가 있다면 해당 단어의 우도는 0이기 때문에 최종 계산된 확률도 0이 나오게 됩니다. 이 때문에 확률값들을 지나치게 작지 않게끔 보정하는 **평활화(smoothing)** 기법이 제안되었습니다.



## 파이썬 구현

실제 구현 단계에서는 다음과 같은 평활화 처리가 필요합니다. $i$번째 단어가 긍정 문서에 단 한번도 쓰이지 않더라도 그 우도확률이 0이 되지 않도록 분자와 분모에 적당히 작은 상수를 더해주는 것입니다.


$$
P({ w }_{ i }|positive)=\frac { k+count({ w }_{ i },positive) }{ 2k+\sum _{ w\in V }^{  }{ count(w,positive) }  }
$$


또한 컴퓨터는 0에 가까운 **부동소수점(floating point number)**을 제대로 처리하지 못하기 때문에 우도의 곱은 로그 우도의 합으로 처리합니다. 예컨대 긍정 범주에 대해서는 다음과 같습니다.


$$
\prod _{ i }^{  }{ P({ word }_{ i }|Pos) } =exp\left[ \sum _{ i }^{  }{ \left\{ \log { P({ word }_{ i }|Pos) }  \right\}  }  \right]
$$


나이브 베이지안 분류기를 '밑바탁부터 시작하는 데이터 과학(조엘 그루스 지음, 인사이트 펴냄)'을 기본으로 해서 살짝 손질한 파이썬 코드는 다음과 같습니다. 별도 토크나이징은 하지 않고 어절 단위(띄어쓰기)로 문자열을 나눠서 학습을 진행합니다.

<script src="https://gist.github.com/ratsgo/45d6eb4822ae27b01329e3b8c15c8f98.js"></script>



학습은 다음과 같이 하면 됩니다.

```python
model = NaiveBayesClassifier()
model.train(trainfile_path='kor_review.csv')
```

학습용 말뭉치(kor_review.csv)는 아래처럼 생겼습니다.

```python
"이게 왜 명작 이라는 건지",0.5
"감동이 오지 않음에 나 역시 놀란다 내가 문제인걸까",2
```

테스트는 다음과 같이 하면 됩니다.

```python
model.classify('모든 것이 완벽하다 평생 함께 갈 영화')
```





## 파일럿 실험

왓챠 영화 리뷰 70만여개를 학습한 뒤 테스트 문장을 다음과 같이 넣어봤습니다. 오른쪽의 숫자는 해당 리뷰가 긍정 범주일 가능성을 나타냅니다. 클 수록 긍정, 작을 수록 부정 리뷰로 분류된 겁니다.

> (1) 단언컨대 이 영화의 결말은 영화사를 통틀어 최고 중 하나입니다 물론 거기까지 이끌어낸 플롯도 실로 대단하고 말이죠 : 0.99998
>
> (2) 이 영화가 지루하고 뻔했다는건 내가 너무 늦게 본 탓이겠지 ㅠㅠ : 0.01623
>
> (3) 졸작이 아니라 대작이다 : 0.13773

간단한 모델임에도 비교적 좋은 성능을 보이는 가운데 (3)과 같이 전혀 엉뚱한 결과를 보이는 점이 눈에 띕니다. 나이브 베이즈 분류기가 단어의 등장순서를 고려하지 않고 빈도만을 세는 단순한 모델이기 때문 아닌가 하는 생각이 듭니다.

다음은 $P(w_i$\|$positive)$가 큰 상위 200개 단어 목록입니다.

> 영화, 수, 이, 그, 너무, 정말, 더, 영화를, 내, 것, 있는, 최고의, 내가, 그리고, 가장, 본, 한, 진짜, 영화가, 잘, 보고, 다시, 없는, 이런, 다, 모든, 대한, 영화는, 없다, 보는, 또, 있다, 마지막, 하는, 최고, 이렇게, 나는, 영화의, 때, 좋다, 그냥, 볼, 난, 왜, 할, 같다, 같은, 좋은, 아름다운, 내내, 중, 다른, 함께, 것이, 꼭, 작품, 것을, 하지만, 봐도, 역시, 이야기, 나도, 나의, 모두, 많은, 않는, 사랑, 완벽한, 아닌, 말이, 어떤, 않은, 보면, 하나, 연기, 싶다, 있을까, 많이, 두, 것은, 아, 아니라, 참, 영화다, 것이다, 인생, 어떻게, 되는, 좋았다, 좋아하는, 한다, 하고, 스토리, 만드는, 만든, 나를, 제일, 지금, 아직도, 위한, 그런, 된, 싶은, 결국, 명작, 위해, 된다, 않는다, 될, 얼마나, 계속, 처음, 밖에, 그의, 사랑을, 없이, 영화에, 끝까지, 이건, 별, 좀, 사람이, 감독의, 봤다, 음악, 우리, 대해, 주는, 인간의, U, 한번, 줄, 않고, 눈물이, 액션, 건, gt, 듯, ㅠㅠ, 봤는데, 우리는, 그렇게, 새로운, 멋진, 안, 아니다, 느낌, 너무나, 사랑하는, 재밌게, 큰, 순간, 걸, 감동, 생각이, 그저, 같이, 나, 연출, 와, 재밌다, 장면, lt, 있었다, 게, 보여주는, 그래서, 이게, 나오는, 없었다, 그래도, 마음이, 진정한, 나에게, 아주, 애니메이션, 삶을, 보면서, 여운이, 배우들의, 날, 생각을, 아닐까, 때문에, 우리가, 특히, 더욱, 느낄, 미친, ㅋㅋ, 연기가, 좋고, 항상, 최고다, 영화관에서, 자신의, 장면이, the, you, 것도

다음은 $P(w_i$\|$negative)$가 큰 상위 200개 단어 목록입니다.

> 영화, 이, 너무, 수, 왜, 더, 그냥, 영화를, 없다, 그, 없는, 이런, 다, 영화는, 것, 영화가, 내가, 정말, 좀, 진짜, 잘, 내, 본, 이렇게, 보는, 한, 보고, 안, 좋은, 하는, 대한, 이건, 있는, 별, 난, 없고, 스토리, 느낌, 영화의, 하지만, 그리고, 아, 이게, 같은, 많이, 같다, 끝까지, 만든, 할, 봤는데, 내내, 볼, 뻔한, 뭐, 무슨, 참, 건, 하고, 모르겠다, 그래도, 있다, 이걸, 듯, 나는, 아닌, 별로, 보면, 하나, 모든, 전혀, 다른, 뭔가, 게, 않는, 차라리, 않은, 때문에, 아니다, 마지막, 연기, 가장, 이제, 영화에, 감독이, 것도, 다시, 근데, 위한, 때, 이야기, gt, 못한, 못, 감독의, 또, 이거, 것이, 아깝다, 뭘, 아니라, 줄, 않는다, 그저, 어떻게, 그나마, 봤다, 없었다, 그래서, 역시, 최악의, 재미가, 않고, 싶다, 두, 나오는, 위해, 것은, 결국, 했다, 한다, lt, 감독, 제대로, 연출, 재미도, 것을, 많은, 1, 스토리가, 도대체, 걸, 그런, 뿐, 없이, 딱, 쓰레기, 0, 보다가, 보다, 그렇게, 않다, 제일, 아무리, 작품, 밖에, ㅠㅠ, 싶은, 전개, 아니고, 중, 영화관에서, 계속, 액션, 영화로, 기억에, 굳이, 만드는, 제발, 되는, 연기는, 대체, 배우들의, 큰, 그만, 보기, 없음, 거, 안되는, 어떤, 솔직히, 된, 모두, 끝, 하나도, 아쉽다, 돈, 한국, 될, 캐릭터, 연기가, 원작을, 함께, 스토리는, 노잼, 않았다, 기억이, 코미디, 배우, 전형적인, 최악, 내용이, 이해가, 생각이, 극장에서, 시간, 지루한, 그게, 수가, 말이, 영화도