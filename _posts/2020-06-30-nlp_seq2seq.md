---
layout: post
title: Seq2Seq (Sequence to Sequence)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [Jay alammar의 깃허브](http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) , 그리고  [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# Seq2Seq (Sequence to Sequence)

## Seq2Seq 이란 무엇인가?

특정 아이템의 시퀀스를 입력 받아 또 다른 시퀀스를 내놓는 모델을 일컫는다. 여기서 아이템이란 문장(혹은 구절)이라는 시퀀스에 존재하는 단어가 될 수도 있고, 동영상이라는 시퀀스 내에 존재하는 이미지가 될 수도 있다.

여기서 중요한 것은 입력 시퀀스(Input Sequence) 내에 존재하는 아이템의 개수와 출력 시퀀스(Output Sequence) 내에 존재하는 아이템의 개수가 같을 필요는 없다는 점이다. 아래 그림에서는 3개의 아이템을 가진 시퀀스가 입력되었지만 모델은 4개의 아이템을 가진 시퀀스를 출력하고 있는 것을 볼 수 있다.

![seq2seq_1](https://user-images.githubusercontent.com/45377884/86036190-7a5d5400-ba78-11ea-8f18-c202b970eeae.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay alammar Github</a></p>

위 이미지를 기계 번역의 관점 구체화 시킨 이미지는 아래와 같다. 입력 시퀀스는 3개의 아이템으로 이루어진 프랑스어 문장이며 모델이 이를 입력받아 4개의 아이템으로 구성된 영어 문장을 출력 시퀀스로 내놓는 것을 볼 수 있다.

![seq2seq_2](https://user-images.githubusercontent.com/45377884/86037788-e2149e80-ba7a-11ea-9ac1-77a6646c60e2.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay alammar Github</a></p>

## Encoder - Decoder 구조

Seq2Seq는 두 가지 핵심적인 구조를 가지고 있다. 각각을 인코더(Encoder)와 디코더(Decoder)라고 한다. 인코더는 입력된 정보를 어떻게 처리(압축)하여 저장하는 지를 담당하는 부분이다. 디코더는 이렇게 압축된 정보를 어떻게 반환해낼 것인가에 대한 역할을 한다. 이를 종합하면 Seq2Seq 모델은 입력 시퀀스로부터 정보를 받아들여 취합(출력)하고 이를 다시 다른 시퀀스로 출력하는 역할을 한다고 할 수 있다.

각각의 구조가 하는 일에 대해서 조금 더 자세히 살펴보도록 하자. 인코더가 하는 일은 입력 시퀀스에 있는 각각의 아이템을 컴파일(Compile)하여 하나의 벡터, 즉 컨텍스트 벡터(Context Vector)로 표현한다. 그리고 시퀀스를 모두 읽어낸 후에는 생성한 컨텍스트 벡터를 디코더에게 넘겨주게 된다. 디코더는 이를 받아 새로운 아이템별로(Item by Item) 새로운 시퀀스를 생성하여 출력하게 된다.

![seq2seq_3](https://user-images.githubusercontent.com/45377884/86038256-99a9b080-ba7b-11ea-8207-9db61a0d42bf.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay alammar Github</a></p>

이를 위에서 본 것과 같이 기계 번역의 예를 들면 아래와 같은 구조로 볼 수 있다. 

![seq2seq_4](https://user-images.githubusercontent.com/45377884/86039320-3a4ca000-ba7d-11ea-8cd7-df00e7d120a4.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay alammar Github</a></p>

## RNN (Recurrent Neural Network)

가장 고전적인 Seq2Seq 모델의 예시로는 RNN이 있다. RNN의 은닉층(Hidden layer)에서는 은닉층 위치에 해당하는 인풋 벡터와 이전 은닉층이 출력한 결과를 동시에 입력받는다. 이를 계산하여 나온 출력값을 출력할 뿐만 아니라 다음의 은닉 층으로 보내준다. 아래는 RNN이 첫 번째 Time Step 에서 작동하는 방식을 이미지로 나타낸 것이다. 

![RNN_1](https://user-images.githubusercontent.com/45377884/86039628-a929f900-ba7d-11ea-8a4e-09e945f10d12.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay alammar Github</a></p>

위에서 나타나는 첫 번째 Time Step에서는 첫 번째에 위치하는 입력 벡터와 0번째 은닉상태 벡터(Hidden State Vector)가 들어간다. 은닉층에서는 이를 계산하여 나온 1번째 은닉 상태 벡터를 다음 은닉층에 넘겨주며 출력 벡터를 내놓는다. 기계 번역 관점에서 전체 Time Step에서 어떻게 진행되는지를 살펴보자.

![seq2seq_5](https://user-images.githubusercontent.com/45377884/86040604-4e919c80-ba7f-11ea-8bd0-8fcba7e224fd.gif)



각 입력이 들어갈 때마다 은닉층이 업데이트 되며 가장 마지막 벡터가 입력된 후에 생성되는 은닉 상태 벡터가 컨텍스트 벡터가 되어 디코더로 전달된다. 디코더는 가장 마지막 은닉 상태 벡터를 받아 일련의 과정을 거친 뒤 아이템을 반환하게 된다. 이를 기계 번역이 되는 과정으로 펼쳐보면 다음과 같이 나타낼 수 있다.

![seq2seq_6](https://user-images.githubusercontent.com/45377884/86040995-f27b4800-ba7f-11ea-8ca1-67b2517573eb.gif)

먼저 첫 번째 토큰 $I_1$ 과 최초의 은닉 상태 벡터 $h_0$ 가 만나 새로운 은닉 상태 벡터 $h_1$ 을 생성한다. 다시 $h_1$ 이 두 번째 토큰 $I_2$ 와 만나 새로운 은닉 상태 벡터 $h_2$ 를 생성한다. 이 과정을 문장이 끝날 때까지 반복하게 되며 문장이 끝난 후에는 최후의 은닉 상태 벡터인 $h_n$ 이 컨텍스트 벡터가 되어 디코더에 입력된다.



## Attention

RNN은 장기 의존성(Long-term Dependency)라는 치명적인 단점을 가지고 있다. 시퀀스가 길어질 경우 RNN으로부터 생성된 컨텍스트 벡터는 가장 뒤쪽에 위치한 아이템의 영향을 많이 받을 수밖에 없다. 앞에 해당하는 아이템일수록 컨텍스트 벡터에 미치는 영향력은 줄어들게 되며 시퀀스가 매우 길어질 경우엔 거의 0이 된다.

장기 의존성의 문제를 해결하기 위하여 RNN의 변형 형태인 LSTM(Long-Term Short Memory),  GRU(Gated Recurrent Unit) 등이 제시되었다. 하지만 이러한 RNN 변형 모델들은 문제를 줄여줄 수 있을뿐 완벽하게 해결하지는 못했다.

하지만 최근에는 Attention 개념이 등장하여 이 문제를 해결하고 있다. 기존 RNN의 경우는 컨텍스트 벡터가 하나의 벡터로 나타났기 때문에 이 구간에서 병목현상이 발생하였다. 하지만 Attention은 모델이 출력 시퀀스 아이템을 생성할 때마다 입력 시퀀스에 있는 아이템 중에서 어떤 것을 주목(Attention)해야 하는 지를 가중치로 연결해준다.

![attention_connect_image](https://lh3.googleusercontent.com/ZDgwFfcgXOkHB-e4tYz1-OvwfP3eXjGJ3l3LqFnxuulbfN1ufFNNZR2NVoWYlnFIuhAptt2WEiw9SPYoSyZ_799RiONKJOro2gSUH6I=w1440-rw-v1)

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory">DeepMind.com</a></p>

Attention에는 모델을 발표한 사람의 이름을 따 Bahadanau attention과 Luong attention이 있다. 전자는 Attention Score를 학습하는 신경망 모델이 따로 있으며 후자는 학습 없이 유사도를 측정하여 Attention Score를 만들어낸다. 하지만 두 모델 간의 성능 차이가 거의 없기 때문에 실질적으로는 Luong attention을 사용한다.

먼저 이미지를 통해서 Attention이 기존의 RNN과 어떻게 다르게 작동하는지 보도록 하자.

![seq2seq_7](https://user-images.githubusercontent.com/45377884/86040873-b942d800-ba7f-11ea-9f59-ee23923f777e.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay alammar Github</a></p>

인코더가 더이상 마지막 은닉 상태 벡터만을 디코더에 입력하지 않는다. 각 Time Step 마다 생성되는 은닉 상태 벡터를 보관하고 있다가 문장이 끝나면 생성된 모든 은닉 상태 벡터를 디코더에 넘겨준다. 입력 시퀀스 내에 존재하는 아이템 개수 만큼의 은닉 상태 벡터를 넘겨줄 수 있기 때문에 결과적으로는 디코더에 더 많은 정보를 넘겨줄 수 있게 된다.

이번에는 디코더에서 일어나는 일을 좀 더 자세히 살펴보자. 아래는 4번째 Time Step, 즉 $n$ 개 만큼의 은닉 상태 벡터가 생성되어 디코더로 전송된 이후에 일어나는 Extra Step 을 순차적으로 도식화한 것이다.

![attention_1](https://user-images.githubusercontent.com/45377884/86044868-ae8b4180-ba85-11ea-9fee-2977edfd47ce.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay alammar Github</a></p>

가장 먼저 인코더가 전해준 은닉 상태 벡터를 살펴본다. 각각의 은닉 상태 벡터는 입력 시퀀스의 아이템(+이전 은닉 상태 벡터)에 의해서 생성되었기 때문에 각 아이템의 정보를 가장 많이 가지게 된다. 각 은닉 상태에 디코더의 은닉 상태 벡터를 내적해주어 가중치 점수(Score)를 부여한다.(Luong attention의 경우 내적으로 보아도 무방) 그리고 이 점수에 소프트맥스(Softmax) 함수를 취해주어 소프트맥스 점수를 구한다. 이렇게 생성된 각각의 소프트맥스 점수는 해당하는 은닉 상태 벡터의 값에 곱해지게 되고 이를 모두 더한 새로운 컨텍스트 벡터(Sum-Up Weighted Vector)를 마지막으로 생성하게 된다. 소프트맥스 스코어가 클수록 현재 디코딩되는 아이템이 볼 때 중요한 은닉 상태 벡터이며 스코어가 작을수록 관계없는 벡터가 된다.

일반적으로는 생성된 컨텍스트 벡터와 디코더의 은닉 상태 벡터를 Concatenate(옆으로 붙임)하여 사용한다. 때문에 전체의 사이즈는 디코더의 은닉 상태 벡터 사이즈에 컨텍스트 벡터 사이즈를 곱한 값이 된다. 이를 디코더의 은닉 층에 집어넣음으로서 출력 벡터가 반환되게 된다. 이 과정을 도식화하면 아래와 같이 나타낼 수 있다.

![attention_2](https://user-images.githubusercontent.com/45377884/86046130-a59b6f80-ba87-11ea-8fe4-358b7a3b6a7f.gif)

<p align="center" style="font-size:80%">이미지 출처 : <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Jay alammar Github</a></p>

위 그림에 \<END\> 는 디코더가 작동하는 시점을 알리는 신호의 역할을 한다. 인코더로부터 넘어온 3개의 은닉 상태 벡터를 사용하여 가중치 벡터(컨텍스트 벡터)를 구한 뒤 디코더의 은닉 상태 벡터와 Concatenate한다. 이렇게 붙여진 벡터는 FFNN(Feed-Forward Neural Network)로 들어간 후 가장 적합한 아이템으로 출력된다. Time Step 4에서 생성되는 출력물은 *"I"* 이며 이것에 해당하는 벡터는 다시 Time Step 5에서 입력 벡터의 역할을 하게 된다. Time Step 5에서도 계산된 컨텍스트 벡터와 디코더 은닉 상태 벡터를 붙여 FFNN에 투입시키게 되며 FFNN이 생성한 벡터와 가장 유사한 *"am"* 이 출력 아이템으로 나오게 된다. 동일한 과정에 의해 *"a", "student"* 가 생성되며 디코더는 과정을 문장이 끝날 때까지 반복한다.

위 과정에서 Attention을 받은 정도를 시각화하면 아래와 같이 나타낼 수 있다.

![seq2seq_9](https://user-images.githubusercontent.com/45377884/86047018-29a22700-ba89-11ea-98ee-a90b2fb70a23.gif)

