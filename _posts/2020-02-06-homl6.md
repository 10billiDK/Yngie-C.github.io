---
layout: post
title: 5. Support Vector Machines(SVM)
category: Hands on Machine Learning
tag: Machine-Learning
---





**결정 트리** (Decision Tree) 또한 SVM과 같이 분류와 회귀, 다중 출력 등 다양한 곳에 사용할 수 있는 다재다능한 머신러닝 알고리즘이다. 게다가 매우 복잡한 데이터셋도 다룰 수 있는 강력한 알고리즘이다.



## 1) 결정 트리 학습과 시각화

사이킷런에서 결정 트리(분류)는 DecisionTreeClassifier() 클래스를 통해 학습시킬 수 있다.

```python
from sklearn.tree import DecisionTreeClassifier

tree_clf = DecisionTreeClassifier(max_depth=2)
```

<br/>

## 2) 예측하기

결정 트리는 다음과 같은 과정을 거쳐 예측을 만들어 낸다.[^1] **루트 노드** (Root Node)부터 샘플이 특정한 기준으로 나누어진다. 노드의 sample 속성은 얼마나 많은 훈련 샘플이 적용되었는지 나타내 준다. 이후 이 샘플의 개수를 통해 노드의 불순도를 측정한다. 이 불순도는 **지니** (Gini) 속성에 나타나게 된다. 한 노드의 모든 샘플이 하나의 클래스에 속해 있다면 이 노드의 지니 점수는 0이다. 아래는 지니 불순도를 계산하는 수식이다. <br/>
$$
G_i = 1 - \sum^n_{k=1} {p_{i,k}}^2
$$
<br/>

- $p_{i,k}$ 는 $i$ 번째 노드에 있는 훈련 샘플 클래스 $k$ 에 속한 샘플의 비율이다.

사이킷런은 이진 트리만 만드는 CART 알고리즘을 사용한다. 그러므로 리프 노드 외의 모든 노드는 자식 노드를 2개씩 가진다. 

<br/>

## 3) 클래스 확률 추정

결정 트리는 한 샘플이 특정 클래스 $k$ 에 속할 확률을 추정할 수도 있다. 먼저 주어진 샘플에 맞는 리프 노드를 찾기 위해서 트리를 탐색한다. 그리고 그 노드에 있는 클래스 $k$ 의 훈련 샘플을 반환하는 방식이다. 

<br/>

## 4) CART 훈련 알고리즘

사이킷런은 결정 트리를 훈련시키기 위해 **CART** (Classification And Regression Tree) 알고리즘을 사용한다. 먼저 훈련 세트를 하나의 특성 $k$ 의 임곗값 $t_k$ 를 사용하여 두 개의 서브셋으로 나눈다. 여기서 $k$ 와 $t_k$ 의 순서쌍 $(k, t_k)$ 는 아래의 비용 함수를 최소화하는 지점으로 결정된다. <br/>
$$
J(k, t_k) = \frac{m_{\text{left}}}{m}G_{\text{left}} + \frac{m_{\text{right}}}{m}G_{\text{right}}
$$

- $G_{\text{left/right}}$ : 왼쪽 / 오른쪽 서브셋의 불순도 
- $m_{\text{left/right}}$ : 왼쪽 / 오른쪽 서브셋의 샘플 수

훈련 세트를 성공적으로 둘로 나누면 같은 방식으로 서브셋을 계속 나누어간다. 그러다가 최대 깊이에 도달하거나, 불순도를 줄이는 분할을 찾을 수 없게 되면 멈추게 된다.[^2]

<br/>

## 5) 계산 복잡도

예측을 하려면 결정 트리를 루트 노드에서부터 리프 노드까지 탐색해야 한다. 일반적인 결정 트리는 거의 균형을 이루고 있으므로 결정 트리를 탐색하기 위해서는 약 $O(\log_2(m))$ 개의 노드를 거쳐야 한다. 각 노드는 하나의 특성값만 확인하기에 복잡도는 특성 수에 상관 없이 $O(\log_2(m))$ 의 값을 가지게 된다.

하지만 훈련 알고리즘은 다르다. 각 노드에서 훈련 샘플의 모든 특성을 비교해야 하기 때문에 훈련 복잡도가 $O(n*m\log_2(m))$ 이 된다. 훈련 세트가 작을 경우에 사이킷런에서는 `presort=True` 로 설정하여 미리 데이터를 정렬할 수 있고, 훈련 속도를 높일 수 있다. 하지만 훈련 세트가 클 경우에는 속도가 많이 느려지게 된다.

<br/>

## 6) 지니 불순도 또는 엔트로피

위에서 보았듯 결정 트리에서 일반적으로 사용되는 기준치는 지니 불순도다. 하지만 `criterion="entropy"` 로 설정하면 엔트로피 불순도를 사용할 수 있다. 엔트로피를 계산하는 식은 아래와 같다. <br/>
$$
H_i = - \sum^n_{k=1 \\ p_{i,k}\neq 0} p_{i,k} \log_2(p_{i,k})
$$
<br/>

엔트로피 식을 사용하더라도 한 샘플만을 담고 있는 세트의 엔트로피는 지니 불순도와 같이 0이 된다. 지니 불순도와 엔트로피 중 어떤 것을 사용하든 큰 차이는 없다. 하지만, 지니 계수에 대한 계산이 좀 더 빠르기 때문에 기본값으로 사용된다. 그리고 엔트로피는 지니 불순도로 형성된 트리보다 조금 더 균형 잡힌 트리를 만든다는 장점이 있다.

<br/>

## 7) 규제 매개변수

결정 트리는 훈련 데이터에 대한 제약 사항이 거의 없다는 장점을 가지고 있다. 결정 트리는 훈련되기 전에 파라미터 수가 결정되지 않는 **비파라미터 모델** (Parametric Model)이다. 비파라미터 모델은 학습 데이터에 맞추기 때문에 자유롭지만, 과대적합되기 쉽다는 단점이 있다.

결정 트리의 과대적합을 피하기 위해서 학습할 때 결정 트리의 자유도를 제한할 필요가 있다. 보통은 결정 트리의 최대 깊이를 설정함으로서 자유도를 제한한다. 사이킷런에서는 `max_depth` 매개변수를 조정하여 결정 트리의 최대 깊이를 설정할 수 있다.

`DecisionTreeClassifier` 에는 `max_depth` 외에도 결정 트리의 형태를 제한하는 다른 매개 변수가 있다. `min_sample_split, min_weight_fraction, max_leaf_nodes, max_features` 등을 설정할 수 있다. `min_` 으로 시작하는 매개 변수를 증가시키거나 `max_` 로 시작하는 매개 변수를 감소시키면 모델에 규제가 커진다.

<br/>

## 8) 회귀

결정 트리는 회귀에도 사용할 수 있다. 사이킷런에서는 `DecisionTreeRegressor` 를 사용하여 결정 트리를 사용한 회귀를 할 수 있다. 위에서 사용되었던 분류 모델과의 가장 큰 차이는 회귀 모델에서는 샘플이 특정 클래스가 아니라 특정 값으로 예측 된다는 점이다. CART 알고리즘은 불순도를 최소화 하는 방향으로 훈련 세트를 분할하는 대신 평균 제곱 오차(MSE)를 최소화하도록 분할하는 것을 제외하고는 위와 동일한 방식으로 작동한다. 회귀를 위한 CART 비용 함수는 아래와 같다.
$$
J(k, t_k) = \frac{m_{\text{left}}}{m}MSE_{\text{left}} + \frac{m_{\text{right}}}{m}MSE_{\text{right}}
$$

- $MSE_{node} = \sum_{i \in \text{node}} (\hat{y}_{\text{node}} - y^{(i)})^2$
- $\hat{y}_{\text{node}} = \frac{1}{m_{node}} \sum_{i \in \text{node}} y^{(i)}$

결정 트리는 분류 모델에서도 그랬던 것처럼 회귀에서도 과대적합되기가 쉽다. 그렇기 때문에 다양한 파라미터를 조정하여 결정 트리를 규제한다면 훨씬 더 일반화된 모델을 만들어낼 수 있을 것이다.

<br/>

## 9) 불안정성

위에서 살펴본 바, 결정 트리는 장점이 많은 모델이다. 이해하기 쉬운 화이트박스[^2] 모델이고, 분류나 회귀 등 다양한 곳에 사용할 수 있으며 성능도 뛰어나다. 하지만 결정 트리는 언제나 계단 모양의 결정 경계를 만들기 때문에 훈련 세트의 회전에 민감하다. 이런 문제를 해결하는 한 가지 방법은 훈련 데이터를 더 좋은 방향으로 회전시키는 PCA 방법을 활용하는 것이다.

결정 트리의 주된 문제는 훈련 데이터에 있는 작은 변화에도 매우 민감하다는 점이다. 데이터가 하나만 빠져도 다른 결정 경계가 그려질 수도 있다. 또한 사이킷런에서 `random_state` 매개변수를 지정하지 않으면 같은 훈련 데이터에서도 다른 모델을 얻게 될 수도 있다. 다음 장에 등장하는 랜덤 포레스트는 여러 개의 결정 트리의 예측을 평균내는 것으로 불안정성의 문제를 해결한다.

<br/>

[^1]: 결정 트리의 장점 중 하나는 데이터 전처리가 거의 필요하지 않다는 것이다. 특히 특성의 스케일을 맞추거나 평균을 원점에 맞추는 (StandardScaler() 등의) 작업이 필요하지 않다.
[^2]: 화이트박스와 블랙박스 - 결정 트리처럼 직관적이고 이해하기 쉬운 모델을 화이트박스 모델이라고 한다. 반대로 앞으로 보게 될 랜덤 포레스트나 신경망은 알기 어려운 블랙박스 모델이다. 블랙박스에 해당하는 알고리즘은 성능이 뛰어나고 예측을 만드는 연산 과정을 쉽게 확인할 수 있지만, 왜 그런 예측을 했는지 설명하기 어렵다.