---
layout: post
title: Bag of Words (BoW)
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) , [자연어 처리 인 액션](http://www.yes24.com/Product/Goods/89232661) 책을 참고하였습니다.



우리가 처리하고자 하는 문서는 각각의 길이가 모두 다르다. 이렇게 서로 다른 길이로 이루어진 문서를 컴퓨터에 집어넣기 위해서는 일정한 길이의 벡터로 변환해주어야 한다. 머신러닝이 다루는 데이터셋은 길이가 동일한 인스턴스로 이루어져 있기 때문이다.



# Bag of Words

문서는 다양한 단어가 모여있는 집합체이다. 때문에 특정 문서에 어떤 단어가 있는지 없는지, 혹은 몇 번이나 등장하는 지를 일일이 세어볼 수 있다. **백오브워즈(Bag of Words, BoW)** 는 '이렇게 헤아린 단어의 빈도로부터 무언가를 추론할 수 있을 것이다'라는 전제로부터 출발하는 방법이다. 백오브워즈는 문서를 벡터화하는 가장 고전적인 방법이다.

Bag of Words는 **단어-문서 행렬(Term-Document Matrix, TDM)** 을 그리는 것에서부터 시작한다. TDM은 분석하고자 하는 문서를 각 열에 배치하고 각 행에는 각각의 단어(Term)을 쓴 뒤 갯수를 세어 기록한 행렬이다. 이를 통해서 각 문서에 특정 단어가 등장하는지/아닌지 혹은 몇 번 등장하는지를 알 수 있다. (TDM을 전치시킨 Document-Term Matrix를 사용하기도 한다.)

<img src="https://user-images.githubusercontent.com/45377884/81645203-206ee380-9464-11ea-81ba-8a8fb9e52f03.png" alt="tdm" style="zoom:100%;" />

<p align="center" style="font-size:80%">이미지 출처 : <a href="https://github.com/pilsung-kang/text-analytics">Text-Analytics Github</a></p>

위 그림에 있는 두 가지 TDM 중 왼쪽에 있는 행렬은 Binary Representation으로 특정 Term이 문서에 있는지 없는지를 나타낸 표현방법이다. 오른쪽은 Frequency Representation은 문서에 단어가 몇 번이나 등장하는 지를 나타낸 표현방법이다. 후자가 더 많은 정보를 제공해주지만 특정 문제에 대해서는 전자가 더 좋은 결과를 보여주기도 하기 때문에 Task에 맞는 표현 방식을 사용하는 것이 좋다. 

Bag of Words의 단점은 단어의 순서를 무시한다는데 있다. 단어의 빈도(혹은 등장 유무)를 셀 뿐 등장 순서를 무시하기 때문에 *"John loves Mary"* 와 *"Mary loves John"* 을 TDM으로 표현하면 같은 표현이 된다. 반대로 Bag of Word의 방식으로 벡터 표현이 주어졌을 때 이게 *"John loves Mary"* 가 변환된 것인지 *"Mary loves John"* 에서 변환된 것인지 아는 것이 불가능하다는 것도 Bag of Words 방식의 단점이다.