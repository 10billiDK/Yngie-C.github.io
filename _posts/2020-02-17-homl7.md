---
layout: post
title: 7. 앙상블 학습과 랜덤 포레스트
category: Hands on Machine Learning
tag: Machine-Learning
---





일련의 예측기로부터 각기 다른 서브셋을 만들어 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 모델을 얻을 수 있다. 이러한 학습 알고리즘을 **앙상블 방법** (Ensemble method) 이라고 한다.



## 1) 투표 기반 분류기

특정 모델보다 더 성능이 좋은 모델을 만드는 가장 간단한 방법은 각 모델의 예측을 모아서 더 많이 선택된 클래스를 예측하는 것이다. 이처럼 다수결 투표로 정해지는 분류기를 직접 투표(Hard Voting, **하드보팅** ) 분류기라고 한다. 하드보팅 분류기는 [큰 수의 법칙]([https://ko.wikipedia.org/wiki/%ED%81%B0_%EC%88%98%EC%9D%98_%EB%B2%95%EC%B9%99](https://ko.wikipedia.org/wiki/큰_수의_법칙)) 덕분에 기존 모델보다 더 좋은 성능을 보일 확률이 높다. 투표에 참여하는 모든 분류기가 클래스의 확률을 측정할 수 있는 경우에는 간접 투표(Soft Voting, **소프트보팅** ) 방식을 사용할 수 있다. 소프트보팅 분류기는 각 분류기마다 클래스의 확률을 구하고, 그 합이 높은 것을 최종 클래스로 선택한다.

<br/>

## 2) 배깅과 페이스팅

앙상블에는 보팅 말고도 **배깅** 과 **페이스팅** 등 다양한 기법이 존재한다. 배깅은 훈련 세트에서 중복을 허용하여 샘플링(bootstrap)하는 방식이다. 반대로 중복을 허용하지 않고 샘플링하는 방식은 페이스팅(pasting)이라고 불린다. 모든 예측기가 훈련을 마치면 앙상블은 각 예측기의 결과를 모아서 새로운 샘플에 대한 예측을 만든다.

<br/>

## 3) 랜덤 패치와 랜덤 서브스페이스



<br/>

## 4) 랜덤 포레스트

- **랜덤 포레스트** (Random Forest)는 배깅 방법을 적용한 결정트리 앙상블이다. 대개 `DecisionTreeClassifier` 의 매개변수와 앙상블 자체를 제어하는 데 필요한 `BaggingClassifier` 의 매개변수를 모두 가지고 있다. 랜덤 포레스트에서는 노드를 분할할 때 최선의 특성을 찾는 대신 무작위로 선택한 특성들 중에서 최적의 특성을 찾는 식의 무작위성을 더 추가한다. 
- **엑스트라 트리** (Extra-Trees) : 트리를 더욱 랜덤하게 만들기 위해서 최적의 임곗값을 찾지 않고, 후보 특성을 사용하여 무작위로 분할한 다음 그 중에서 최상의 분할을 선택하는 방법이다. 최적 임곗값 찾기는 트리 알고리즘에서 가장 시간이 많이 걸리는 부분이므로 엑스트라 트리를 사용할 경우 더 빠르게 학습시킬 수 있다. 사이킷런에서 엑스트라 트리를 만드려면 `ExtraTreesClassifier` (회귀의 경우 `ExtraTreesRegressor` ) 를 사용한다.
- 특성 중요도 : 랜덤 포레스트의 또 다른 장점은 특성의 상대적 중요도를 측정하기 쉽다는 것이다. 사이킷런은 훈련이 끝날 때마다 특성의 중요도 점수를 측정한다.[^1] 훈련이 끝난 뒤 특성마다 자동으로 이 점수를 계산하고 중요도의 전체 합이 1이 되도록 결괏값을 정규화한다. 이 값은 `feature_importance_` 변수에 저장되어 있다. 

<br/>

## 5) 부스팅

- **부스팅** (boosting) : 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법이다. 부스팅 방법의 아이디어는 앞의 모델을 보완해나가면서 일련의 예측기를 학습시키는 것이다. 부스팅 방법에는 여러 가지가 있지만 가장 인기 있는 것은 **Adaboost** (Adaptive Boosting)와 **GBM** (Gradient Boosting Machine)이다. 

- **Adaboost** : 가장 간단한 부스팅의 아이디어는 이전 모델이 과소적합했던 샘플의 가중치를 높이는 것이다. 이렇게 해서 새로운 예측기는 학습하기 어려운 샘플에 점점 더 맞춰나가게 된다. 이것이 Adaboost가 모델을 개선해나가는 방식이다. 

  - AdaBoost의 에러율 ( $r_j$ )

  $$
  r_j = \frac{\sum_{i=1}^m w^i}{}
  $$



- **그래디언트 부스팅** (Gradient Boosting) : 그래디언트 부스팅도 Adaboost처럼 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가한다. 하지만 샘플의 가중치를 직접 수정하는 방법이 아니라 이전 예측기가 만든 잔여 오차에 새로운 예측기를 학습시키는 방법을 선택한다.

<br/>

## 6) 스태킹

- **스태킹** (Stacking) : 마지막 앙상블 모델은 스태킹이다. 각자의 모델로 예측을 진행한 후, 이 예측을 입력으로 받아 마지막 예측기( **메타 학습기** )가 최종 예측을 만들어낸다. 

<br/>

[^1]: 특성을 사용한 노드가 랜덤 포레스트 내에 있는 모든 트리에 걸쳐서 평균적으로 불순도를 얼마나 감소시키는지 확인하여 특성의 중요도를 체크한다. 더 정확히 말하면 가중치 평균이며 각 노드의 가중치는 연관된 훈련 샘플수와 동일하다. 