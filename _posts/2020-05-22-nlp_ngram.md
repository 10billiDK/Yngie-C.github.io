---
layout: post
title: N-gram
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) , [자연어 처리 인 액션](http://www.yes24.com/Product/Goods/89232661) 책을 참고하였습니다.



# N-gram

Count 기반의 문서 표현 방법의 마지막은 **엔그램(N-gram)** 에 기반한 방법이다. N-gram이란 확률을 추정할 때 전체 단어를 조합하는 대신 일부 단어 조합의 출현 빈도만을 계산하는 방법이다. 전체 텍스트를 조합하지 않는 이유는 희소성(Sparsity) 문제가 발생하기 때문이다. N이 7~8정도만 넘어가더라도 말뭉치에 단어 조합이 없는 경우가 많다. 이 때 대부분의 단어 조합 확률은 0으로 나오게 된다. 이런 문제를 해결하기 위해서 조합하는 단어의 수를 줄여야만 하는데 이 때 얼마만큼의 단어 조합을 볼 것인지를 N-gram으로 정하게 된다. N의 크기에 따라 Uni(1)-gram, Bi(2)-gram, Tri(3)-gram, 4-gram, 5-gram... 등이 있다.

이렇게 할 수 있는 이유는 **마르코프 가정(Markov Assumption)** 때문이다. 마르코프 가정이란 특정 시점에서 어떤 상태의 확률은 그 직전 상태에만 의존한다는 논리이다. 마르코프 가정을 하게 되면 이전에 있는 모든 시퀀스를 탐색하지 않고도 타겟 단어의 확률을 구해낼 수 있다. 수식으로 나타내면 아래와 같다.



$$
P(x_i|x_1, x_2, \cdots , x_{i-1}) \approx P(x_i|x_{i-k}, \cdots , x_{i-1})
$$



N-gram 에서 사용하는 $N = k+1$ 이므로 Bi(2)-gram, Tri(3)-gram의 식은 다음과 같이 나타낼 수 있다.



$$
\text{Bi-gram : } P(x_i|x_1, x_2, \cdots , x_{i-1}) \approx P(x_i|x_{i-1}) \qquad \\
\text{Tri-gram : } P(x_i|x_1, x_2, \cdots , x_{i-1}) \approx P(x_i|x_{i-2}, x_{i-1})
$$



연쇄 법칙을 통해 N-gram이 단어의 확률을 구하는 방식을 확장하면 다음과 같이 한 문장의 확률도 구해낼 수 있다.



$$
P(x_1, x_2, \cdots , x_{i-1}, x_i) \approx \prod^n_{i=1} P(x_i|x_{i-k}, \cdots , x_{i-1})
$$



N-gram 방법은 출현하지 않는 단어 시퀀스를 어떻게 표현할까에 대한 문제를 가진다. 말뭉치에 출현하지 않는 단어 조합은 모두 확률이 0이 되기 때문이다. 물론 *"컵을 먹다", "책꽂이를 먹다"* 처럼 실제로 쓰지 않는 말도 있겠지만 *"마라룽샤를 먹다"* 처럼 말은 되지만 말뭉치가 충분히 크지 않아 등장하지 않는 단어들도 있는데 두 단어 시퀀스 모두 N-gram 모델에서는 0이 된다. 이런 문제를 해결하기 위해서 출현 빈도값을 보정해주는 것을 **스무딩(Smoothing)** 혹은 **디스카운팅(Discounting)** 이라고 한다. 기본적인 스무딩의 방법은 모든 단어 시퀀스의 출현 빈도에 작은 값을 더해주는 것이다. 수식으로 나타내면 다음과 같다.


$$
P(w_i|w_{<i}) \approx \frac{\text{Count}(w_{<i},w_i) + k}{\text{Count}(w_{<i}) + kV} \approx \frac{\text{Count}(w_{<i},w_i) + (m/V)}{\text{Count}(w_{<i}) + m}
$$


N-gram 모델의 성능을 높이기 위한 방법으로 **Kneser-Ney(KN) 디스카운팅** 이 있다. KN 디스카운팅의 기본적인 아이디어는 목적 단어 $w$ 이전에 나타나는 단어 $v$ 가 얼마나 다양한지를 알아내는 데에 있다. 다양한 단어 뒤에 나타나는 단어일수록 말뭉치에서 보지 못한 단어 시퀀스로 나타날 가능성이 높다는 내용이다. KN 디스카운팅에 있는 Score를 다음과 같이 모델링한다.


$$
\text{Score}_{\text{continuation}}(w) \propto |\{v:\text{Count}(v,w) > 0\}| \\
\text{Score}_{\text{continuation}}(w) = \frac{|\{v:\text{Count}(v,w) > 0\}|}{\sum_{w^{\prime}}|\{v:\text{Count}(v,w^{\prime}) > 0\}|}
$$


식에서 $w^{\prime}$ 은 $v$ 뒤에 등장하는 다른 단어들을 나타낸다. KN 디스카운팅을 사용하여 단어가 등장할 확률 $P_{KN}$ 을 모델링하면 아래와 같이 나타낼 수 있다. 아래 식에서 $d$ 는 상수이다. (보통은 0.75를 사용한다.)


$$
P_{KN} (w_i|w_{i-1}) = \frac{\max(\text{Count}(w_{i-1},w_i) - d,0)}{\text{Count}(w_i)} + \lambda(w_{i-1}) \times \text{Score}_{\text{continuation}}(w_i) \\
\text{where} \quad \lambda(w_{i-1}) = \frac{d}{\sum_v \text{Count}(w_{i-1})} \times |\{w:c(w_{i-1},v) > 0\}|
$$


가장 많이 사용되는 방법은 위 수식에 적당한 수정을 더해준 Modified-KN 디스카운팅이 많이 사용된다. 식은 복잡하지만 SRILM 이라는 툴킷에 구현되어 있기 때문에 그것을 사용하면 된다.

N-gram에서 단어 시퀀스가 길어질수록 실제 말뭉치에 등장하는 횟수가 급격히 줄어들게 된다. 백오프 방식은 이 문제를 해결하기 위해 도입한 방법이다. N이 클 때 작은 N의 조합으로 인터폴레이션(Interpolation, 보간) 함으로써 더 높은 일반화 효과를 얻을 수 있다.


$$
\tilde{P}(w_n|w_{n-k}, \cdots , w_{n-1}) = \lambda_1 P(w_n|w_{n-k}, \cdots , w_{n-1}) \qquad \qquad \\
\qquad \qquad \qquad \quad + \lambda_2 P(w_n|w_{n-k+1}, \cdots , w_{n-1}) \\
+ \cdots \qquad \quad \\
+ \lambda_k P(w_n), \\
\text{where} \quad \sum_i \lambda_i = 1
$$
