---
layout: post
title: 벡터의 직교성 (Orthogonality)
category: Linear Algebra
tag: Linear-Algebra
---



# Orthogonal Vector Space

## Orthogonal Vector Space & Subspace

- 왜 **직교벡터** (Orthogonal Vector)가 중요한가?
  - 선형 독립인 기저(Basis) 벡터
  - 선형 결합(Linear Combination)시 계산하기가 쉽다.
- 두 벡터( $\mathbf{x}, \mathbf{y}$ )가 이루는 각도 $(0^\circ < \text{angle} < 90^\circ)$ : 내적으로 구할 수 있다.
  
  - $\mathbf{x}^T\mathbf{y} = 0 \rightarrow \text{angle} = 90^\circ$ 
  - $\mathbf{x}^T\mathbf{y} < 0 \rightarrow \text{angle} > 90^\circ$ 
  - $\mathbf{x}^T\mathbf{y} > 0 \rightarrow \text{angle} < 90^\circ$ 
  
- 영벡터가 아닌 벡터 $\vec{v_1}, \vec{v_2}, \vec{v_3}, ... , \vec{v_n}$ 에 대하여, 벡터들이 서로 직교한다면 $\vec{v_i}^T\vec{v_j} = 0 \quad (i \neq j, \vert\vert\vec{v_1}\vert\vert \neq 0)$  이고 벡터들은 서로 **선형 독립**이다.
- **직교 부분공간** (Orthogonal Subspace) : 한 부분공간 내의 모든 벡터들은 직교인 부분공간의 모든 벡터들과 직교한다.
  - Row space $\perp$ Null Space,  $(\mathbb{C}(\mathbf{A}^T) \perp \mathbb{N}(\mathbf{A})) \in \mathbb{R}^n$ 
  - Column space $\perp$ Left Null Space,  $(\mathbb{C}(\mathbf{A}) \perp \mathbb{N}(\mathbf{A}^T)) \in \mathbb{R}^m$
- 벡터 공간의 차원
  - 벡터 공간을 Span할 수 있는 독립벡터의 개수
  - $\mathbf{A}$ 의 랭크
- 직교 보충부분공간

<br/>

## 2) Cosines and Projection onto Line

- $\mathbf{v}$ 를 $\mathbf{u}$ 로 사영(Projection) 하는 과정 

![](http://www.maths.usyd.edu.au/u/MOW/vectors/images/v105x.gif)

- 사영을 벡터로 나타내면 $\mathbf{u}^T\mathbf{v}$ 이고, 그 값을 유도되는 식은 다음과 같다. 처음 식은 제 2 코사인 법칙으로부터 시작한다.

$$
\vert\vert\mathbf{u} - \mathbf{v}\vert\vert^2 = \vert\vert\mathbf{u}\vert\vert^2 + \vert\vert\mathbf{v}\vert\vert^2 - 2\vert\vert\mathbf{u}\vert\vert \cdot \vert\vert\mathbf{v}\vert\vert \cos \theta \\
(\mathbf{u} - \mathbf{v})^T(\mathbf{u} - \mathbf{v}) = \mathbf{u}^T\mathbf{u} + \mathbf{v}^T\mathbf{v} - 2\vert\vert\mathbf{u}\vert\vert \cdot \vert\vert\mathbf{v}\vert\vert \cos \theta \\
\mathbf{u}^T\mathbf{v} + \mathbf{v}^T\mathbf{u} = 2\vert\vert\mathbf{u}\vert\vert \cdot \vert\vert\mathbf{v}\vert\vert \cos \theta \\
\therefore \mathbf{u}^T\mathbf{v} = \vert\vert\mathbf{u}\vert\vert \cdot \vert\vert\mathbf{v}\vert\vert \cos \theta
$$



- $(\mathbf{u} - \hat{\mathbf{x}} \cdot \mathbf{v}) \perp \mathbf{v}$ 일 때, 

$$
\mathbf{u}^T(\mathbf{v} - \hat{\mathbf{x}} \cdot \mathbf{u}) = 0 \\
\mathbf{u}^T\mathbf{v} - \hat{\mathbf{x}} \cdot \vert\vert\mathbf{u}\vert\vert^2 = 0 \\
\hat{\mathbf{x}} = \frac{\mathbf{u}^T \mathbf{v}}{\mathbf{u}^T\mathbf{u}} \\
\mathbf{p} = \hat{\mathbf{x}} \cdot \mathbf{u}
$$

 <br/>

## 3) Projection and Least Squares

- Projection Matrix, $\mathbf{P}$ : 위의 식을 특정 벡터( $\mathbf{u}, \mathbf{v}$ )가 아닌 행렬 $\mathbf{A}$ 에 대하여 나타내면, $\mathbf{b} - \mathbf{A}\hat{\mathbf{x}} \perp a_i$ 일 때

$$
\mathbf{A}^T(\mathbf{b} - \mathbf{A}\hat{\mathbf{x}}) = 0 \\
\mathbf{A}^T\mathbf{A}\hat{\mathbf{x}} = \mathbf{A}^T\mathbf{b} \\
\hat{\mathbf{x}} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}
$$

이며, Projection Matrix $\mathbf{P}\mathbf{b} = \mathbf{A}\hat{\mathbf{x}} = \mathbf{A}(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}$ 를 만족하므로, $\mathbf{P} = \mathbf{A}(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T$ 이다.

- **Gram-Schmidt orthogonalization**

먼저 2차원에서, $\vec{v}$ 를 $\vec{u}$ 로 사영시킨다고 하자. 

<p align="center"><img src="https://t1.daumcdn.net/cfile/tistory/253B584B591BED9B37?download" style="zoom:50%;" /></p>



<p align="center"><img src="https://gaussian37.github.io/assets/img/math/la/gram_schmidt_process/2.png" style="zoom:40%;" /></p>





 <br/>

## 4) Orthogonal Basis

- $q_1, q_2, q_3, ... , q_n$ 는 직교이며 크기가 1(즉, Orthonormal)인 벡터라고 하면 다음의 경우를 만족한다.

$$
q_i \cdot q_j = \begin{cases} 0 \qquad \text{when } i \neq j \\ 1 \qquad \text{when } i = j\end{cases}
$$

즉, $\mathbf{Q} = \left[\begin{array}{ccc} q_1 \text{ } q_2 \text{ } ... \text{ } q_n \end{array} \right]$ 인 $\mathbf{Q}$ 에 대하여, $\mathbf{Q}^T\mathbf{Q} = \mathbf{I}$ 이므로 $\mathbf{Q}^T = \mathbf{Q}^{-1}$ (Left Inverse) 이다. 



<br/>

