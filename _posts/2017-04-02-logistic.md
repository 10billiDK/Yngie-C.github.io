---
title: 로지스틱 회귀(Logistic Regression)와 포아송 회귀(Poisson Regression)
category: Machine Learning
tag: Logistic Regression, Poisson Regression
---

이번 포스팅에선 범주형 변수를 예측하는 모델인 **로지스틱 회귀(Logistic Regression)**와 이산형 변수에 적용되는 모델인 **포아송 회귀(Poisson Regression)**에 대해 살펴보려고 합니다. 이번 글은 고려대 강필성 교수님과 역시 같은 대학의 김성범 교수님 강의를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.



## 로지스틱 회귀의 문제의식

**다중선형회귀(Multiple Linear Regression)**는 수치형 설명변수 X와 연속형 숫자로 이뤄진 종속변수 Y간의 관계를 선형으로 가정하고 이를 가장 잘 표현할 수 있는 회귀계수를 데이터로부터 추정하는 모델입니다. 이 회귀계수들은 모델의 예측값과 실제값의 차이, 즉 **오차제곱합(error sum of squares)**을 최소로 하는 값들입니다. 이를 만족하는 최적의 계수들은 회귀계수에 대해 미분한 식을 0으로 놓고 풀면 **명시적인 해**를 구할 수 있습니다. 어쨌든 설명변수가 p개인 다중선형회귀의 일반 식은 아래와 같이 쓸 수 있습니다.

$$y={ \beta  }_{ 0 }+{ \beta  }_{ 1 }{ x }_{ 1 }+{ \beta  }_{ 2 }{ x }_{ 2 }+...+{ \beta  }_{ p }{ x }_{ p }+\varepsilon $$

한번 예를 들어보겠습니다. 33명의 성인 여성에 대한 나이와 혈압 데이터가 좌측 하단 표와 같이 주어졌다고 칩시다. 그러면 우리는 오차제곱합을 최소로 하는 회귀계수를 구할 수 있고 이를 그래프로 그리면 우측 하단 그림과 같습니다. 나이라는 설명변수에 대응하는 계수는 1.222로 나타났는데요, 이를 통해 우리는 나이를 한살 더 먹으면 혈압이 1.222mm/Hg만큼 증가한다는 걸 알 수 있게 됩니다.

<a href="http://imgur.com/muiAC6k"><img src="http://i.imgur.com/muiAC6k.png" width="700px" title="source: imgur.com" /></a>

그러면 혈압이라는 연속형 숫자 대신 범주형 변수를 이용해 위와 같은 회귀모델을 구축한다면 어떤 일이 발생할까요? 좌측 하단과 같이 나이와 암 발생여부(1이면 발병, 0이면 정상) 데이터가 주어졌다고 칩시다. 이를 위와 동일한 방식으로 회귀모델을 구축하고 그래프로 그리면 우측 하단과 같이 우스꽝스러운 모양이 될 겁니다.

<a href="http://imgur.com/Co19p9c"><img src="http://i.imgur.com/Co19p9c.png" width="700px" title="source: imgur.com" /></a>

위와 같은 문제가 발생하는 근본적인 이유는 종속변수 Y의 성질 때문입니다. 혈압의 경우 숫자 그 자체로 의미를 지니는 변수이지만, 암 발생여부는 그렇지 않습니다. 발병(1)과 정상(0) 사이에 중간 범주가 없을 뿐더러 심지어 정상을 1, 발병을 0으로 바꾸어도 큰 상관이 없습니다. 숫자가 아무 의미를 지니지 않는다는 얘기죠. 이처럼 Y가 **범주형(categorical)** 변수일 때는 다중선형회귀 모델을 그대로 적용할 수 없다는 겁니다. 이러한 문제 때문에 로지스틱 회귀 모델이 제안됐습니다.



## 로지스틱 함수와 Odds

우선 **로지스틱 함수(Logistic Function)**와 **승산(Odds)**에 대해 알아보겠습니다. 로지스틱 회귀의 뼈대가 되는 아이디어이기 때문입니다.

실제 많은 자연, 사회현상에서는 특정 변수에 대한 확률값이 선형이 아닌 S-커브 형태를 따르는 경우가 많다고 합니다. 이러한 S-커브를 함수로 표현해낸 것이 바로 로지스틱 함수입니다. 분야에 따라 **시그모이드 함수**로도 불리기도 하는데요, x값으로 어떤 값이든 입력받을 수가 있지만 출력 결과는 항상 0에서 1사이 값이 됩니다. 즉 **확률밀도함수(probability density function)** 요건을 충족시키는 함수라는 이야기입니다. 그 식과 그래프 모양은 아래와 같습니다.

$$y=\frac { 1 }{ 1+{ e }^{ -x } } $$

<a href="http://imgur.com/E0eI8OU"><img src="http://i.imgur.com/E0eI8OU.png" width="500px" title="source: imgur.com" /></a>

승산(Odds)이란 임의의 사건 A가 발생하지 않을 확률 대비 일어날 확률의 비율을 뜻하는 개념입니다. 아래와 같은 식으로 쓸 수가 있습니다.

$$odds=\frac { P(A) }{ P({ A }^{ c }) } =\frac { P(A) }{ 1-P(A) } $$

만약 P(A)가 1이라면 승산은 무한대로 치솟을 겁니다. 반대로 P(A)가 0이라면 0이 될 겁니다. 바꿔 말하면 승산이 커질수록 사건 A가 발생할 확률이 커진다고 이해해도 될 겁니다. 승산을 그래프로 그리면 아래와 같습니다.

<a href="http://imgur.com/CGvcrV7"><img src="http://i.imgur.com/CGvcrV7.png" width="200px" title="source: imgur.com" /></a>



## 이항 로지스틱 회귀

이제 우리는 범주가 두 개인 분류 문제를 풀어야 합니다. 앞선 챕터에서 말씀드렸듯 종속변수 $Y$가 연속형 숫자가 아닌 범주일 때는 기존 회귀 모델을 적용할 수 없습니다. 그럼 문제를 바꿔서 풀어봅시다. 회귀식의 장점은 그대로 유지하되 종속변수 $Y$를 범주가 아니라 연속형 숫자인 **승산(Odds)**으로 두자는 말입니다. 입력벡터 $x_i$가 범주 0일 확률을 분자로 두고, 1에 해당할 확률을 분모로 둔 것이 바로 승산입니다. 아래 식처럼 쓸 수 있습니다.

$$Odds=\frac { P(Y=1|X=\overrightarrow { { x }_{ i } } ) }{ 1-P(Y=1|X=\overrightarrow { { x }_{ i } } ) } ={ \beta  }_{ 0 }+{ \beta  }_{ 1 }{ x }_{ 1 }+{ \beta  }_{ 2 }{ x }_{ 2 }+...+{ \beta  }_{ p }{ x }_{ p }$$

하지만 좌변(승산)의 범위는 0에서 무한대의 범위를 갖습니다. 하지만 우변(회귀식)은 음의 무한대에서 양의 무한대 범위를 가지기 때문에 식이 성립하지 않는 경우가 존재할 수 있습니다. 여기서 승산에 로그를 취하면 어떻게 될까요? 로그 승산의 그래프는 아래와 같은데요. 이렇게 되면 로그 승산 또한 음의 무한대에서 양의 무한대로 그 범위가 변환되게 됩니다. 이제야 비로소 좌변(승산)이 우변(회귀식)의 범위와 일치하게 되는 셈이지요. 

<a href="http://imgur.com/z0u3ytN"><img src="http://i.imgur.com/z0u3ytN.png" width="200px" title="source: imgur.com" /></a>

로그 승산을 활용해 이진 분류를 위한 회귀분석 식을 쓰면 아래와 같은 형태가 됩니다. 예컨대 관측치 벡터의 첫번째 요소인 $x_1$에 대응하는 1번 회귀계수가 학습 결과 2.5로 정해졌다고 칩시다. 그렇다면 $x_1$이 1단위 증가하면 범주 1에 해당하는 로그 승산이 2.5 커집니다. **로그 승산이 양수이면 해당 범주에 속할 확률, 즉 P(Y=1\|X=x)도 커지고, 음수라면 그 확률은 줄어들게 됩니다.** 로지스틱 회귀 모델 회귀계수의 부호는 해당 변수가 증가할 때 특정 범주에 속할 확률이 증가하는지, 감소하는지 방향을 나타내고, 회귀계수의 절대값은 확률 증가/감소의 강도를 드러냅니다.

$$\log { (Odds) } =\log { (\frac { P(Y=1|X=\overrightarrow { { x }_{ i } } ) }{ 1-P(Y=1|X=\overrightarrow { { x }_{ i } } ) } ) } ={ \beta  }_{ 0 }+{ \beta  }_{ 1 }{ x }_{ 1 }+{ \beta  }_{ 2 }{ x }_{ 2 }+...+{ \beta  }_{ p }{ x }_{ p }$$

위 식 양변에 exp의 지수를 취해주면 아래와 같습니다.

$$\frac { P(Y=1|X=\overrightarrow { x_{ i } } ) }{ 1-P(Y=1|X=\overrightarrow { { x }_{ i } } ) } ={ e }^{ { \beta  }_{ 0 }+{ \beta  }_{ 1 }{ x }_{ 1 }+{ \beta  }_{ 2 }{ x }_{ 2 }+...+{ \beta  }_{ p }{ x }_{ p } }$$

위 식을 입력값 x가 주어졌을 때 해당 데이터가 범주 1이 될 확률을 기준으로 정리해주면 아래와 같습니다.

$$\frac { P(Y=1|X=\overrightarrow { { x }_{ i } } ) }{ 1-P(Y=1|X=\overrightarrow { { x }_{ i } } ) } ={ e }^{ { \beta  }_{ 0 }+{ \beta  }_{ 1 }{ x }_{ 1 }+{ \beta  }_{ 2 }{ x }_{ 2 }+...+{ \beta  }_{ p }{ x }_{ p } }$$

위 식 어디서 많이 본 형태 아닙니까? 네 맞습니다. 바로 직전 챕터에서 설명드린 로지스틱 함수의 꼴과 같습니다. 이 식에 범주 정보를 모르는 관측치 x를 넣으면 범주 1에 속할 확률을 반환해 줍니다. 그 확률이 0.5보다 크면 범주 1로, 이보다 작으면 범주 0으로 분류하면 됩니다.



## 다항 로지스틱 회귀

그렇다면 범주가 세 개 이상인 다항 로지스틱 회귀는 어떻게 이해하면 될까요? 우선 범주가 3개뿐이라고 가정해 보겠습니다. 그렇다면 지금까지의 방식 그대로 로그 승산을 좌변, 회귀식을 우변에 둔 아래 식을 쓸 수가 있습니다. 

$$\log { \frac { P(Y=1|X=\overrightarrow { { x }_{ i } } ) }{ P(Y=3|X=\overrightarrow { { x }_{ i } } ) }  } ={ \beta  }_{ 1 }^{ T }{ \overrightarrow { { x }_{ i } }  }\\ \log { \frac { P(Y=2|X=\overrightarrow { { x }_{ i } } ) }{ P(Y=3|X=\overrightarrow { { x }_{ i } } ) }  } ={ \beta  }_{ 2 }^{ T }{ \overrightarrow { { x }_{ i } }  }$$

여기서 회귀계수 $β$는 상수항을 포함한 벡터 형태이고 관측치 $x_i$는 상수항에 대응하는 요소 1이 추가된 벡터입니다.

$$\overrightarrow { { x }_{ i } } =\begin{bmatrix} 1 & { x }_{ 1 } & { x }_{ 2 } & ... & { x }_{ p } \end{bmatrix}\\ \overrightarrow { { \beta  }_{ k } } =\begin{bmatrix} { \beta  }_{ k0 } & \beta _{ k1 } & \beta _{ k2 } & ... & \beta _{ kp } \end{bmatrix}$$

위 두 개 수식 양변에 exp를 취해준 뒤 모든 식을 더해 이를 정리해주면 아래와 같이 식을 다시 쓸 수 있습니다. 저 같은 경우 승산을 취할 때 분모로 범주 3을 선택했는데, 사실 1을 취하든 2을 선택하든 관계는 없습니다. 어쨌든 관측치 X가 주어졌을 때 해당 관측치가 범주 3에 속할 확률은 아래와 같습니다.

$$P(Y=3|X=\overrightarrow { { x }_{ i } } )=\frac { { e }^{ { { \beta  } }_{ 3 }^{ T }\overrightarrow { { x }_{ i } }  } }{ 1+{ e }^{ { { \beta  } }_{ 1 }^{ T }\overrightarrow { { x }_{ i } } +{ { \beta  } }_{ 2 }^{ T }\overrightarrow { { x }_{ i } }  } } $$

위와 같은 방식으로 범주 1에 속할 확률, 2에 속할 확률도 얼마든지 구할 수 있습니다. 이렇게 각 범주에 속할 확률을 모두 구한 뒤 가장 높은 확률값을 내는 범주로 분류를 하는 방식입니다. 

최종적으로 관측치 $x_i$가 k번째 클래스로 분류될 확률은 아래 식과 같습니다.

$$P(Y=k|X=\overrightarrow { x } )=\frac { { e }^{ { \beta  }_{ k }^{ T }\overrightarrow { x }  } }{ 1+\sum _{ i=1 }^{ K-1 }{ { e }^{ { \beta  }_{ i }^{ T }\overrightarrow { x }  } }  }\quad (k=1,2,...,K-1)$$



## 로지스틱 회귀의 파라메터 추정

로지스틱 회귀는 **베르누이 시행(Bernoulli trial)**을 전제로 하는 모델입니다. 베르누이 시행이란 어떤 실험이 두 가지 결과만을 가지는 경우를 뜻합니다. 베르누이 시행의 결과에 따라 0(실패) 또는 1(성공)의 값을 대응시키는 **확률변수(random variable)**를 베르누이 확률변수라 합니다. 이 확률변수의 확률분포를 베르누이 분포라고 합니다. 베르누이 확률변수 Y의 분포는 아래 표와 같습니다.

|   $y_i$    |   0    |  1   |
| :--------: | :----: | :--: |
| $P(Y=y_i)$ | $1- p$ | $p$  |

위 표를 수식으로 정리하면 아래와 같습니다.

$$P(Y={ y }_{ i })={ p }^{ { y }_{ i } }{ (1-p) }^{ 1-{ y }_{ i } }\quad ({ y }_{ i }=0,1)$$

베르누이 확률변수 Y에 관한 **우도함수(likelihood function)**은 아래와 같습니다.

$$L=\prod _{ i }^{  }{ { p }^{ { y }_{ i } }{ (1-p) }^{ 1-{ y }_{ i } }} $$

로지스틱 회귀의 파라메터 β는 **최우추정법(Maximum Likelihood Estimation)**으로 구합니다. 여기에 로그를 취해도 대소관계는 달라지지 않으므로 **로그 우도함수(log-likelihood function)**를 최대로 하는 회귀계수 β가 우리가 알고 싶은 값이 됩니다. 학습데이터에 관측치가 $i​$개가 있고, 범주가 2개뿐이라고 하면 이항로지스틱 모델의 로그 우도함수는 아래와 같이 전개할 수 있습니다.


$$
\begin{align*}
\ln { L } &=\ln { \prod _{ i }^{  }{ { p }^{ { y }_{ i } }{ (1-{ p }) }^{ 1-{ y }_{ i } } }  } \\ &=\ln { \prod _{ i }^{  }{ { (\frac { p }{ 1-p } ) }^{ { y }_{ i } } }  } +\sum _{ i }^{  }{ \ln { (1-p) }  } \\ &=\sum _{ i }^{  }{ { y }_{ i }\ln { (\frac { p }{ 1-p } ) }  } +\sum _{ i }^{  }{ \ln { (1-p) }  } \\ &=\sum _{ i }^{  }{ { y }_{ i }({ \beta  }^{ T }{ \overrightarrow { x_{ i } }  }) } +\sum _{ i }^{  }{ \ln { (1+exp({ \beta  }^{ T }\overrightarrow { x_{ i } } )) }  }  \end{align*}
$$


다만 위 로그 우도함수는 추정 대상 파라메터인 회귀계수 β에 대해 비선형이기 때문에 선형회귀와 같이 명시적인 해가 존재하지 않습니다. 따라서 **뉴턴-랩슨법(Newton-Raphson Method)** 같은 점진적이고 반복적인 방식을 통해 해를 찾게 됩니다. 예컨대 아래와 같습니다. (출처 : [영문 위키피디아](https://en.wikipedia.org/wiki/Newton%27s_method))

<a href="http://imgur.com/uCLGlkK"><img src="http://i.imgur.com/uCLGlkK.gif" width="400px" title="source: imgur.com" /></a>



## 로지스틱 회귀를 공간적 이해하기

이항 로지스틱 회귀분석은 $d$차원의 데이터를 둘로 구분하는 $d-1$차원의 초평면을 찾는 것으로 이해할 수 있습니다. 가령 3차원 데이터가 주어졌다고 칩시다. 그러면 여기에서 도출된 로지스틱 회귀 모델은 아래와 같이 시각화할 수 있습니다.

<a href="http://imgur.com/SSCvRID"><img src="http://i.imgur.com/SSCvRID.png" width="500px" title="source: imgur.com" /></a>

로지스틱 함수의 분모에서 $β^Tx$에 주목해 봅시다. $β^Tx$가 0이라면 $y$(범주가 1이 될 확률)은 0.5가 됩니다. $β^Tx$가 양수라면 $y$가 1에 가깝고 음수라면 0에 가깝습니다. 그렇다면 $β^Tx=0$라는 직선이 원 데이터의 범주를 둘로 가르고 있는 셈입니다. 

이를 4차원 이상 공간으로 확장하면 로지스틱 회귀분석은 $d$차원의 데이터를 구분하는 $d-1$차원의 초평면을 찾는 것으로 이해할 수 있습니다.

여기에서 $β=β_0-β_1$로 둡시다. 그렇다면 범주가 1이 될 확률은 아래와 같이 쓸 수 있습니다.
$$
\begin{align*}
y&=\frac { 1 }{ 1+exp({ -\beta  }^{ T }x) } \\ &=\frac { exp(-{ \beta  }_{ 1 }^{ T }x) }{ exp(-{ \beta  }_{ 1 }^{ T }x)+exp({ -\beta  }^{ T }x-{ \beta  }_{ 1 }^{ T }x) } \\ &=\frac { exp(-{ \beta  }_{ 1 }^{ T }x) }{ exp(-{ \beta  }_{ 1 }^{ T }x)+exp\{ -{ (\beta  }_{ 0 }^{ T }-{ \beta  }_{ 1 }^{ T }+{ \beta  }_{ 1 }^{ T })x\}  } \\ &=\frac { exp(-{ \beta  }_{ 1 }^{ T }x) }{ exp(-{ \beta  }_{ 1 }^{ T }x)+exp(-{ \beta  }_{ 0 }^{ T }x) }
\end{align*}
$$

이항 로지스틱 회귀이므로 범주가 0일 확률은 $1-y$가 됩니다. 아래와 같습니다.
$$
\begin{align*}
1-y&=1-\frac { exp(-{ \beta  }_{ 1 }^{ T }x) }{ exp(-{ \beta  }_{ 1 }^{ T }x)+exp(-{ \beta  }_{ 0 }^{ T }x) } \\ &=\frac { exp(-{ \beta  }_{ 1 }^{ T }x)+exp(-{ \beta  }_{ 0 }^{ T }x)-exp(-{ \beta  }_{ 1 }^{ T }x) }{ exp(-{ \beta  }_{ 1 }^{ T }x)+exp(-{ \beta  }_{ 0 }^{ T }x) } \\ &=\frac { exp(-{ \beta  }_{ 0 }^{ T }x) }{ exp(-{ \beta  }_{ 1 }^{ T }x)+exp(-{ \beta  }_{ 0 }^{ T }x) }
\end{align*}
$$
$y$와 $1-y$ 모두 분모가 같으므로 분자가 결정적인 역할을 합니다. 범주가 1이 될 확률값에 중요한 역할을 하는 건 $β_1$, 0이 될 확률값에 중요한 역할을 하는 건 $β_0$입니다. 이 둘을 각각 범주 1과 범주 0의 대표 벡터로 해석할 수 있습니다. 이를 그림으로 도식화하면 아래와 같습니다.

<a href="http://imgur.com/HCoNm6w"><img src="http://i.imgur.com/HCoNm6w.png" width="400px" title="source: imgur.com" /></a>

그런데 두 클래스의 데이터가 같은 방향에 존재할 경우 $β_0, β_1$만으로는 분류하기 어려울 수도 있습니다. 아래 그림처럼요.

<a href="http://imgur.com/uBI1OKz"><img src="http://i.imgur.com/uBI1OKz.png" width="400px" title="source: imgur.com" /></a>

벡터 $β$에 포함된 bias term $β_{k0}$(스칼라)는 범주를 잘 구분하도록 입력값 $x$를 평행이동시키는 역할을 합니다.
$$
\begin{align*}
\overrightarrow {  x  } &=\begin{bmatrix} 1 & { x }_{ 1 } & { x }_{ 2 } & ... & { x }_{ p } \end{bmatrix}\\ \overrightarrow { { \beta  }_{ k } } &=\begin{bmatrix} { \beta  }_{ k0 } & \beta _{ k1 } & \beta _{ k2 } & ... & \beta _{ kp } \end{bmatrix}\\ exp(-\overrightarrow { { \beta  }_{ k }^{ T } } \overrightarrow { x } )&=exp\{-({ \beta  }_{ k0 }+\beta _{ k1 }{ x }_{ 1 }+...+\beta _{ kp }{ x }_{ p })\}\\ &=exp\left[ -\{ \beta _{ k1 }{ (x }_{ 1 }-{ k }_{ 1 })+...+\beta _{ kp }{ (x }_{ p }-{ k }_{ p })\}  \right] 
\end{align*}
$$
<a href="http://imgur.com/34Eua0Z"><img src="http://i.imgur.com/34Eua0Z.png" width="400px" title="source: imgur.com" /></a>

마지막으로 로지스틱 회귀와 뉴럴네트워크의 관계에 대해 살펴보겠습니다. 로지스틱 함수(=시그모이드 함수)를 이용하는 **feed-forward neural network**는 로지스틱 회귀를 여러 번하는 것과 동일합니다. 아래 그림(출처 : 스탠포드 CS231n)을 보시면 **활성함수(activation function)**로 시그모이드를 쓰면 정확히 로지스틱 회귀와 같습니다.

<a href="http://imgur.com/euw7qQu"><img src="http://i.imgur.com/euw7qQu.png" width="400px" title="source: imgur.com" /></a>

뉴럴네트워크의 특징은 선형 분리가 불가능한 데이터를 조금씩 선형 분리가 가능한 공간으로 바꿔주는 것입니다. 이를 **representation learning**이라고 합니다. 이와 관련해 자세한 내용은 [이곳](https://ratsgo.github.io/deep%20learning/2017/04/25/representationlearning/)을 참고하시기 바랍니다.




## 포아송 회귀

일반 선형 회귀가 **정규분포(Normal Distribution)**, 로지스틱 회귀가 베르누이분포를 전제로 하는 모델이라면 포아송 회귀는 종속변수 Y가 포아송분포를 따를 것이라는 가정 하에 구축된 모델입니다. 포아송 분포란 단위 시간 안에 어떤 사건이 몇 번 발생할 것인지를 표현하는 **이산확률분포**인데요. 즉 포아송 회귀는 종속변수 Y의 값이 숫자로서 의미가 있지만, 그 숫자가 연속적이지 않을 때 사용하는 모델입니다. 예컨대 하루 교통사고 건수, 특정 질병 환자 수 등이 여기에 해당합니다. 

정해진 시간 안에 어떤 사건이 일어날 횟수에 대한 **기댓값**을 λ라고 했을 때 포아송 분포의 확률함수는 다음과 같이 정의됩니다.

$$P(Y=y)=\frac { { \lambda  }^{ y }{ e }^{ -\lambda  } }{ y! } \quad (y=0,1,2...)$$

포아송 회귀 모델은 다음과 같이 정의됩니다. 포아송 회귀의 파라메터 β도 로지스틱 회귀와 마찬가지로 최우추정법을 통해 구합니다. 명시적인 해가 없기 때문에 반복적인 방식으로 계산하게 됩니다. 

$$E[{ y }_{ i }]={ \lambda  }_{ i }=exp({ x }_{ i }^{ T }\beta )$$

지금까지 논의한 내용을 토대로 실전적인 팁을 드리자면, **종속변수 Y가 연속형 숫자라면 일반 선형 회귀를, 범주형 변수라면 로지스틱 회귀를, 숫자로서의 의미를 가지나 이산형 변수라면 포아송 회귀를 사용하면 좋을 것 같습니다.**