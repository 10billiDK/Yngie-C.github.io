---
title: Topic Modeling, LDA
category: From frequency to semantics
tag: LDA
---

이번 글에서는 말뭉치로부터 토픽을 추출하는 **토픽모델링(Topic Modeling)** 기법 가운데 하나인 **잠재디리클레할당(Latent Dirichlet Allocation, LDA)**에 대해 살펴보도록 하겠습니다. 이번 글 역시 고려대 강필성 교수님 강의를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.



## 모델 개요

LDA란 주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지에 대한 확률모형입니다. LDA는 토픽별 단어의 분포, 문서별 토픽의 분포를 모두 추정해 냅니다. LDA의 개략적인 도식은 다음과 같습니다.

<a href="http://imgur.com/r5e5qvs"><img src="http://i.imgur.com/r5e5qvs.png" width="600px" title="source: imgur.com" /></a>

우선 LDA는 특정 토픽에 특정 단어가 나타날 확률을 내어 줍니다. 예컨대 위 그림에서 노란색 토픽엔 gene이라는 단어가 등장할 확률이 0.04, dna는 0.02, genetic은 0.01입니다. 대략 '유전자' 관련 토픽임을 알 수 있네요. 

이번엔 문서를 보겠습니다. 주어진 문서를 보면 파란색, 빨간색 토픽에 해당하는 단어보다는 노란색 토픽에 해당하는 단어들이 많네요. 따라서 위 문서의 토픽은 유전자일 가능성이 큽니다. 이렇듯 문서의 토픽 비중 또한 LDA의 산출 결과물입니다. 

위 그림 우측에 있는 'Topic proportions & assignments'가 LDA의 핵심 프로세스입니다. LDA는 문서가 생성되는 과정을 확률모형으로 모델링한 것이기 때문인데요. 글쓰기를 예로 들면 이렇습니다. 

우선 글감 내지 주제를 정해야 합니다. 이후 실제 글을 작성할 때는 어떤 단어를 써야할지 결정합니다. LDA도 마찬가지입니다. 우선 말뭉치로부터 얻은 토픽 분포로부터 토픽을 뽑습니다. 이후 해당 토픽에 해당하는 단어들을 뽑습니다.

이제 반대 방향으로 생각해보겠습니다. 현재 문서에 등장한 단어들은 어떤 토픽에서 뽑힌 단어들일까요? 이건 명시적으로 알기는 어렵습니다. 말뭉치에 등장하는 단어들 각각에 꼬리표가 달려있는 건 아니니까요. 

그런데 LDA는 이렇게 말뭉치 이면에 존재하는 정보를 추론해낼 수 있습니다. LDA에 **잠재(Latent)**라는 이름이 붙은 이유입니다. LDA의 학습은 바로 이러한 잠재정보를 알아내는 과정입니다.



## 모델 아키텍처

LDA의 아키텍처, 즉 LDA가 가정하는 문서생성과정은 다음과 같습니다. $D$는 말뭉치 전체 문서 개수, $K$는 전체 토픽 수(하이퍼 파라메터), $N$은 $d$번째 문서의 단어 수를 의미합니다. 네모칸은 해당 횟수만큼 반복하라는 의미이며 동그라미는 변수를 가리킵니다. 화살표가 시작되는 변수는 조건, 화살표가 향하는 변수는 결과입니다.

우리가 관찰 가능한 변수는 $d$번째 문서에 등장한 $n$번째 단어 $w_{d,n}$이 유일합니다(음영 표시). 우리는 이 정보만을 가지고 하이퍼파라메터(사용자 지정) $α,β$를 제외한 모든 잠재 변수를 추정해야 합니다. 앞으로 이 글에서는 이 그림을 기준으로 설명할 예정이기 때문에 잘 기억해두시면 좋을 것 같습니다.

<a href="http://imgur.com/CEGcfoM"><img src="http://i.imgur.com/CEGcfoM.png" title="source: imgur.com" /></a>



## LDA 모델의 변수

우선 각 변수를 설명하겠습니다. $Φ_k$는 $k$번째 토픽에 해당하는 벡터입니다. 말뭉치 전체의 단어 개수만큼의 길이를 가졌습니다. 예컨대 $Φ_1$은 아래 표에서 첫번째 열입니다. 마찬가지로 $Φ_2$는 두번째, $Φ_3$은 세번째 열벡터입니다. $Φ_k$의 각 요소값은 해당 단어가 $k$번째 토픽에서 차지하는 비중을 나타냅니다. $Φ_k$는 확률이므로 모든 요소의 합은 1이 됩니다(아래 표 기준으로는 열의 합이 1).

|   Terms    | Topic 1 | Topic 2 | Topic 3 |
| :--------: | :-----: | :-----: | :-----: |
|  Baseball  |  0.000  |  0.000  |  0.200  |
| Basketball |  0.000  |  0.000  |  0.267  |
|   Boxing   |  0.000  |  0.000  |  0.133  |
|   Money    |  0.231  |  0.313  |  0.400  |
|  Interest  |  0.000  |  0.312  |  0.000  |
|    Rate    |  0.000  |  0.312  |  0.000  |
|  Democrat  |  0.269  |  0.000  |  0.000  |
| Republican |  0.115  |  0.000  |  0.000  |
|   Cocus    |  0.192  |  0.000  |  0.000  |
| President  |  0.192  |  0.063  |  0.000  |

그런데 아키텍처를 자세히 보면 $Φ_k$는 하이퍼파라메터 $β$에 영향을 받는 걸 알 수 있습니다. 이는 LDA가 토픽의 단어비중 $Φ_k$이 디리클레분포를 따른다는 가정을 취하기 때문입니다. LDA 기법에 디리클레라는 이름이 붙은 이유이기도 합니다. 디리클레분포 관련 자세한 내용은 [이곳](https://ratsgo.github.io/statistics/2017/05/28/binomial/)을 참고하시면 좋을 것 같습니다.

$θ_d$는 $d$번째 문서가 가진 토픽 비중을 나타내는 벡터입니다. 전체 토픽 개수 $K$만큼의 길이를 가집니다. 예컨대 $θ_1$은 아래 표에서 첫번째 행벡터, $θ_5$는 다섯번째 행벡터가 됩니다. $θ_d$의 각 요소값은 $k$번째 토픽이 해당 $d$번째 문서에서 차지하는 비중을 나타냅니다. $θ_d$는 확률이므로 모든 요소의 합은 1이 됩니다(아래 표 기준으로는 행의 합이 1).

| Docs  | Topic 1 | Topic 2 | Topic 3 |
| :---: | :-----: | :-----: | :-----: |
| Doc 1 |  0.400  |  0.000  |  0.600  |
| Doc 2 |  0.000  |  0.600  |  0.400  |
| Doc 3 |  0.375  |  0.625  |  0.000  |
| Doc 4 |  0.000  |  0.375  |  0.625  |
| Doc 5 |  0.500  |  0.000  |  0.500  |
| Doc 6 |  0.500  |  0.500  |  0.000  |

$θ_d$ 역시 하이퍼파라메터 $α$에 영향을 받습니다. 이는 LDA가 문서의 토픽비중 $θ_d$이 디리클레분포를 따른다는 가정을 취하기 때문입니다. 

이번엔 $z_{d,n}$에 대해 살펴보겠습니다. $z_{d,n}$은 $d$번째 문서 $n$번째 단어가 어떤 토픽에 해당하는지 할당해주는 역할을 합니다. 예컨대 세번째 문서의 첫번째 단어는 'Topic2'일 가능성이 높겠네요. Topic1과 2가 뽑힐 확률이 각각 0.375, 0.625이거든요. 

$w_{d,n}$은 문서에 등장하는 단어를 할당해주는 역할을 합니다. $Φ_k$와 $z_{d,n}$에 동시에 영향을 받습니다. 의미는 이렇습니다. 직전 예시에서 $z_{3,1}$은 실제로 Topic2에 할당됐다고 칩시다. 이제 $Φ_2$를 봅시다. 그러면 $w_{3,1}$은 Money가 될 가능성이 높겠네요. Topic2의 단어 분포 가운데 Money가 0.313으로 가장 높거든요.



## LDA의 inference

지금까지 LDA가 가정하는 문서생성과정과 잠재변수들이 어떤 역할을 하는지 설명했습니다. 이제는 $w_{d,n}$를 가지고 잠재변수를 역으로 추정하는 **inference** 과정을 살펴보겠습니다. 다시 말해 LDA는 토픽의 단어분포와 문서의 토픽분포의 결합으로 문서 내 단어들이 생성된다고 가정합니다. LDA의 inference는 실제 관찰가능한 문서 내 단어를 가지고 우리가 알고 싶은 토픽의 단어분포, 문서의 토픽분포를 추정하는 과정입니다.

여기에서 LDA가 가정하는 문서생성과정이 합리적이라면 해당 확률과정이 우리가 갖고 있는 말뭉치를 제대로 설명할 수 있을 것입니다. 바꿔 말해 토픽의 단어분포와 문서의 토픽분포의 결합확률이 커지도록 해야 한다는 이야기입니다. 확률과정과 결합확률을 각각 그림과 수식으로 나타내면 다음과 같습니다. ($z_{d,n}$:per-word topic assignment, $θ_d$:per-document topic proportions, $Φ_k$:per-corpus topic distributions)

<a href="http://imgur.com/ArQyvuO"><img src="http://i.imgur.com/ArQyvuO.png" title="source: imgur.com" /></a>


$$
\begin{align*}
p(&{ \phi  }_{ 1:K },{ \theta  }_{ 1:D },{ z }_{ 1:D },{ w }_{ 1:D })=\\ &\prod _{ i=1 }^{ K }{ p({ \phi  }_{ i }|\beta ) } \prod _{ d=1 }^{ D }{ p({ \theta  }_{ d }|\alpha ) } \left\{ \prod _{ n=1 }^{ N }{ p({ z }_{ d,n }|{ \theta  }_{ d })p(w_{ d,n }|{ \phi  }_{ 1:K },{ z }_{ d,n }) }  \right\}
\end{align*}
$$
위 수식에서 사용자가 지정한 하이퍼파라메터 $α,β$와 우리가 말뭉치로부터 관찰가능한 $w_{d,n}$을 제외한 모든 변수가 미지수가 됩니다. 따라서 우리는 $p(z,Φ,θ$\|$w)$를 최대로 만드는 $z,Φ,θ$를 찾아야 합니다. 이것이 LDA의 inference입니다. 그런데 분모에 해당하는 $p(w)$를 단번에 구할 수 없기 때문에 **깁스 샘플링** 같은 기법을 사용하게 됩니다. 깁스 샘플링 관련 자세한 내용은 [이곳](https://ratsgo.github.io/statistics/2017/05/31/gibbs/)을 참고하시면 좋을 것 같습니다.



## LDA와 깁스 샘플링

LDA에서는 나머지 변수는 고정시킨 채 한 변수만을 변화시키되, 불필요한 일부 변수를 샘플링에서 제외하는 **collapsed gibbs sampling** 기법을 씁니다. LDA에서는 $Φ,θ$를 계산에서 생략합니다. 어쨌든 LDA의 깁스 샘플링 과정을 나타낸 수식은 다음과 같습니다.
$$
p({ z }_{ i }=j|{ z }_{ -i },w)
$$
위 식의 의미는 이렇습니다. 말뭉치가 주어졌기 때문에 $w$는 우리가 이미 알고 있는 값입니다. $z$는 각 단어가 어떤 토픽에 할당돼 있는지를 나타내는 변수인데요. $z_{-i}$는 $i$번째 단어의 토픽 정보를 제외한 모든 단어의 토픽 정보를 가리킵니다. 식 전체적으로는 $w$와 $z_{-i}$가 주어졌을 때 문서의 $i$번째 단어의 토픽이 $j$일 확률을 뜻합니다. 아래 그림을 볼까요?

<a href="http://imgur.com/Olo5Tta"><img src="http://i.imgur.com/Olo5Tta.png" width="400px" title="source: imgur.com" /></a>

위 그림에서 $z_i$는 record라는 단어가 속하는 토픽입니다. 깁스 샘플링을 위해 토픽 정보를 지워 놓았습니다. 나머지 단어에 대한 토픽 정보는 그대로 씁니다. 이것이 바로 $z_{-i}$입니다. 이 상태의 정보를 토대로 record라는 단어가 어떤 토픽에 속할지 할당하는 것이 LDA의 깁스 샘플링 과정입니다. 이해를 돕기 위해 그림 하나 더 보겠습니다.

<a href="http://imgur.com/T9DG4PH"><img src="http://i.imgur.com/T9DG4PH.gif" title="source: imgur.com" /></a>

위 그림은 이렇게 이해하면 됩니다. 각 행은 문서를 나타냅니다. 동그라미는 각 문서에 속한 단어들입니다. 동그라미 안의 숫자들은 토픽 ID입니다. 처음엔 랜덤하게 뿌려 놓습니다.

첫번째 깁스 샘플링 대상인 첫번째 문서의 첫번째 단어 $z_{0,0}$의 토픽 정보를 지웁니다. 나머지 단어들의 토픽정보를 토대로 가장 그럴싸한 토픽 ID를 새로 뽑았습니다. 예시 그림에선 3이네요.

이번엔 $z_{0,1}$ 차례입니다. 첫번째 문서의 두번째 단어 $z_{0,1}$의 토픽 정보를 지웁니다.  새로 뽑은 $z_{0,0}$을 포함한 나머지 단어들의 토픽정보를 토대로 가장 그럴싸한 토픽 ID를 또 새로 뽑습니다. 1입니다. 

이런 식으로 문서 내 모든 단어와 말뭉치 내 모든 문서에 대해 깁스 샘플링을 반복하면 어느 순간부터는 모든 단어에 대한 토픽 할당 정보가 수렴하게 됩니다. 



## 실제 계산과정

몇 가지 수식 정리 과정을 거치면 $z_i$가 $j$번째 토픽에 할당될 확률은 다음과 같이 쓸 수 있습니다.


$$
p({ z }_{ i }=j|{ z }_{ -i },w)=\frac { { n }_{ d,k }+{ \alpha  }_{ k } }{ \sum _{ i=1 }^{ K }{ ({ n }_{ d,i }+{ \alpha  }_{ i }) }  } \times \frac { { v }_{ k,{ w }_{ d,n } }+{ \beta  }_{ { w }_{ d,n } } }{ \sum _{ i=1 }^{ V }{ { v }_{ k,j }+{ \beta  }_{ j } }  } 
$$
