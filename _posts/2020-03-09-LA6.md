---
layout: post
title: 고유값(Eigenvalue)과 고유벡터(Eigenvector)
category: Linear Algebra
tag: Linear-Algebra
---





## 1) Same Important Properties of det(A)

- 아래와 같이 타원처럼 생긴 벡터공간이 있고, 각각의 점들은 행렬 $\mathbf{A}$ 의 열벡터라고 하자. 이 벡터공간 내 임의의 벡터 $\vec{a}$ 에 대하여 $\vert\vert k\vec{a} - \mathbf{x}\vert\vert$ 를 최소로 하는 벡터의 방향은 장축(검은색 선분)방향일 것이다. 이 때의 벡터 $\mathbf{x}$ 를 $\mathbf{A}$ 의 **고유벡터(Eigenvector)** 라 하며, 이 $\mathbf{x}$ 에 대하여 $\mathbf{A}\mathbf{x} = \lambda \mathbf{x}$ 를 만족시키는 $\lambda$ 를 행렬 $\mathbf{A}$ 의 **고유값(Eigenvalue)** 라고 한다.

<p align="center"><img src="https://i.stack.imgur.com/FdfAd.jpg" alt="Eigenvector" style="zoom: 45%;" /></p>

- $(\mathbf{A} - \lambda\mathbf{I})\mathbf{x} = 0$ 이고, $\mathbf{x} \neq \vec{0}$ 이므로 $\det (\mathbf{A} - \lambda\mathbf{I}) = 0$ . 즉, Singular이다. $\mathbf{x}$ 를 구하는 문제는 행렬 $\mathbf{A}$ 의 Null Space를 구하는 문제와도 같다. 

<br/>

## 2) Cosines and Projection onto Line

- $\mathbf{v}$ 를 $\mathbf{u}$ 로 사영(Projection) 하는 과정 

![](http://www.maths.usyd.edu.au/u/MOW/vectors/images/v105x.gif)

- 사영을 벡터로 나타내면 $\mathbf{u}^T\mathbf{v}$ 이고, 그 값을 유도되는 식은 다음과 같다. 처음 식은 제 2 코사인 법칙으로부터 시작한다.

$$
\vert\vert\mathbf{u} - \mathbf{v}\vert\vert^2 = \vert\vert\mathbf{u}\vert\vert^2 + \vert\vert\mathbf{v}\vert\vert^2 - 2\vert\vert\mathbf{u}\vert\vert \cdot \vert\vert\mathbf{v}\vert\vert \cos \theta \\
(\mathbf{u} - \mathbf{v})^T(\mathbf{u} - \mathbf{v}) = \mathbf{u}^T\mathbf{u} + \mathbf{v}^T\mathbf{v} - 2\vert\vert\mathbf{u}\vert\vert \cdot \vert\vert\mathbf{v}\vert\vert \cos \theta \\
\mathbf{u}^T\mathbf{v} + \mathbf{v}^T\mathbf{u} = 2\vert\vert\mathbf{u}\vert\vert \cdot \vert\vert\mathbf{v}\vert\vert \cos \theta \\
\therefore \mathbf{u}^T\mathbf{v} = \vert\vert\mathbf{u}\vert\vert \cdot \vert\vert\mathbf{v}\vert\vert \cos \theta
$$



- $(\mathbf{u} - \hat{\mathbf{x}} \cdot \mathbf{v}) \perp \mathbf{v}$ 일 때, 

$$
\mathbf{u}^T(\mathbf{v} - \hat{\mathbf{x}} \cdot \mathbf{u}) = 0 \\
\mathbf{u}^T\mathbf{v} - \hat{\mathbf{x}} \cdot \vert\vert\mathbf{u}\vert\vert^2 = 0 \\
\hat{\mathbf{x}} = \frac{\mathbf{u}^T \mathbf{v}}{\mathbf{u}^T\mathbf{u}} \\
\mathbf{p} = \hat{\mathbf{x}} \cdot \mathbf{u}
$$

 <br/>

## 3) Projection and Least Squares

- Projection Matrix, $\mathbf{P}$ : 위의 식을 특정 벡터( $\mathbf{u}, \mathbf{v}$ )가 아닌 행렬 $\mathbf{A}$ 에 대하여 나타내면, $\mathbf{b} - \mathbf{A}\hat{\mathbf{x}} \perp a_i$ 일 때

$$
\mathbf{A}^T(\mathbf{b} - \mathbf{A}\hat{\mathbf{x}}) = 0 \\
\mathbf{A}^T\mathbf{A}\hat{\mathbf{x}} = \mathbf{A}^T\mathbf{b} \\
\hat{\mathbf{x}} = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}
$$

이며, Projection Matrix $\mathbf{P}\mathbf{b} = \mathbf{A}\hat{\mathbf{x}} = \mathbf{A}(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}$ 를 만족하므로, $\mathbf{P} = \mathbf{A}(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T$ 이다.

- **Gram-Schmidt orthogonalization** 





- Generalized Least Square
  - 



 <br/>

## 4) Orthogonal Basis

- dd

<br/>

