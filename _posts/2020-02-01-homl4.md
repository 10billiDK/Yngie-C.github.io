---
layout: post
title: 4. Training Models
category: Hands on Machine Learning
tag: Machine-Learning
---

 

## 1) 선형회귀 

**선형 회귀모델** : 일반적인 선형 회귀 모델은 다음과 같이 나타난다.
$$
\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n
$$

- $\hat{y}$ : 예측값, $\quad n$ : 특성의 수,$\quad x_i$ : $i$ 번째 특성값
- $w_j$ : j번째 모델 파라미터 ($w_0$ : 편향,    $w_1, w_2, ... , w_n$ : 특성의 가중치)

위의 식을 벡터 형태로 나타내면
$$
\hat{y} = h_w(\mathbf{x}) =  w^T \cdot{\mathbf{x}}
$$

- $h_w$ : 가설 함수
- $w^T$ : 모델 파라미터 벡터
- $\mathbf{x}$ : 샘플의 특성 벡터

[2장]([https://yngie-c.github.io/hands%20on%20machine%20learning/2020/01/29/homl2/](https://yngie-c.github.io/hands on machine learning/2020/01/29/homl2/)) 에서 회귀모델에 가장 널리 사용되는 지표를 평균 제곱근 오차(RMSE)라고 하였음. 실제로는 RMSE와 같은 결과를 내면서 더 간단한 평균 제곱 오차(Mean Square Error, MSE)를 사용한다.
$$
MSE(\mathbf{X}, h_w) = \frac{1}{m}\sum_{i=1}^m( w^T\cdot\mathbf{x}^{(i)} - y^{(i)})^2
$$


**정규 방정식** : 비용 함수를 최소화 하는 해석적인 방법 (자세한 설명은 [이곳]([https://ko.wikipedia.org/wiki/%EC%B5%9C%EC%86%8C%EC%A0%9C%EA%B3%B1%EB%B2%95#%EC%84%A0%ED%98%95_%EC%B5%9C%EC%86%8C%EC%A0%9C%EA%B3%B1%EB%B2%95](https://ko.wikipedia.org/wiki/최소제곱법#선형_최소제곱법)) 을 참조하자)
$$
\hat{w} = (\mathbf{X}^T \cdot \mathbf{X})^{-1} \cdot \mathbf{X}^T \cdot \mathbf{y}
$$


- $\hat{w}$ 는 비용 함수를 최소화하는 $w$ 의 값
- $\mathbf{y}$ 는 $y_1$ 부터 $y_m$ 까지 포함하는 타깃 벡터

정규 방정식을 이용해 $\hat{w}$ 를 구해보자

```python
import numpy as np
X_b = np.c_[np.ones((100, 1)), X]
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
```

정규 방정식의 계산 복잡도 : $(\mathbf{X}^T \cdot \mathbf{X})^{-1}$ 을 구하기 위한 계산 복잡도(Computational Complexity)는 샘플의 수 $m$ 에 대해서는 $O(m)$ , 특성 수 $n$ 에 대해서는 일반적으로 $O(n^{2.4})$ 에서 $O(n^{3})$ 사이를 나타낸다. 즉, 특성 수가 늘어날수록 정규 방정식을 이용한 방법은 계산 시간이 훨씬 더 오래 걸리게 된다.

<br/>

## 2) 경사 하강법

**경사 하강법** (Gradient Descent, GD) : 매우 일반적인 최적화 알고리즘. 비용 함수를 최소화하는 방향으로 반복해서 파라미터를 조정해 나간다. 구체적으로 보면 $w$ 를 임의의 값으로 시작하여 최솟값에 수렴할 때까지 점진적으로 향상시켜나감. 중요한 파라미터는 스텝의 크기. 이는 **학습률** (Learning rate) 하이퍼 파라미터로 결정한다. 학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로 시간이 오래 걸리게 되고, 학습률이 너무 크면 적절한 수렴점을 찾지 못할 수도 있다. 

무작위로 결정되는 시작점이 유발하는 경사 하강법의 2가지 문제점

- 지역 최솟값(local minimum)으로의 수렴 : 전역 최솟값(global minimum)이 아닌 지역 최솟값에 수렴할 수도 있다. 
- 평탄한 구간에서의 시간 지연 : 평탄한 구간을 지나게 되는 경우 시간이 오래 걸리고 모델이 일찍 멈추게 된다.

다행히 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택하여 선을 긋더라도 곡선을 가르지 않는 볼록 함수(Convex Function). 오직 하나의 전역 최솟값만을 가진다. 또한 연속된 함수이고 기울기가 갑자기 변하지 않음. [^1] 

특성의 스케일이 다른 경우에도 수렴하는데 시간이 오래 걸릴 수 있으므로 특성 스케일링을 통해 같은 스케일을 갖도록 만들어야 한다.

**배치 경사 하강법** : 경사 하강법을 구현하려면 모델의 각 파라미터 $w_j$ 에 대해서 비용 함수의 그래디언트를 계산. 이를 위해서는 **편도함수** (Partial Derivative)를 구해야 한다. 파라미터 $w_j$ 에 대한 비용 함수의 편도함수는 다음과 같다.
$$
\frac{\partial}{\partial w_j}MSE(w) = \frac{2}{m}\sum_{i=1}^{m}(w^T \cdot \mathbf{x}^{(i)} - y^{(i)})x^{(i)}_j
$$

편도함수는 아래의 식을 사용해 한꺼번에 계산할 수도 있다. [^2]
$$
\nabla_w MSE(w) = \left[\begin{array}{ccc} \frac{\partial}{\partial w_0}MSE(w) \\ \frac{\partial}{\partial w_1}MSE(w) \\ ... \\ \frac{\partial}{\partial w_n}MSE(w) \end{array}\right] = \frac{2}{m}\mathbf{X}^T \cdot (\mathbf{X} \cdot w - \mathbf{y})
$$

그래디언트 벡터가 구해지면 학습률($\eta$) 을 곱하여 기존의 $w$ 에서 빼주어 새로운 $w$ 를 구하는 방법을 계속 반복한다.
$$
w^{d+1} = w^d - \eta \nabla_w MSE(w)
$$



여기서 그래디언트 벡터에 곱해줄 적절한 학습률을 찾기 위해 그리드 탐색을 사용한다. 그리드 탐색에서 수렴하는데 너무 오래 걸리는 모델을 막기 위해 반복 횟수를 제한해야 한다. 반복 횟수가 너무 작으면  최적점에 도달하기 전에 알고리즘이 멈추고, 너무 크면 모델 파라미터가 변하지 않는 동안 시간을 낭비하게 된다.  그래서 일반적으로 반복 횟수는 크게 하되 그래디언트 벡터가 특정 값($\epsilon$)[^3]보다 작아지면 알고리즘을 중지하는 방법을 사용한다.

**확률적 경사 하강법** (Stochastic Gradient Descent) : 배치 경사 하강법은 매 스텝마다 전체 훈련 세트를 이용하여 그래디언트를 계산하므로 훈련 세트가 커지면 시간이 오래 걸린다는 단점이 있다. 반대로 확률적 경사 하강법은 매 스텝에서 딱 한 개의 샘플을 무작위로 선택하고 그 하나의 샘플에 대한 그래디언트를 계산한다. 반복마다 사용되는 데이터의 양이 적기 때문에 알고리즘이 훨씬 빠르다.

대신 배치 경사 하강법에 비해 훨씬 불안정하며, 알고리즘이 최솟값과 유사한 값에 도달하긴 하지만 최솟값에 도달하지 못한다. 이런 문제를 해결하기 위해 시작할 때는 학습률을 크게 하고 점차 작게 줄여서 전역 최솟값에 도달하게 하는 학습 스케줄(Learning Schedule) 함수를 사용한다. 일반적으로 한 반복에서 샘플의 개수($m$)만큼 되풀이 되고, 이때의 각 반복을 **에포크** (epoch)라고 한다. 사이킷런에서는 확률적 경사하강법을 위한 SGDRegressor() 클래스를 제공한다.

```python
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=50, penalty=None, eta0=0.1)
sgd_reg.fit(X, y.ravel())
```

**미니배치 경사 하강법** (Mini-batch Gradient Descent) : 각 스텝에서 전체 훈련 세트나 하나의 샘플을 기반으로 그래디언트를 계산하는 것이 아니라 미니배치라고 부르는 임의의 작은 샘플 세트에 대해 그래디언트를 계산하는 방법.

| 알고리즘             | 샘플 수(m)<br>이 클 때 | 특성 수(n)<br>이 클 때 | 외부 메모리<br>학습 지원 | 스케일<br>조정 필요 | 하이퍼<br>파라미터 수 | 사이킷런<br>클래스 |
| -------------------- | ---------------------- | ---------------------- | ------------------------ | ------------------- | --------------------- | ------------------ |
| 정규방정식           | 빠름                   | 느림                   | X                        | X                   | 0                     | LinearRegression   |
| 배치 경사 하강법     | 느림                   | 빠름                   | X                        | O                   | 2                     | N/A                |
| 확률적 경사 하강법   | 빠름                   | 빠름                   | O                        | O                   | >=2                   | SGDRegressor       |
| 미니배치 경사 하강법 | 빠름                   | 빠름                   | O                        | O                   | >=2                   | N/A                |

<br/>

## 3) 다항 회귀

직선보다 복잡한 데이터셋의 경우. 비선형 데이터를 학습하는 데 선형 모델을 사용할 수 있음. 각 특성의 거듭제곱을 새로운 특성으로 추가하고, 이 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련시키기 때문. 이것을 **다항 회귀** (Polynomial Regression)라고 함. 사이킷런에서는 거듭제곱 특성 생성을 위해 PolynomialFeatures() 라는 클래스를 제공한다.[^4]

```python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = ploy_features.fit_transform(X)
# degree로 모델의 차수를 정하고 fit_transform()을 이용하여 특성을 생성해준다.
```

<br/>

## 4) 학습 곡선

degree를 많이 높여 고차 다항 회귀를 적용하면 선형 회귀에서보다 훨씬 더 훈련 데이터에 잘 맞추려 한다. 이런 현상은 과적합을 유발한다. 모델이 과적합인지 과소적합인지 알 수 있는 첫 번째 방법은 교차 검증이다. 훈련 데이터에서 모델의 성능이 좋지만 교차 검증 점수가 낮다면 그 모델은 과적합, 둘 모두의 점수가 낮다면 그 모델은 과소적합 되었다고 말합니다.

다음 방법은 **학습 곡선** 을 이용하는 것. 이 곡선은 훈련 세트와 검증 세트의 모델 성능을 훈련 세트 크기의 함수로 나타낸다. 곡선을 그리기 위해서는 다른 크기의 훈련 세트의 서브셋을 만들어 여러 번 훈련시키면 된다. 곡선을 정의하는 함수는 다음과 같다.

```python
from sklearn.metrics import mean_squared_error
from sklearn_model_selection import train_test_split
import matplotlib.pyplot as plt

def plot_learning_curves(model, X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    train_errors, val_errors = [], []
    
    for m in range(1, len(X_train)):
        model.fit(X_train[:m], y_train[:m])
        y_train_predict = model.predict(X_train[:m])
        y_val_predict = model.predict(X_val)
        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))
        val_errors.append(mean_squared_error(y_val, y_val_predict))
    
    plt.plot(np.sqrt(train_errors), label="train_set")
    plt.plot(np.sqrt(val_errors), label="val_set")
```

- 과소적합 학습 모델의 학습 곡선 그래프(아래 그래프는 단순 선형 회귀 모델)



<p align="center">
  <img src="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_04in04.png" alt="Underfitting" style="zoom: 33%;" />  
</p>

먼저 Training set의 그래프(빨간색)를 보면, 훈련 세트에 샘플이 한 개 혹은 두 개일 때는 모델이 완벽하게 작동한다. 그러나 샘플이 점점 추가되면서 모델이 훈련 데이터를 완벽히 학습하는 것이 불가능해진다. 그래서 오차가 일정 수준까지 계속 상승하게 된다. Validation set의 그래프(파란색)를 보면, 샘플이 적을 때는 제대로 일반화가 되지 않아 오차가 크지만, 샘플이 추가될수록 오차 감소가 완만해져서 훈련 세트와 그래프가 가까워진다. 

- 과대적합 학습 모델의 학습 곡선 그래프(아래 그래프는 10차 다항 회귀 모델)

<p align="center">
    <img src="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_04in05.png" alt="Overfitting" style="zoom:33%;" />
</p>

먼저 Training set의 그래프(빨간색)를 보면, 10차원 모델이므로 샘플 수가 10개 일 때까지는 오차가 없다가 이후부터 오차가 점점 증가하기 시작한다. Validation set의 그래프(파란색)를 보면, 처음에는 모든 데이터에 맞춰진 과적합때문에 오차가 매우 크지만 샘플 수가 많아질수록 오차가 낮아지게 된다.[^5]

- 특징
  - 샘플이 많을 때, 오차의 수준은 과소적합 모델(약 1.8)보다 과대적합 모델(약 0.8~1.1)이 더 낮다.
  - 두 곡선 사이에는 존재하는 공간(즉, 훈련 세트와 검증 세트의 오차 차이)은 과대적합 모델이 더 크다. 이는 훈련 세트의 오차가 검증 데이터보다 훨씬 더 낮다는 뜻인데, 이는 과대적합 모델의 특징이다.

※ **편향 / 분산 트레이드 오프** : 통상적으로 복잡도가 증가할수록, 모델의 분산이 늘어나고 편향은 줄어드는 현상.

- 편향(Bias) : 잘못된 가정으로 인한 오차. 편향이 큰 모델은 훈련 데이터에 과소적합 되기 쉽다.
- 분산(Variance) : 훈련 데이터에 있는 작은 변동에 모델이 과도하게 민감할 때 나타나는 오차. 분산이 큰 모델은 훈련 데이터에 과소적합 되기 쉽다.
- 줄일 수 없는 오차 : 데이터 자체의 노이즈로 인한 오차. 이 오차는 데이터에서 노이즈를 제거해야만 없앨 수 있다.

<br/>

## 5) 규제가 있는 선형 모델

과대적합을 감소시키기 위한 규제 방법이 존재한다. 선형 회귀 모델에서는 보통 모델의 가중치를 제한함으로써 규제를 가한다.

**릿지 회귀** (Ridge) : 티호노프(Tikhonov) 규제라고도 부른다. 규제항 $\alpha \sum_{i=1}^n w_1^2$ 이 비용함수에 추가된다. 학습 알고리즘을 데이터에 맞추는 것뿐만 아니라 모델의 가중치가 가능한 한 작게 유지되도록 한다. 규제항은 훈련하는 동안에만 비용함수에 추가되며, 훈련이 끝나면 모델의 성능을 규제가 없는 성능 지표로 평가한다.

$\alpha = 0$ 이면 선형 회귀와 같고, $\alpha$ 가 커질수록 모든 가중치가 0에 가까워지면서 데이터의 평균을 지나는 수평선이 된다. 릿지 회귀의 비용 함수 $f(w)$ 는 아래와 같다.
$$
f(w) = MSE(w) + \alpha \frac{1}{2} \sum_{i=1}^n w_i^2
$$
식에서 볼 수 있듯, $w_0$ 은 규제되지 않으며, $\mathbf{w}^\prime$ 를 $w_1, w_2, ... , w_n$ 라고 정의하면 규제항은 $\frac{1}{2}(\Vert\mathbf{w}^\prime\Vert_2)^2$ 와 같다. 여기서 $\Vert \cdot \Vert_2$ 가 가중체 벡터의 $l_2$ 노름이다. 경사 하강법에 적용하려면 MSE 그래디언트 벡터 $(\nabla_w MSE(w))$ 에 $\alpha \mathbf{w}^\prime$ 을 더하면 된다.

리지 회귀는 정규 방정식과 경사 하강법에 모두 사용 가능하며 릿지 회귀의 정규방정식은 $\hat{w} = (\mathbf{X}^T \cdot \mathbf{X} + \alpha \mathbf{A})^{-1} \cdot \mathbf{X}^T \cdot \mathbf{y}$  이다. 두 방법의 장단점은 선형 회귀와 같다. 사이킷런에서는 릿지 회귀가 적용된 각각의 방법을 다음의 코드로 작동시킬 수 있다.

```python
# 정규 방정식
from sklearn.linear_model import Ridge
ridge_reg = Ridge(alpha=1, solver="cholesky")
ridge_reg.fit(X, y)

# 확률적 경사 하강법
sgd_reg = SGDRegressor(max_iter=5, penalty="12")
sgd_reg.fit(X, y.ravel())
# penalty는 사용할 규제를 지정한다. "12"는 SGD의 비용 함수에 L2 노름의 제곱을 2로 나눈 규제항을 추가하게 한다. (즉, 릿지 회귀와 같다.)
```

**라쏘 회귀** (Lasso) : 라쏘 회귀는 비용 함수에 규제항을 더하지만 $l_2$ 노름의 제곱을 2로 나눈 것 대신 가중치 벡터의 $l_1$ 노름을 사용한다. 라쏘 회귀의 비용 함수 $J(w)$ 는 다음과 같다.
$$
J(w) = MSE(w) + \alpha \sum_{i=1}^n|w_i|
$$
라쏘 회귀의 특징은 덜 중요한 특성의 가중치를 완전히 제거하려고 한다는 점이다. 다시 말해 라쏘 회귀는 자동으로 특성 선택을 하고 희소 모델(Sparse model)[^6]을 만든다. 라쏘의 비용 함수는 $w_i = 0$ 에서 미분 가능하지 않다. 하지만 $w_i = 0$ 에서 서브 그래디언트 벡터(Subgradient vector) $\mathbf{g}$ 를 사용하면 경사 하강법을 적용하는 데 문제가 없다.
$$
g(w,J) = \nabla_w MSE(w) + \left[\begin{array}{ccc} \text{sign}(w_1) \\ \text{sign}(w_2) \\ ... \\ \text{sign}(w_n) \end{array}\right] \quad 여기서 \quad \text{sign}(w_i) = \begin{cases} -1 \quad w_i < 0 \\ 0 \quad w_i = 0 \\ +1 \quad w_i > 0 \end{cases}
$$
사이킷런에서는 다음과 같은 코드로 라쏘 모델을 작동할 수 있다.

```python
# 정규 방정식
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha=0.1)
lasso_reg.fit(X, y)

# 확률적 경사 하강법
sgd_reg = SGDRegressor(max_iter=5, penalty="11")
sgd_reg.fit(X, y.ravel())
```

**엘라스틱 넷** (Elastic Net) : 엘라스틱 넷은 위의 두 모델(릿지 회귀, 라쏘 회귀)을 절충한 모델이다. 혼합 비율은 파라미터 $r$ 을 사용하여 조절한다. $r=0$ 이면 릿지 회귀와 같고, $r=1$ 이면 라쏘 회귀와 같아진다. 엘라스틱 넷의 비용 함수는 다음과 같다.
$$
J(w) = MSE(w) + r \alpha \sum_{i=1}^n|w_i| + \frac{1-r}{2} \alpha \sum_{i=1}^n w_i^2
$$
다음은 사이킷런에서 엘라스틱 넷을 작동시키는 방법이다. (r은 l1_ratio 값으로 조정할 수 있다.)

```python
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)
elastic_net.fit(X, y)
```

어떤 규제 방법을 선택해야 할까? 일반적으로는 릿지를 기본으로 하지만, 중요한 특성의 수가 적다고 판단되면 라쏘나 엘라스틱 넷이 더 낫다. 그 중 특성 수($n$)가 훈련 샘플 수($m$) 보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 라쏘가 문제를 일으킬 수 있으므로 엘라스틱 넷을 선택한다.

**조기 종료** (Early stopping) : 검증 에러가 최솟값에 도달하면 반복을 멈추고 훈련을 중지시키는 방법.

<p align="center">
    <img src="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_0415.png" alt="EarlyStopping" style="zoom:33%;" />
</p>

에포크 수가 적을 때는 진행됨에 따라 에러가 줄어들지만, 일정 시점 이후부터는 Validation set의 에러가 증가한다. 조기 종료는 Best model 에 해당하는(즉, Validation set의 에러가 최소인) 지점에서 즉시 훈련을 멈추는 것을 뜻한다. 조기 종료는 사이킷 런에서 다음과 같이 구현할 수 있다.

```python
from sklearn.base import clone
sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,
                      learning_rate="constant", eta0=0.0005)
minimum_val_error = float("inf")
best_epoch=None
best_model=None
for epoch in range(1000):
    sgd_reg.fit(X_train, y_train)
    y_val_predict = sgd_reg.predict(X_val)
    
    val_error = mean_squared_error(y_val, y_val_predict)
    if val_error < minimum_val_error:
        minimum_val_error = val_error
        best_epoch = epoch
        best_model = clone(sqd_reg)
```

<br/>

## 6) 로지스틱 회귀

**로지스틱 회귀** (Logistic Regression) : 로지스틱 회귀는 샘플이 특정 클래스에 속할 확률을 추정하는 데 널리 사용된다. 추정 확률이 50%가 넘으면 모델은 그 샘플이 해당 클래스에 속한다고 예측하며, 그렇지 않은 경우에는 클래스에 속하지 않는다고 평가한다.

로지스틱 회귀는 선형 회귀와 같이 입력 특성의 가중치 합을 계산한다. 그러나 바로 출력하지 않고 결괏값의 로지스틱을 출력한다.
$$
\hat{p} = h_w(\mathbf{x}) = \sigma (w^T \cdot \mathbf{x})
$$
$\sigma (\cdot)$ : 시그모이드 함수. 치역(결괏값의 범위)을 (0,1)로 만드는 S자 형태의 함수.
$$
\sigma(t) = \frac{1}{1+\text{exp}(-t)}
$$

<p align="center">
    <img src="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/assets/mlst_04in06.png" alt="sigmoid" style="zoom: 50%;" />
</p>

로지스틱 회귀 모델로 샘플 $\mathbf{x}$ 가 양성 클래스에 속할 확률 $\hat{p} = h_0(\mathbf{x})$ 를 추정하면 이에 대한 예측 $\hat{y}$ 을 쉽게 구할 수 있다.

$\hat{y} = \begin{cases} 0 \quad \hat{p} < 0.5 \\ 1 \quad \hat{p} \geq 0.5 \end{cases}$

$t < 0$ 이면 $\sigma(t) < 0.5$ 이고, $t \geq 0$ 이면 $\sigma(t) \geq 0.5$ 이다. 따라서 로지스틱 회귀 모델은 $w^T \cdot \mathbf{x}$ 가 양수일 때 1이라고 예측하고, 음수일 때는 0이라고 예측한다.

**로지스틱 회귀의 훈련** : 로지스틱 회귀 모델에서 훈련의 목적은 양성 샘플에 대해서 높은 확률을 추정하고 음성 샘플에 대해서는 낮은 확률을 추정하는 모델의 파라미터 벡터($\mathbf{w}$)를 찾는 것이다. 다음의 비용함수 $c(\mathbf{w})$ 를 통해 $\mathbf{w}$ 를 찾을 수 있다.
$$
c(w) = \begin{cases} -\text{log}(\hat{p}) \qquad y = 1 \\ -\text{log}(1 - \hat{p}) \quad y=0 \end{cases}
$$
이 비용 함수는 $t$ 가 0에 가까워지면, $-\text{log}(t)$ 가 매우 커지고, $t$ 가 1에 가까워지면 $-\text{log} (t)$ 는 0에 가까워진다. 다시 말해 전자의 경우 모델이 양성 샘플을 0에 가까운 확률로 추정하면 비용이 매우 커지게 되고, 후자의 경우 모델이 양성 샘플을 1에 가깝게 추정하면 비용이 0에 가까워지게 된다. 이를 전체 훈련 세트에 대한 비용 함수로 확장할 수 있으며, 이를 **로그 손실** (log loss)이라고 한다.
$$
J(w) = -\frac{1}{m}\sum_{i=1}^m[y^{(i)}\text{log}(\hat{p}^{(i)}) + (1 - y^{(i)})\text{log}(1 - \hat{p}^{(i)})]
$$
위 비용 함수의 최솟값을 계산하는 알려진 해는 없으나, 볼록 함수이므로 경사 하강법을 통해 전역 최솟값을 찾을 수 있다. 비용 함수의 $j$ 번째 모델 파라미터 $w_j$ 에 대해 편미분을 하면 아래의 식이 도출된다.
$$
\frac{\partial}{\partial w_j}J(w) = \frac{1}{m} \sum_{i=1}^m (\sigma(w^T \cdot \mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)}
$$
0과 1의 클래스로 분류될 확률이 모두 0.5가 되는 지점에서 로지스틱 회귀 모델의 결정 경계(Decision boundary)가 만들어진다. 다른 선형 모델처럼 로지스틱 회귀 모델도 $l_1, l_2$ 페널티를 사용하여 규제할 수 있다. 사이킷런은 기본적으로 $l_2$ 페널티를 기본으로 한다.

**소프트맥스 회귀** (Softmax Regression) : 로지스틱 회귀 모델의 이진 분류를 다중 분류로 확장한 것을 소프트맥스 회귀 또는 **다항 로지스틱 회귀** (Multinomial Logistic Regression)라고 한다. 샘플 $\mathbf{x}$ 가 주어지면 소프트맥스 회귀 모델이 각 클래스 $k$ 에 대한 점수 $s_k(\mathbf{x})$ 를 계산하고, 그 점수에 소프트맥스 함수(또는 정규화된 지수 함수(normalized exponential))를 적용하여 각 클래스의 확률을 추정한다. $s_k(\mathbf{x})$ 는 아래와 같은 식으로 계산할 수 있다.
$$
s_k(\mathbf{x}) = (w^{(k)})^T \cdot \mathbf{x}
$$
각 클래스는 자신만의 파라미터 벡터 $w^{(k)}$ 가 있으며 이 벡터들은 파라미터 행렬 $\mathbf{w}$ 에 행으로 저장된다. 샘플 $\mathbf{x}$ 에 대해 각 클래스의 점수가 계산되면 소프트맥스 함수를 통과시켜 클래스 $k$ 에 속할 확률인 $\hat{p}_k$ 를 구할 수 있다.
$$
\hat{p}_k = \sigma(\mathbf{s}(\mathbf{x}))_k = \frac{\text{exp}(s_k{(\mathbf{x}}))}{\sum_{j=1}^K \text{exp}(s_j(\mathbf{x}))}
$$

- $K$ : 클래스 수
- $\mathbf{s}(\mathbf{x})$ : 샘플 $\mathbf{x}$ 에 대한 각 클래스의 점수를 담고 있는 벡터
- $\sigma(\mathbf{s}(\mathbf{x}))_k$ : 샘플 $\mathbf{x}$ 에 대한 각 클래스의 점수가 주어졌을 때 이 샘플이 클래스 $k$ 에 속할 추정 확률

로지스틱 회귀 모델과 마찬가지로 소프트맥스 회귀 모델도 추정 확률이 가장 높은 클래스를 선택한다.[^7]
$$
\hat{y} = \text{argmax}_k \sigma (\mathbf{s}(\mathbf{x}))_k = \text{argmax}_k s_k(\mathbf{x}) = \text{argmax}_k ((w^{(k)})^T \cdot \mathbf{x})
$$

- $\text{argmax}$ 연산은 함수를 최대화하는 변수의 값을 반환합니다. 이 식에서는 추정확률 $\sigma (\mathbf{s}(\mathbf{x}))_k$ 가 최대인 $k$ 값을 반환한다.

소프트맥스 회귀 모델의 훈련 방법 : 모델이 타겟 클래스에 대해서는 높은 확률을, 다른 클래스에 대해서는 낮을 확률을 추정하게 하는 **크로스엔트로피** (Cross entropy) 비용 함수[^8]를 사용한다.
$$
J(\mathbf{w}) = \frac{1}{m}\sum_{i=1}^m \sum_{k=1}^K y^{(i)}_k\text{log}(\hat{p}_k^{(i)})
$$

- $i$ 번째 샘플에 대한 타깃 클래스가 $k$ 일 때, $y^{(i)}_k$ 가 1이고, 그 외에는 0이다.

위 함수의 $w^{(k)}$ 에 대한 그래디언트 벡터는 아래와 같다.
$$
\nabla_{w^k} J(\mathbf{w}) = \frac{1}{m}\sum_{i=1}^m (\hat{p}_k^{(i)} - y^{(i)}_k) \mathbf{x}^{(i)}
$$
이제 각 클래스에 대한 그래디언트 벡터를 계산할 수 있으므로 비용 함수를 최소화 하기 위한 파라미터 행렬인 $\mathbf{w}$ 를 찾기 위해 경사 하강법을 사용할 수 있다.

사이킷런에서 LogisticRegression() 은 클래스가 둘 이상일 때 기본적으로 일대다(OvA) 전략을 사용한다. 하지만 multi_class 매개변수를 "multimodal"로 바꾸면 소프트맥스 회귀를 사용할 수 있다. 그리고 solver 매개변수에 "lbfgs"와 같이 소프트맥스 회귀를 지원하는 알고리즘을 지정해야 한다. 이때, 기본적으로 하이퍼파라미터 C를 사용하여 조절할 수 있는 $l_2$ 규제가 적용된다. 위 내용을 코드로 구현하면 다음과 같다.

```python
softmax_reg = LogisticRegression(multi_class="multimodal", solver="lbfgs", C=10)
softmax_reg.fit(X, y)
```



<br/>

[^1]: 이 함수의 도함수가 립시츠 연속(Lipschitz continuous). 립시츠 연속이란 어떤 함수의 도함수가 일정한 범위 안에서 변할 때 이 함수를 립시츠 연속 함수라고 한다. MSE는 x가 무한대일 때 기울기가 무한대가 되므로 국부적인(locally) 립시츠 함수. 
[^2]: 이 공식은 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 훈련한다. 때문에 이 알고리즘을 배치 경사 하강법(Batch Gradient Descent)이라고 한다. 배치 경사 하강법은 모든 데이터 셋을 사용하기 때문에 훈련 세트가 커질수록 매우 느려진다. 하지만 특성의 수에는 민감하지 않으므로 특성의 수가 많을 때는 정규 방정식보다 경사 하강법을 사용한다.
[^3]: 비용 함수의 모양에 따라 달라지겠지만 특정 값($\epsilon$) 의 범위 안에서 최적의 솔루션에 도달하기 위해서는 $O(1/\epsilon)$ 의 반복이 걸릴 수 있다. 즉,  $\epsilon$ 을 줄일수록 반복 횟수는 늘어나게 된다.  

[^4]: PolynomialFeatures(degree=d)는 특성이 n개인 배열을 특성이 $\frac{(n+d)!}{d!n!}$ 개인 배열로 변환한다. 특성 수가 엄청나게 늘어날 수 있으니 주의.
[^5]: 과대적합 모델을 개선하는 한 가지 방법은 검증 오차가 훈련 오차에 근접할 때까지 더 많은 훈련 데이터를 추가하는 것.
[^6]: 희소모델은 성분 중 0이 많은(즉, non-zero 값이 희소한 모델을 가리킨다.) 자세한 설명은 [이곳](https://dp-story.tistory.com/4) 에서 더 자세히 볼 수 있다.

[^7]: 소프트맥스 회귀 모델은 한 번에 하나의 클래스만 예측한다. 그래서 상호 배타적인 클래스에서만 사용해야 하며, 하나의 사진에서 여러 사람의 얼굴을 인식하는 것과 같은 다중 출력 문제에는 사용할 수 없다.
[^8]: 크로스엔트로피에 대한 정보는 [이곳](https://m.blog.naver.com/PostView.nhn?blogId=yonggeol93&logNo=221230536533&proxyReferer=https%3A%2F%2Fwww.google.com%2F) 에서 더 자세히 볼 수 있다.