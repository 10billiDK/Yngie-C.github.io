---
layout: post
title: 8. 차원 축소
category: Hands on Machine Learning
tag: Machine-Learning
---





## 1) 차원의 저주

- **차원의 저주** : 머신러닝에서 특성이 너무 많은 경우 훈련 속도가 느려지고, 좋은 모델을 만드는 데 방해되는 현상이 일어나는데 이를 차원의 저주(turse of dimensionality)라고 한다.
- 고차원의 세계에선 우리가 생각할 수 없는 일들이 자주 일어난다. 2차원 단위면적에서 임의의 점 1개를 선택할 때 경계선과의 거리가 0.001 이내일 확률은 0.4% 이지만, 10,000차원 초입방체에서는 확률이 99.999999% 이상이 된다. 또, 단위 면적, 단위 부피에서 두 점을 선택할 경우의 평균 거리는 각각 약 0.52, 0.66이다. 하지만 10,000차원에서 두 점을 선택할 경우 평균 거리는 약 428.25다. 후자의 예시는 고차원에서는 데이터들이 희박한(Sparse) 상태에 놓여있다는 것을 보여준다. 

<br/>

## 2) 차원 축소를 위한 접근 방법

- **투영** (projection) : 대부분의 데이터는 모든 차원에 걸쳐 균일하게 퍼져있지 않다. 많은 특성은 거의 변화가 없는 반면 몇몇 특성은 서로 강하게 연관되어 있다. 결과적으로 모든 훈련 샘플은 고차원 공간 안의 저차원 부분 공간(subspace)에 놓여 있다. 하지만 아래와 같이 스위스 롤 형태로 데이터가 놓여있는 경우 투영을 통해서는 제대로 된 데이터의 흐름을 볼 수 없다.

![SwissRoll](https://www.researchgate.net/profile/John_Burgoyne2/publication/200688576/figure/fig1/AS:305995638165506@1449966453759/The-Swiss-roll-data-set-On-the-left-the-data-is-presented-in-its-original-form-On.png)

- **매니폴드** (manifold) : 이럴 때 사용하는 방법이 매니폴드 방식이다. 많은 차원축소 알고리즘은 데이터가 놓여있는 매니폴드를 모델링하는 방식으로 작동한다. 

<br/>

## 3) PCA

- **PCA** (주성분 분석, Principal Component Analysis) : PCA는 가장 인기있는 차원축소 알고리즘이다. 먼저 데이터에 가장 가까운 초평면을 이용한 뒤, 데이터를 이 평면에 투영시킨다.

  - 분산 보존 : 첫 번째 과제는 가장 높은 분산을 가지는 데이터의 축을 찾는 것이다. 다른 방향으로 투영하는 것보다 분산이 최대로 보존되는 축으로 투영하는 것이 정보 손실을 최소한으로 줄일 수 있기 때문이다. 원본 데이터 셋과 투영된 것 사이의 평균 제곱 거리를 최소화하는 축을 찾는다고 표현할 수도 있다.
  - **주성분** (Principal Component, PC) : 첫 번째 축을 찾고 난 이후에는, 이 축과 수직이며 남은 분산을 최대로 보존하는 두 번째 축을 찾는다. 이런 과정을 반복하면서 여러 개의 축을 찾아 나간다. $i$ 번째 축을 정의하는 단위 벡터를 $i$ 번째 주성분이라 한다. 훈련 세트의 주성분은 **특잇값 분해** (SVD)라는 수학적 기술을 통하여 훈련 세트 행렬( $\mathbf{X}$ )를 $\mathbf{X} = \mathbf{U} \cdot \sum \cdot \mathbf{V}^T$ 로 분해할 수 있다. 여기서 $\mathbf{V}$ 에 찾고자 하는 모든 주성분이 담겨 있다.

  <p align="center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/800px-GaussianScatterPCA.svg.png" alt="PC" style="zoom: 50%;" /></p>
- $d$ 차원으로 투영하기 : 주성분을 모두 추출했다면 $d$ 개의 주성분을 선택하여 $d$ 차원으로 축소시킬 수 있다. 초평면에 훈련 세트를 투영하기 위해서는 행렬 $\mathbf{X}$ 와 첫 $d$ 개의 주성분을 담은 (즉, $\mathbf{V}$ 의 첫 $d$ 열로 구성된) 행렬 $\mathbf{W}_d$ 를 점곱하면 된다.
  
$$
\mathbf{X}_{d-\text{proj}} = \mathbf{X} \cdot \mathbf{W}_d
$$



- 사이킷런 사용하기 : 사이킷런에서는 SVD 분해 방법을 사용하여 PCA를 구현한다.
  
```python
  from sklearn.decomposition import PCA
  
  pca = PCA(n_components = 2)
  X2D = pca.fit_transform(X)
```

  PCA 변환기를 데이터셋에 학습시키고 나면 `components_` 변수를 사용해 주성분을 확인할 수 있다. 

  - 설명된 분산의 비율 : `explained_variance_ratio_` 를 사용하면 유용한 정보를 얻을 수 있다. 이 변수는 각 주성분의 축을 따라 있는 데이터셋의 분산 비율을 나타내 준다. 
  - 적절한 차원 수 선택하기 : 축소할 차원 수를 선택할 때에는 충분한 분산(95% 등의 특정 수치)이 될 때까지 더해야 한다. (시각화를 위해 2 or 3차원으로 축소하는 경우는 예외) 아래의 코드는 차원을 축소하지 않고 PCA를 계산한 뒤 훈련 세트의 분산을 95%로 유지하는 데 필요한 최소한의 차원 수를 계산해준다.

  ```python
  pca = PCA()
  pca.fit(X_train)
  cumsum = np.cumsum(pca.explained_variance_ratio_)
  d = np.argmax(cumsum >= 0.95) + 1
  ```

  그런 다음  `n_components=d` 로 설정하여 PCA를 다시 실행하면 된다. 아니면 아래와 같은 방법도 존재한다.

  ```python
  pca = PCA(n_components=0.95)
  X_reduced = pca.fit_transform(X_train)
  ```

  

  - 압축을 위한 PCA
  - 점진적 PCA
  - 랜덤 PCA

<br/>

## 4) 커널 PCA

- **커널 PCA** (kernel PCA, kPCA) : [5장]([https://yngie-c.github.io/hands%20on%20machine%20learning/2020/02/05/homl5/](https://yngie-c.github.io/hands on machine learning/2020/02/05/homl5/)) 에서 SVM에 적용하였던 커널 트릭 기법을 PCA에 적용한 것을 커널 PCA라고 한다. 이 기법은 투영된 후에 샘플의 군집을 유지하거나 꼬인 매니폴드에 가까운 데이터셋을 펼칠 때도 유용하다. 아래의 코드는 사이킷런에서 커널 PCA를 적용할 수 있는 코드이다.

```python
from sklearn.decomposition import KernelPCA

rbf_pca = KernelPCA(n_components=2, kernel="rbf", gamma=0.04)
X_reduced = rbf_pca.fit_transform(X)
```

커널 선택과 하이퍼 파라미터 튜닝 : kPCA는 비지도 학습이기 때문에 좋은 커널과 하이퍼파라미터를 선택하기 위한 명확한 성능 측정 기준이 없다. 하지만 차원 축소는 종종 지도 학습의 전처리 단계로 활용되므로 그리드 탐색을 이용하여 주어진 문제에서 성능이 가장 좋은 하이퍼 파라미터를 선택할 수 있다. 아래 코드를 수행한 뒤 가장 좋은 결과를 도출하는 커널과 하이퍼파라미터는 `best_params` 변수에 저장된다.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

clf = Pipeline([
    ("kpca", KernelPCA(n_components=2)),
    ("log_reg", LogisticRegression())
])
param_grid = [{
    "kpca_gamma" : np.linspace(0.03, 0.05, 10),
    "kpca_kernel" : ["rbf", "sigmoid"]
}]

grid_search = GridSearchCV(clf, param_grid, cv=3)
grid_search.fit(X, y)
```



?? 

<br/>

## 5) LLE

- **지역 선형 임베딩** (Locally Linear Embedding, LLE) : 지역 선형 임베딩은 투영에 의존하지 않는 매니폴드 학습이다. 먼저 각 데이터 샘플이 가장 가까운 이웃에 얼마나 선형적으로 연관되어 있는지 측정한다. 그리고 국부적인 관계가 가장 잘 보존되는 훈련 세트의 저차원 표현을 찾는다. 이는 특히 잡음이 너무 많지 않은 경우에 꼬인 매니폴드를 펼치는 데 잘 작동한다.

  - 먼저 알고리즘이 각 훈련 샘플 $\mathbf{x}^{(i)}$ 에 대하여 가장 가까운 $k$ 개의 샘플을 찾는다. 그런 다음 이 이웃에 대한 선형 함수로 $\mathbf{x}^{(j)}$ 를 재구성한다. 더 구체적으로 말하면 $\mathbf{x}^{(j)}$ 와 $\sum^m_{j=1}w_{i,j}\mathbf{x}^{(j)}$ 사이의 제곱 거리가 최소인 $w_{i,j}$ 를 찾는 것이다. 수학적으로 표현하면 아래와 같다. 

  $$
  \hat{\mathbf{W}} = \text{argmin} \sum^m_{i=1}(\mathbf{x}^{(i)} - \sum^m_{j=1}w_{i,j}\mathbf{x}^{(j)})^2 \\
  \begin{cases} w_{i,j} = 0 \qquad \mathbf{x}^{(j)}\text{ 가 } \mathbf{x}^{(i)}\text{의 최근접 이웃 k개 중 하나가 아닐 때} \\ \sum^m_{j=1} w_{i,j} = 1 \quad \text{i = 1, 2, ..., m일 때} \end{cases}
  $$

  - 이 단계를 거치면 가중치 행렬 $\hat{\mathbf{W}}$ 은 훈련 샘플 사이에 있는 지역 선형 관계를 담고 있다. 이제 두 번째 단계는 가능한 한 이 관계가 보존되도록 훈련 샘플을 $d$ 차원 공간으로 매핑한다. 만약 $\mathbf{z}^{(i)}$ 가 $d$ 차원 공간에서 $\mathbf{x}^{(i)}$ 의 상이라면 가능한 $\mathbf{z}^{(i)}$ 와 $\sum^m_{j=1}\hat{w}_{i,j}\mathbf{z}^{(j)}$ 사이의 거리가 최소화되어야 한다. 첫 번째 단계와 비슷해 보이지만, 샘플을 고정하고 최적의 가중치를 찾는 대신 반대로 가중치를 고정하고 저차원의 공간에서 샘플 이미지의 최적 위치를 찾는다. $\mathbf{Z}$ 는 모든 $\mathbf{z}^{(i)}$ 를 포함하는 행렬이다.

  $$
  \mathbf{Z} = \text{argmin} \sum^m_{i=1}(\mathbf{z}^{(i)} - \sum^m_{j=1}w_{i,j}\mathbf{z}^{(j)})^2
  $$

  - 사이킷런이 제공하는 LLE 구현의 계산 복잡도는 $k$ 개의 가장 가까운 이웃을 찾는 데 $O(m \log(m) n \log(k))$ , 가중치 최적화에  $O(mnk^3)$ , 저차원 표현을 만드는 데 $O(dm^2)$ 이다. 마지막 항의 $m^2$ 때문에 이 알고리즘을 대량의 데이터셋에 적용하기는 어렵다.



<br/>

## 6) 다른 차원 축소 기법

사이킷런은 다양한 차원 축소 기법을 제공한다. 다음은 그 중에서 가장 널리 사용되는 방법들이다.

- 다차원 스케일링(Multidimensional Scaling, MDS)은 샘플 간의 거리를 보존하면서 차원을 축소한다.
- Isomap은 각 샘플을 가장 가까운 이웃과 연결하는 식으로 그래프를 만든다. 그런 다음 샘플 간의 지오데식 거리(geodesic distance)를 유지하면서 차원을 축소한다.
- t-SNE(t-Distributed Stochastic Neighbor Embedding)는 비슷한 샘플은 가까이, 비슷하지 않은 샘플은 멀리 떨어지도록 하면서 차원을 축소한다. 주로 시각화에 많이 사용되며 특히 고차원 공간에 있는 샘플의 군집을 시각화할 때 사용된다.
- 선형 판별 분석(Linear Discriminant Analysis, LDA)은 사실 분류 알고리즘이다. 하지만 훈련 과정에서 클래스 사이를 가장 잘 구분하는 축을 학습한다. 이 축으 ㄴ데이터가 투영되는 초평면을 정의하는 데 사용할 수 있다. 이 알고리즘의 장점은 투영을 통해 가능한 한 클래스를 멀리 떨어지게 유지시키므로 SVM 분류기 같은 다른 분류 알고리즘을 적용하기 전에 차원을 축소시키는 데 좋다.

<br/>

