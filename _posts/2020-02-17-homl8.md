---
layout: post
title: 8. 차원 축소
category: Hands on Machine Learning
tag: Machine-Learning
---





## 1) 차원의 저주

- **차원의 저주** : 머신러닝에서 특성이 너무 많은 경우 훈련 속도가 느려지고, 좋은 모델을 만드는 데 방해되는 현상이 일어나는데 이를 차원의 저주(turse of dimensionality)라고 한다.
- 고차원의 세계에선 우리가 생각할 수 없는 일들이 자주 일어난다. 2차원 단위면적에서 임의의 점 1개를 선택할 때 경계선과의 거리가 0.001 이내일 확률은 0.4% 이지만, 10,000차원 초입방체에서는 확률이 99.999999% 이상이 된다. 또, 단위 면적, 단위 부피에서 두 점을 선택할 경우의 평균 거리는 각각 약 0.52, 0.66이다. 하지만 10,000차원에서 두 점을 선택할 경우 평균 거리는 약 428.25다. 후자의 예시는 고차원에서는 데이터들이 희박한(Sparse) 상태에 놓여있다는 것을 보여준다. 

<br/>

## 2) 차원 축소를 위한 접근 방법

- **투영** (projection) : 대부분의 데이터는 모든 차원에 걸쳐 균일하게 퍼져있지 않다. 많은 특성은 거의 변화가 없는 반면 몇몇 특성은 서로 강하게 연관되어 있다. 결과적으로 모든 훈련 샘플은 고차원 공간 안의 저차원 부분 공간(subspace)에 놓여 있다. 하지만 아래와 같이 스위스 롤 형태로 데이터가 놓여있는 경우 투영을 통해서는 제대로 된 데이터의 흐름을 볼 수 없다.

![SwissRoll](https://www.researchgate.net/profile/John_Burgoyne2/publication/200688576/figure/fig1/AS:305995638165506@1449966453759/The-Swiss-roll-data-set-On-the-left-the-data-is-presented-in-its-original-form-On.png)

- **매니폴드** (manifold) : 이럴 때 사용하는 방법이 매니폴드 방식이다. 많은 차원축소 알고리즘은 데이터가 놓여있는 매니폴드를 모델링하는 방식으로 작동한다. 

<br/>

## 3) PCA

- **PCA** (주성분 분석, Principal Component Analysis) : PCA는 가장 인기있는 차원축소 알고리즘이다. 먼저 데이터에 가장 가까운 초평면을 이용한 뒤, 데이터를 이 평면에 투영시킨다.

  - 분산 보존 : 첫 번째 과제는 가장 높은 분산을 가지는 데이터의 축을 찾는 것이다. 다른 방향으로 투영하는 것보다 분산이 최대로 보존되는 축으로 투영하는 것이 정보 손실을 최소한으로 줄일 수 있기 때문이다. 원본 데이터 셋과 투영된 것 사이의 평균 제곱 거리를 최소화하는 축을 찾는다고 표현할 수도 있다.
  - **주성분** (Principal Component, PC) : 첫 번째 축을 찾고 난 이후에는, 이 축과 수직이며 남은 분산을 최대로 보존하는 두 번째 축을 찾는다. 이런 과정을 반복하면서 여러 개의 축을 찾아 나간다. $i$ 번째 축을 정의하는 단위 벡터를 $i$ 번째 주성분이라 한다. 훈련 세트의 주성분은 **특잇값 분해** (SVD)라는 수학적 기술을 통하여 훈련 세트 행렬( $\mathbf{X}$ )를 $\mathbf{X} = \mathbf{U} \cdot \sum \cdot \mathbf{V}^T$ 로 분해할 수 있다. 여기서 $\mathbf{V}$ 에 찾고자 하는 모든 주성분이 담겨 있다.

  <p align="center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/800px-GaussianScatterPCA.svg.png" alt="PC" style="zoom: 50%;" /></p>

  - $d$ 차원으로 투영하기 : 주성분을 모두 추출했다면 $d$ 개의 주성분을 선택하여 $d$ 차원으로 축소시킬 수 있다. 초평면에 훈련 세트를 투영하기 위해서는 행렬 $\mathbf{X}$ 와 첫 $d$ 개의 주성분을 담은 (즉, $\mathbf{V}$ 의 첫 $d$ 열로 구성된) 행렬 $\mathbf{W}_d$ 를 점곱하면 된다.

  $$
  \mathbf{X}_{d-\text{proj}} = \mathbf{X} \cdot \mathbf{W}_d
  $$

  

  - 사이킷런 사용하기

  - 설명된 분산의 비율
  - 적절한 차원 수 선택하기
  - 압축을 위한 PCA
  - 점진적 PCA
  - 랜덤 PCA

<br/>

## 4) 커널 PCA

- [5장]([https://yngie-c.github.io/hands%20on%20machine%20learning/2020/02/05/homl5/](https://yngie-c.github.io/hands on machine learning/2020/02/05/homl5/)) 에서 샘플을 고차원 공간으로 매핑하여 서포트 벡터 머신의 비선형 분류와 회귀를 가능하게 하는 커널 트릭에 대해 이야기 했다. 고차원 결
  - 커널 선택과 하이퍼 파라미터 튜닝

<br/>

## 5) LLE

- 지역 선형 임베딩

<br/>

## 6) 다른 차원 축소 기법



<br/>