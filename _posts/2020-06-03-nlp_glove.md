---
layout: post
title: Glove & Fasttext
category: NLP
tag: NLP
---



본 포스트의 내용은 [고려대학교 강필성 교수님의 강의](https://www.youtube.com/watch?v=pXCHYq6PXto&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm) 와 [김기현의 자연어처리 딥러닝 캠프](http://www.yes24.com/Product/Goods/74802622) , [밑바닥에서 시작하는 딥러닝 2](http://www.yes24.com/Product/Goods/72173703) , [한국어 임베딩](http://m.yes24.com/goods/detail/78569687) 책을 참고하였습니다.



# GloVe

Word2Vec의 한계는 자주 사용되는 단어에 대해서 너무 많이 계산한다는 데 있다. *'the'* 와 같이 빈도가 매우 높은 단어가 있을 경우 학습에 불균형이 발생하기 마련이다. 몇 가지 샘플링 방식을 통해 계산량을 보정해 줄 수는 있지만 계산 문제를 완벽하게 해결 해주지는 못한다. 이를 원천적으로 해결하기 위해 등장한 것이 **GloVe** 이다. 아래 예시를 통해 GloVe 를 알아보자.

| Prob. and Ratio.                                        | k = solid            | k = gas              | k = water            | k = fashion          |
| ------------------------------------------------------- | :------------------- | -------------------- | -------------------- | -------------------- |
| $P(k \vert \text{ice})$                                 | $1.9 \times 10^{-4}$ | $6.6 \times 10^{-5}$ | $3.0 \times 10^{-3}$ | $1.7 \times 10^{-5}$ |
| $P(k \vert \text{steam})$                               | $2.2 \times 10^{-5}$ | $7.8 \times 10^{-4}$ | $2.2 \times 10^{-3}$ | $1.8 \times 10^{-5}$ |
| $\frac{P(k \vert \text{ice})}{P(k \vert \text{steam})}$ | 8.9                  | $8.5 \times 10^{-2}$ | 1.36                 | 0.96                 |

위 표는 *ice* 와 *steam* 이 각각 등장했을 때 *solid, gas, water, fashion* 이 등장하는 빈도를 나타낸 표이다. 표의 3번째 행에서 볼 수 있듯 *solid* 가 등장하는 빈도는 글에 *steam* 보다 *ice* 가 등장했을 때 훨씬 높다. 반대로 *gas* 가 등장하는 빈도는 *ice* 보다 *steam* 이 등장했을 때 더 낮다. *water, fashion* 이 등장하는 빈도는 글에 *ice* 가 나오든 *steam* 이 나오든 큰 상관이 없다.

위 3번째 행을 일반화한 $\frac{P(k|i)}{P(k|j)}$ 을 하나의 함수 $F$ 로 나타내보자.


$$
F(w_i, w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}} = \frac{P(k|i)}{P(k|j)}
$$


이 함수를 적당히 변형시켜 각각의 단어 벡터를 빼준 값과 단어 k의 벡터를 내적한 것을 변수로 한다고 하면 다음과 같이 나타낼 수 있다.


$$
F((w_i - w_j)^T \tilde{w}_k) = \frac{P_{ik}}{P_{jk}} = \frac{P(k|i)}{P(k|j)}
$$



이 $F$ 를 이용하여 표 3행에 있는 $\frac{P(k \vert \text{ice})}{P(k \vert \text{steam})}$ 와 역수를 나타내면 아래와 같이 나타낼 수 있다.


$$
\frac{P(\text{solid} \vert \text{ice})}{P(\text{solid} \vert \text{steam})} = F((\text{ice} - \text{steam})^T\text{solid}) \\
\frac{P(\text{solid} \vert \text{steam})}{P(\text{solid} \vert \text{ice})} = F((\text{steam} - \text{ice})^T\text{solid})
$$

**준동형 사상(Homomorphism)** 이라는 수학적 방법을 사용하면 함수 $F$ 의 구조를 유추해볼 수 있다. 아래의 수식을 살펴보면, 위 두 식에서 함수 내 변수는 덧셈에 대한 역원 관계에 있고, 함숫값은 곱셈에 대한 역원으로 표현됨을 알 수 있다. 

$$
(\text{ice} - \text{steam})^T\text{solid} = - (\text{steam} - \text{ice})^T\text{solid} \\
F((\text{ice} - \text{steam})^T\text{solid}) = \frac{1}{F((\text{steam} - \text{ice})^T\text{solid})}
$$

결국 $F$ 는 $F(a+b) = F(a)F(b)$ 를 만족하는 함수여야 하므로 $e^x$ 형태의 지수함수가 된다. $F(x) = \exp(x)$ 로 두고 단어 벡터의 내적 값을 아래와 같이 $\log X_{ik}$ 에 대한 값으로 나타내 줄 수 있다.


$$
w_i^T\tilde{w}_k = \log P_{ik} = \log X_{ik} - \log X_i \\
w_i^T\tilde{w}_k = \log X_{ik} - b_i - \tilde{b_k} \\
w_i^T\tilde{w}_k + b_i + \tilde{b_k} = \log X_{ik}
$$


위 식을 이용하여 Glove의 목적 함수를 쓸 수 있다.


$$
J = \sum_{i,j=1}^V (w_i^T\tilde{w}_j + b_i + \tilde{b_j} - \log X_{ij})^2 \\
\Rightarrow J = \sum_{i,j=1}^V f(X_{ij})(w_i^T\tilde{w}_j + b_i + \tilde{b_j} - \log X_{ij})^2
$$


여기에서 추가되는 함수 $f$ 는 3가지 조건을 만족해야 한다. $f(0) = 0$ 이어야 하고 동시 발생 확률이 낮은 단어를 감소시키면 안된다. 마지막으로 높은 $x$ 에 대해서는 ~ 해야한다. 이러한 함수 $f$ 를 사용하여 고빈도 단어의 영향력을 줄여준다. 



# Fasttext

NNLM, Word2Vec, Glove 는 모두 각 단어가 가진 형태적(morhology) 특성을 무시하고 있다는 한계를 가지고 있다. 때문에 터키어, 핀란드어처럼 형태적으로 복잡한 언어에는 잘 작동하지 않는다. **Fasttext** 는 이러한 단점을 해결하기 위해서 character 레벨에서부터 접근한다. character N-gram을 학습하며 단어는 character N-gram 벡터들의 합으로 표현한다.

Fasttext에서 사용되는 서브워드 모델은 아래와 같이 표현할 수 있다. 아래 수식에서 $G_w \subset {1, \cdots , G}$ 이다.


$$
\text{score}(w,c) = \sum_{g \in G_w} \mathbf{z}_g^T\mathbf{v}_c
$$


이런 방법으로 단어를 character N-gram 벡터의 값으로 표현한다. N-gram의 사이즈는 3,4,5,6 이며 단어마다 같은 character 시퀀스를 가지고 있더라도 평균을 내기 때문에 서로 다른 값으로 표현된다.
