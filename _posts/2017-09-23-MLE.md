---
title: 최대우도추정(Maximum Likelihood Estimation)
category: Statistics
tag: Maximum Likelihood Estimation
---

이번 글에서는 최대우도추정(Maximum Likelihood Estimation)에 대해 살펴보도록 하겠습니다. 이 글은 Ian Goodfellow 등이 집필한 Deep Learning Book과 위키피디아를 정리했음을 먼저 밝힙니다. 그럼 시작하겠습니다.





## 최대우도추정

최대우도추정(maximum likelihood estimation)이란 어떤 확률분포에서 샘플링한 값들을 바탕으로 해당 분포의 모수(parameter)를 구하는 방법입니다. 결과를 보고 원인을 추정하는 방법이라고 이해하면 좋을 것 같습니다. 

동전던지기를 예로 들어보겠습니다. 앞면이 나올 확률을 우리가 알고 싶은 모수를 $θ$라 두고 실제 동전을 던져본 결과 앞, 뒤, 앞이 나왔다고 하면 $θ$의 우도(likelihood) $L(θ)$는 다음과 같습니다.


$$
L\left( \theta ;\uparrow \downarrow \uparrow  \right) =P(\uparrow \downarrow \uparrow|\theta )=\theta \times (1-\theta )\times \theta ={ \theta  }^{ 2 }(1-\theta )
$$


최대우도추정은 우도(모수 $θ$가 주어졌을 때 관측치 $x$가 나타날 확률)를 최대로 만드는 모수 $θ$를 선택하는 방법입니다. 위 예시 기준으로 하면 $x$가 나타날 확률은 $θ$에 관한 식으로 표현되고, 식의 최대값은 $θ$에 대해 편미분한 식이 0이 되는 지점($θ(-3θ+2)=0$)이므로 최대우도추정 결과는 $θ=2/3$가 됩니다.





## 최대우도추정 vs 크로스 엔트로피, KL-Divergence

우리가 가진 학습데이터의 분포를 $P_{data}$, 모델이 예측한 결과의 분포를 $P_{model}$, 모델의 모수(파라메터)를 $θ$라고 두면 최대우도추정은 다음과 같이 쓸 수 있습니다. 아래 식이 유도되는 과정은 이렇습니다. 확률은 1보다 작기 때문에 계속 곱하면 그 값이 지나치게 작아져 언더플로우(underflow) 문제가 발생하므로 로그를 취합니다. 로그우도의 기대값은 로그우도의 합에 데이터 개수($m$)로 나누어 구합니다. 그런데 전체 값에 로그를 취하거나 스케일을 하여도 대소관계는 변하지는 않으므로 아래의 두 식이 동일한 의미를 갖습니다.


$$
\begin{align*}
{ \theta  }_{ ML }&=arg\max _{ \theta  }{ { P }_{ model }\left( X|\theta  \right)  } \\ &=arg\max _{ \theta  }{ \left\{ { E }_{ X\sim { \hat { P }  }_{ data } }\left[ \log { { P }_{ model }\left( x|\theta  \right)  }  \right]  \right\}  } 
\end{align*}
$$


쿨백-라이블러 발산(Kullback-Leibler divergence, KLD)은 두 확률분포의 차이를 계산하는 데 사용하는 함수입니다. 딥러닝 모델을 만들 때 예로 들면 우리가 가지고 있는 데이터의 분포 $P_{data}$와 모델이 추정한 데이터의 분포 $P_{model}$ 간에 차이를 KLD를 활용해 구할 수 있고, KLD를 최소화하는 것이 모델의 학습 과정이 되겠습니다. KLD의 식은 다음과 같이 정의됩니다.


$$
{ D }_{ KL }\left( P||Q \right) ={ E }_{ X\sim \hat{P}_{data} }\left[ \log { \hat{P}_{data}(x) } -\log { {P}_{model}(x) }  \right]
$$


그런데 위 식에서 왼쪽 term이 가리키는 $P_{data}$는 우리가 가지고 있는 데이터의 분포를 가리키며 학습과정에서 바뀌는 것이 아니므로 KLD를 최소화하는 건 위 식에서 오른쪽 term이 나타내는 값을 최소화한다는 의미가 됩니다. 위 식의 오른쪽 term(=아래의 term)을 크로스 엔트로피(cross entropy)라고 합니다.


$$
-{ E }_{ X\sim \hat { P } _{ data } }\left[ \log { { P }_{ model }(x) }  \right]
$$


크로스 엔트로피를 최대우도추정과 비교해 봅시다. 식을 비교해서 보면 **크로스 엔트로피(혹은 KLD) 최소화**가 **우도의 최대화**와 본질적으로 같습니다. 이 때문에 최대우도추정은 우리가 가지고 있는 데이터의 분포와 모델이 추정한 데이터의 분포를 가장 유사하게 만들어주는 모수(파라메터)를 찾아내는 방법이라고 봐도 될 것 같습니다. 이와 관련해서는 [이곳](https://ratsgo.github.io/statistics/2017/09/22/information/)을 더 참고하시면 좋을 것 같습니다.





## 최대우도추정 vs 최소제곱오차

머신러닝에서는 주로 조건부 우도를 최대화하는 방식으로 학습을 합니다. 입력값 $X$와 모델의 파라메터 $θ$가 주어졌을 때 정답 $Y$가 나타날 확률을 최대화하는 $θ$를 찾는 것입니다. 우리가 가지고 있는 데이터가 학습 과정에서 바뀌는 것은 아니므로 $X$와 $Y$는 고정된 상태입니다. 모델에 $X$를 넣었을 때 실제 $Y$에 가장 가깝게 반환하는 $θ$를 찾아내는 것이 관건이라고 볼 수 있겠습니다. $m$개의 모든 관측치가 i.i.d(independent and identically distributed)라고 가정하고, 언더플로우 방지를 위해 우도에 로그를 취한다면 최대우도추정 식은 다음과 정리할 수 있습니다.


$$
\begin{align*}
{\theta  }_{ ML }&=arg\max _{ \theta  }{ { P }_{ model }\left( Y|X;\theta  \right)  } \\ &=arg\max _{ \theta  }{ \sum _{ i=1 }^{ m }{ \log { { P }_{ model }\left( y_{ i }|{ x }_{ i };\theta  \right)  }  }  }
\end{align*}
$$


여기에서 $P_{model}$이 가우시안 확률함수라고 가정을 해봅시다. 다시 말해 $X$와 $Y$가 정규분포를 따를 것이라고 가정해 보는 것입니다. 그러면 정규분포 확률함수로부터 이 모델의 로그우도 합은 다음과 같이 쓸 수 있습니다. (분산 $σ^2$도 고정돼 있다고 가정, 사용자가 특정 상수값으로 지정)


$$
\sum _{ i=1 }^{ m }{ \log { { P }_{ model }\left( y_{ i }|{ x }_{ i };\theta  \right)  }  } =-m\log { \sigma  } -\frac { m }{ 2 } \log { 2\pi  } -\sum _{ i=1 }^{ m }{ \frac { { \left\| { \hat { y }  }_{ i }-{ y }_{ i } \right\|  }^{ 2 } }{ 2{ \sigma  }^{ 2 } }  } 
$$


선형회귀(liner regression)의 목적식은 평균제곱오차(Mean Squared Error)입니다. MSE의 식은 다음과 같이 정의됩니다.


$$
MSE=\frac { 1 }{ m } \sum _{ i=1 }^{ m }{ { \left\| { \hat { y }  }_{ i }-{ y }_{ i } \right\|  }^{ 2 } } 
$$


우리가 정규분포를 가정한 모델의 로그우도 합과 MSE를 비교해봅시다. 로그우도 합의 수식에서 세 개의 term 가운데 앞의 두 개의 term은 모두 상수값으로 학습과정에서 변하는 값이 아니며 로그우도의 합을 최대화하는 데 영향을 끼치는 term이 아닙니다. 이번엔 로그우도 합의 세번째 term과 MSE를 비교해 보겠습니다. 사용자가 지정한 $σ$, 데이터 개수 $m$은 모두 상수값이므로, 이들 또한 로그우도합과 MSE 값의 크기에 영향을 줄 수 없습니다. 

따라서 우리가 가정하는 확률모델이 정규분포일 경우, 우도를 최대화하는 모수(파라메터)와 평균제곱오차를 최소화하는 모수가 본질적으로 동일하다는 이야기가 됩니다.





## 왜 최대우도추정인가?

최대우도추정 기법으로 추정한 모수는 **일치성(consistency)**과 **효율성(efficiency)**이라는 좋은 특성을 가지고 있다고 합니다. 일치성이란 추정에 사용하는 표본의 크기가 커질 수록 진짜 모수값에 수렴하는 특성을 가리킵니다. 효율성이란 일치성 등에서 같은 추정량 가운데서도 분산이 작은 특성을 나타냅니다. 추정량의 효율성을 따질 때는 보통 평균제곱오차(MSE)를 기준으로 하는데, 크래머-라오 하한 정리에 의하면 일치성을 가진 추정량 가운데 최대우도추정량보다 낮은 MSE를 지닌 추정량이 존재하지 않는다고 합니다.

이러한 이유로 머신러닝에서는 모수를 추정할 때 최대우도추정 기법을 자주 쓴다고 합니다. 이미 언급했듯 이 최대우도추정 기법은 크로스 엔트로피 등과 깊은 관련을 맺고 있습니다. 이 때문에 많은 머신러닝 기법에서 손실함수로 크로스 엔트로피를 쓰는 것 같습니다.

